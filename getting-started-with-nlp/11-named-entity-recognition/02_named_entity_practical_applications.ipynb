{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02-named-entity-practical-applications.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNPpz11B8FYavWm4ZqqtYzT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-research-and-practice/blob/main/getting-started-with-nlp/11-named-entity-recognition/02_named_entity_practical_applications.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practical Applications of NER"
      ],
      "metadata": {
        "id": "RU7NpNkwNd5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s suppose ourselves of the scenario: it is widely known that certain events influence the trends of stock price movements: specifically, you can extract relevant facts from the news and then use these facts to predict company stock prices. \n",
        "\n",
        "Suppose you have access to a large collection of news; now your task is to extract the relevant events and facts\n",
        "that can be linked to the stock market in the downstream (stock market price prediction)\n",
        "application. \n",
        "\n",
        "How will you do that?\n",
        "\n",
        "This means that you have access to a collection of news texts, and among other\n",
        "preprocessing steps, you apply NER. Then you can focus only on the texts and sentences\n",
        "that are relevant for your task: for instance, if you are interested in the recent events, in\n",
        "which a particular company (e.g., “Apple”) participated, you can easily identify such texts,\n",
        "sentences, and contexts.\n",
        "\n",
        "<img src='https://github.com/rahiakela/natural-language-processing-research-and-practice/blob/main/getting-started-with-nlp/11-named-entity-recognition/images/ner1.png?raw=1' width='600'/>"
      ],
      "metadata": {
        "id": "DsPiYRUpiEeT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "1J2JXewgjX7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install spacy"
      ],
      "metadata": {
        "id": "GiZLc2EhNRtl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md "
      ],
      "metadata": {
        "id": "YPHVKVdoOkv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After install, just restart the colab runtime."
      ],
      "metadata": {
        "id": "-JKMIft1PDAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Xia3EBUbNoKw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download dataset from Kaggle."
      ],
      "metadata": {
        "id": "EC57Fy-gka7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload() # upload kaggle.json file"
      ],
      "metadata": {
        "id": "aJ8JQ3XMkfBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "mkdir -p ~/.kaggle\n",
        "mv kaggle.json ~/.kaggle/\n",
        "ls ~/.kaggle\n",
        "chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# download dataset from kaggle> URL: https://www.kaggle.com/datasets/snapcrack/all-the-news?select=articles1.csv\n",
        "kaggle datasets download -d snapcrack/all-the-news\n",
        "unzip -qq all-the-news.zip\n",
        "rm -rf all-the-news.zip"
      ],
      "metadata": {
        "id": "wC571VN_kfmQ",
        "outputId": "594b9f6a-d665-46f4-9ae6-a6cd44925db7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json\n",
            "Downloading all-the-news.zip to /content\n",
            " 93% 226M/244M [00:01<00:00, 135MB/s]\n",
            "100% 244M/244M [00:02<00:00, 127MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Loading and Exploration"
      ],
      "metadata": {
        "id": "fcLWUBtMjbP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use the data that has already been extracted from a range of news portals: the\n",
        "dataset called “All the news” is hosted on the Kaggle website. \n",
        "\n",
        "The dataset consists of\n",
        "143,000 articles scraped from 15 news websites, including The New York Times, CNN,\n",
        "Business Insider, The Washington Post, etc."
      ],
      "metadata": {
        "id": "GFXWIr9omDVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_df = pd.read_csv(\"articles1.csv\")\n",
        "news_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "Uq52j_N4NqON",
        "outputId": "e1e0fece-9a4b-46e6-e8a1-fe997a8f47c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0     id                                              title  \\\n",
              "0           0  17283  House Republicans Fret About Winning Their Hea...   \n",
              "1           1  17284  Rift Between Officers and Residents as Killing...   \n",
              "2           2  17285  Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...   \n",
              "3           3  17286  Among Deaths in 2016, a Heavy Toll in Pop Musi...   \n",
              "4           4  17287  Kim Jong-un Says North Korea Is Preparing to T...   \n",
              "\n",
              "      publication                         author        date    year  month  \\\n",
              "0  New York Times                     Carl Hulse  2016-12-31  2016.0   12.0   \n",
              "1  New York Times  Benjamin Mueller and Al Baker  2017-06-19  2017.0    6.0   \n",
              "2  New York Times                   Margalit Fox  2017-01-06  2017.0    1.0   \n",
              "3  New York Times               William McDonald  2017-04-10  2017.0    4.0   \n",
              "4  New York Times                  Choe Sang-Hun  2017-01-02  2017.0    1.0   \n",
              "\n",
              "   url                                            content  \n",
              "0  NaN  WASHINGTON  —   Congressional Republicans have...  \n",
              "1  NaN  After the bullet shells get counted, the blood...  \n",
              "2  NaN  When Walt Disney’s “Bambi” opened in 1942, cri...  \n",
              "3  NaN  Death may be the great equalizer, but it isn’t...  \n",
              "4  NaN  SEOUL, South Korea  —   North Korea’s leader, ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-42c1a4cc-32a2-4190-a30f-96f5b2f10a9f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>publication</th>\n",
              "      <th>author</th>\n",
              "      <th>date</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>url</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>17283</td>\n",
              "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Carl Hulse</td>\n",
              "      <td>2016-12-31</td>\n",
              "      <td>2016.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>17284</td>\n",
              "      <td>Rift Between Officers and Residents as Killing...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Benjamin Mueller and Al Baker</td>\n",
              "      <td>2017-06-19</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>After the bullet shells get counted, the blood...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>17285</td>\n",
              "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Margalit Fox</td>\n",
              "      <td>2017-01-06</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>17286</td>\n",
              "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>William McDonald</td>\n",
              "      <td>2017-04-10</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>17287</td>\n",
              "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Choe Sang-Hun</td>\n",
              "      <td>2017-01-02</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42c1a4cc-32a2-4190-a30f-96f5b2f10a9f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-42c1a4cc-32a2-4190-a30f-96f5b2f10a9f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-42c1a4cc-32a2-4190-a30f-96f5b2f10a9f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNgX3EIUPSgK",
        "outputId": "524919b6-acdd-4985-f44b-019b9bd17569"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the data from 15 news sources is split between several .csv files, let’s find out which news sources are covered."
      ],
      "metadata": {
        "id": "yHzv9lZrm9yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source = news_df[\"publication\"].unique()\n",
        "print(source)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qXgJ4enV3kF",
        "outputId": "d9ee2613-e1b8-4a19-d828-15903aaaa226"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['New York Times' 'Breitbart' 'CNN' 'Business Insider' 'Atlantic']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's extract the content of articles from a specific source."
      ],
      "metadata": {
        "id": "CJrgIzVy11jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a condition for the publication source to be “New York Times”\n",
        "condition = news_df[\"publication\"].isin([\"New York Times\"])\n",
        "# Select the content from all articles that satisfy this condition and only extract the first 1000 of them\n",
        "content_df = news_df.loc[condition, :][\"content\"][:1000]\n",
        "content_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXbVmQNfY27t",
        "outputId": "8a43de61-9c7b-42bf-deeb-4fbc59664505"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000,)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the contents of these articles\n",
        "content_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYoeWVbVZA5H",
        "outputId": "682f9212-dcbc-46ff-a30d-d5d541c89dbe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    WASHINGTON  —   Congressional Republicans have...\n",
              "1    After the bullet shells get counted, the blood...\n",
              "2    When Walt Disney’s “Bambi” opened in 1942, cri...\n",
              "3    Death may be the great equalizer, but it isn’t...\n",
              "4    SEOUL, South Korea  —   North Korea’s leader, ...\n",
              "Name: content, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Named Entity Types Exploration"
      ],
      "metadata": {
        "id": "YgCvesu72yjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s start by iterating through the news articles, collecting all named entities identified in\n",
        "texts, and storing the number of occurrences in a Python dictionary.\n",
        "\n",
        "<img src='https://github.com/rahiakela/natural-language-processing-research-and-practice/blob/main/getting-started-with-nlp/11-named-entity-recognition/images/ner2.png?raw=1' width='600'/>\n",
        "\n",
        "Let's populate a dictionary with NEs extracted from news articles."
      ],
      "metadata": {
        "id": "Br-prtEc22N4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_md\")"
      ],
      "metadata": {
        "id": "J9E9932Y61-k"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_entites(data_frame):\n",
        "  named_entities = {}\n",
        "  processed_docs = []\n",
        "\n",
        "  for item in data_frame:\n",
        "    # Process each news article with spaCy’s NLP pipeline\n",
        "    doc = nlp(item)\n",
        "    processed_docs.append(doc)\n",
        "\n",
        "    for ent in doc.ents:\n",
        "      # For each entity, extract the text\n",
        "      entity_text = ent.text\n",
        "      # Identify the type of the entity with ent.label_\n",
        "      entity_type = str(ent.label_)\n",
        "      # For each entity type, extract the list of currently stored entities with their counts\n",
        "      current_ents = {}\n",
        "      if entity_type in named_entities.keys():\n",
        "        current_ents = named_entities.get(entity_type)\n",
        "      current_ents[entity_text] = current_ents.get(entity_text, 0) + 1\n",
        "      named_entities[entity_type] = current_ents\n",
        "  return named_entities, processed_docs"
      ],
      "metadata": {
        "id": "04NGzSJQZJnp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "named_entities, processed_docs = collect_entites(content_df)"
      ],
      "metadata": {
        "id": "bhXqh2tYZQ5O"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's print out the named entities dictionary."
      ],
      "metadata": {
        "id": "zCsnUrqG7acJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_out(named_entities):\n",
        "  for key in named_entities.keys():\n",
        "    print(key)\n",
        "    # Extract all entities of a particular type from the dictionary\n",
        "    entities = named_entities.get(key)\n",
        "    sorted_keys = sorted(entities, key=entities.get, reverse=True)\n",
        "    # Sort the entries by their frequency in descending order and print out the most frequent n ones\n",
        "    for item in sorted_keys[:10]:\n",
        "      # It would be most informative to only look into entities that occur more than once\n",
        "      if entities.get(item) > 1:\n",
        "        print(f\"   {item}: {str(entities.get(item))}\")"
      ],
      "metadata": {
        "id": "UdcDYQ3V7coo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_out(named_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4So4Qe1ZWyZ",
        "outputId": "139cca6e-65d2-4aa2-e132-52762b731ffd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPE\n",
            "   the United States: 1141\n",
            "   Russia: 526\n",
            "   China: 514\n",
            "   Washington: 503\n",
            "   New York: 385\n",
            "   America: 356\n",
            "   Iran: 294\n",
            "   Mexico: 266\n",
            "   Britain: 237\n",
            "   California: 206\n",
            "NORP\n",
            "   American: 980\n",
            "   Republicans: 523\n",
            "   Republican: 473\n",
            "   Democrats: 398\n",
            "   Russian: 337\n",
            "   Chinese: 288\n",
            "   Americans: 267\n",
            "   British: 180\n",
            "   Democrat: 166\n",
            "   Muslim: 164\n",
            "PERSON\n",
            "   Trump: 3634\n",
            "   Obama: 839\n",
            "   Clinton: 186\n",
            "   Spicer: 134\n",
            "   Donald J. Trump: 128\n",
            "   Hillary Clinton: 123\n",
            "   Sessions: 123\n",
            "   Gorsuch: 116\n",
            "   Barack Obama: 115\n",
            "   Kushner: 110\n",
            "ORG\n",
            "   Trump: 768\n",
            "   Senate: 373\n",
            "   Congress: 344\n",
            "   Twitter: 310\n",
            "   White House: 235\n",
            "   The New York Times: 230\n",
            "   the White House: 223\n",
            "   Times: 211\n",
            "   House: 207\n",
            "   Google: 134\n",
            "MONEY\n",
            "   1: 66\n",
            "   2: 23\n",
            "   10: 19\n",
            "   millions of dollars: 19\n",
            "   100: 18\n",
            "   3: 18\n",
            "   billions of dollars: 17\n",
            "   5: 16\n",
            "   4: 15\n",
            "   $1 billion: 14\n",
            "CARDINAL\n",
            "   one: 1382\n",
            "   two: 910\n",
            "   000: 591\n",
            "   three: 349\n",
            "   One: 338\n",
            "   four: 172\n",
            "   seven: 170\n",
            "   1: 155\n",
            "   five: 131\n",
            "   2: 118\n",
            "DATE\n",
            "   Friday: 428\n",
            "   Wednesday: 350\n",
            "   Tuesday: 324\n",
            "   2015: 274\n",
            "   Thursday: 270\n",
            "   Monday: 266\n",
            "   last year: 257\n",
            "   Sunday: 257\n",
            "   Saturday: 240\n",
            "   years: 220\n",
            "LAW\n",
            "   the Affordable Care Act: 119\n",
            "   Constitution: 89\n",
            "   Roe v. Wade: 13\n",
            "   the Clean Air Act: 12\n",
            "   the Geneva Conventions: 9\n",
            "   the First Amendment: 6\n",
            "   The Affordable Care Act: 5\n",
            "   the Emoluments Clause: 4\n",
            "   the   Act: 4\n",
            "   the Constitution: 3\n",
            "LOC\n",
            "   Europe: 149\n",
            "   Africa: 52\n",
            "   Asia: 52\n",
            "   Silicon Valley: 50\n",
            "   Earth: 40\n",
            "   the Middle East: 38\n",
            "   West: 30\n",
            "   South: 28\n",
            "   earth: 27\n",
            "   North: 22\n",
            "ORDINAL\n",
            "   first: 1181\n",
            "   second: 223\n",
            "   third: 81\n",
            "   First: 37\n",
            "   fourth: 37\n",
            "   40th: 22\n",
            "   fifth: 19\n",
            "   45th: 12\n",
            "   sixth: 10\n",
            "   Second: 8\n",
            "TIME\n",
            "   morning: 123\n",
            "   night: 114\n",
            "   hours: 68\n",
            "   evening: 54\n",
            "   afternoon: 43\n",
            "   Wednesday night: 22\n",
            "   midnight: 21\n",
            "   last night: 21\n",
            "   Monday night: 19\n",
            "   Sunday night: 18\n",
            "FAC\n",
            "   the White House: 91\n",
            "   Broadway: 72\n",
            "   Trump Tower: 34\n",
            "   Vatican: 31\n",
            "   Capitol: 30\n",
            "   the National Mall: 16\n",
            "   Great Bear Lake: 16\n",
            "   Kennedy Airport: 13\n",
            "   Times Square: 11\n",
            "   Kennedy International Airport: 11\n",
            "QUANTITY\n",
            "   000 miles: 6\n",
            "   000 feet: 4\n",
            "   about 30 miles: 3\n",
            "   000 barrels: 3\n",
            "   000 tons: 3\n",
            "   40 miles: 3\n",
            "   hundreds of miles: 3\n",
            "   less than a mile: 3\n",
            "   150 miles: 3\n",
            "   about 600 miles: 3\n",
            "PERCENT\n",
            "   5 percent: 29\n",
            "   20 percent: 21\n",
            "   4 percent: 19\n",
            "   3 percent: 18\n",
            "   2 percent: 15\n",
            "   6 percent: 14\n",
            "   1 percent: 13\n",
            "   40 percent: 12\n",
            "   9 percent: 12\n",
            "   8 percent: 11\n",
            "EVENT\n",
            "   World War II: 49\n",
            "   the Super Bowl: 41\n",
            "   Super Bowl: 33\n",
            "   New Year’s Eve: 15\n",
            "   the Vietnam War: 13\n",
            "   Inauguration Day: 11\n",
            "   the Cold War: 10\n",
            "   the Australian Open: 10\n",
            "   the Cultural Revolution: 10\n",
            "   New Year’s Day: 9\n",
            "PRODUCT\n",
            "   Facebook: 47\n",
            "   Twitter: 43\n",
            "   Android: 15\n",
            "   Oscars: 12\n",
            "   Neanderthals: 11\n",
            "   Trump: 7\n",
            "   Boule: 7\n",
            "   Dolphins: 6\n",
            "   Titanic: 6\n",
            "   Globes: 6\n",
            "WORK_OF_ART\n",
            "   La La Land: 31\n",
            "   Moonlight: 25\n",
            "   Saturday Night Live: 22\n",
            "   Bible: 17\n",
            "   The Daily: 16\n",
            "   Hidden Figures: 14\n",
            "   The Mary Tyler Moore Show: 13\n",
            "   Meet the Press: 12\n",
            "   Today: 11\n",
            "   Fences: 10\n",
            "LANGUAGE\n",
            "   English: 46\n",
            "   French: 10\n",
            "   Arabic: 8\n",
            "   Spanish: 6\n",
            "   Tilikum: 4\n",
            "   Hebrew: 4\n",
            "   Mandarin: 3\n",
            "   Latin: 2\n",
            "   Russian: 2\n",
            "   Filipino: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way in which you can explore the statistics on various NE types is to aggregate\n",
        "the counts on the types and print out the number of unique entries.\n",
        "\n",
        "To do that, you extract and aggregate the statistics for each NE type, and in the end, you print out the results in a tabulated format, with each row\n",
        "storing the statistics on a separate NE type.\n",
        "\n",
        "Let's aggregate the counts on all named entity types."
      ],
      "metadata": {
        "id": "DrlZ1ZBaaJ2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "rows.append([\"Type:\", \"Entries:\", \"Total:\"])\n",
        "\n",
        "for ent_type in named_entities.keys():\n",
        "  rows.append([ent_type, str(len(named_entities.get(ent_type))), str(sum(named_entities.get(ent_type).values()))])\n",
        "\n",
        "columns = zip(*rows)\n",
        "column_widths = [max(len(item) for item in col) for col in columns]\n",
        "\n",
        "for row in rows:\n",
        "  print(\"\".join(\" {:{width}} \".format(row[i], width=column_widths[i]) for i in range(0, len(row))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U38138WRaLnn",
        "outputId": "b02d9679-1c5a-464c-c001-fa692f35d9e9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Type:        Entries:  Total: \n",
            " GPE          1760      15100  \n",
            " NORP         541       7525   \n",
            " PERSON       10000     30268  \n",
            " ORG          4893      15215  \n",
            " MONEY        681       1239   \n",
            " CARDINAL     1216      9097   \n",
            " DATE         3107      15117  \n",
            " LAW          129       412    \n",
            " LOC          455       1462   \n",
            " ORDINAL      69        1736   \n",
            " TIME         587       1614   \n",
            " FAC          548       1060   \n",
            " QUANTITY     308       358    \n",
            " PERCENT      268       658    \n",
            " EVENT        230       562    \n",
            " PRODUCT      294       537    \n",
            " WORK_OF_ART  1322      1951   \n",
            " LANGUAGE     17        94     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As this table shows, the most frequently used named entities in the news articles are entities\n",
        "of the following types: PERSON, GPE, ORG, and DATE. This is, perhaps, not very surprising:\n",
        "after all, most often news report on the events that are related to people (PERSON),\n",
        "companies (ORG), countries (GPE), and usually news articles include references to specific\n",
        "dates.\n",
        "\n",
        "At the same time, the least frequently used entities are the ones of the type\n",
        "LANGUAGE: there are only 17 unique languages mentioned in this news articles dataset, and\n",
        "in total they are mentioned 85 times.\n",
        "\n",
        "You may also note that ORDINAL type has only 68\n",
        "unique entries: it is, naturally, a very compact list of items including entries like first, second,\n",
        "third, and so on."
      ],
      "metadata": {
        "id": "J3-jl_nSa0mt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Information Extraction"
      ],
      "metadata": {
        "id": "fNq66HbRBTzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the scenario again: your task is to\n",
        "build an information extraction application focused on companies and the news that report\n",
        "on these companies. The dataset at hand, contains information on\n",
        "as many as 4,892 companies. Of course, not all of them might be of interest to you, so it\n",
        "would make sense to select a few and extract information on them.\n",
        "\n",
        "Recall that `spaCy`’s NLP pipeline processes sentences (or full documents) and returns a\n",
        "data structure, which contains all sorts of information on the words in the sentence (text),\n",
        "including the information about the word’s type (part-of-speech, e.g., verb, noun, etc.), its\n",
        "named entity type, its role in the sentence (e.g., main verb or ROOT, main action’s participant\n",
        "or nsubj, and so on).\n",
        "\n",
        "<img src='https://github.com/rahiakela/natural-language-processing-research-and-practice/blob/main/getting-started-with-nlp/11-named-entity-recognition/images/ner3.png?raw=1' width='600'/>\n",
        "\n",
        "In addition, each word has a unique index that is linked to its position in the sentence. If\n",
        "a named entity consists of multiple words, some of them may be marked with the `nsubj` or\n",
        "`dobj` relations (i.e., relevant relations in your application), but your goal is to extract not\n",
        "only the word marked as `nsubj` or `dobj` but the whole named entity, which plays this role. \n",
        "\n",
        "To\n",
        "do that, the best way is to match the named entities to their roles in the sentence via the\n",
        "indexes assigned to the named entities in the sentence.\n",
        "\n",
        "<img src='https://github.com/rahiakela/natural-language-processing-research-and-practice/blob/main/getting-started-with-nlp/11-named-entity-recognition/images/ner4.png?raw=1' width='600'/>\n",
        "\n",
        "Your goal is to identify whether The New York Times is one\n",
        "of the participants of the main action (wrote) in this sentence – the subject (the entity that\n",
        "performs the action) or an object (an entity to which the action applies). Indeed, The New\n",
        "York Times as a whole is the subject – it is the entity that performed the action of writing.\n",
        "\n",
        "However, since linguistic analysis applies to individual words rather than whole expressions,\n",
        "technically only the word Times is directly dependent on the main verb wrote – this is shown\n",
        "through the chain of relations.\n",
        "\n",
        "How can you extract the whole expression The\n",
        "New York Times?\n",
        "\n",
        "To do that, you first identify the indexes of the words covered by this expression in the\n",
        "sentence: for The New York Times these are `[0, 1, 2, 3]`.\n",
        "\n",
        "Next, you check if a word with any of these indexes plays a role of the subject or an\n",
        "object in the sentence. Indeed, the word that is the subject in the sentence has the index of 3.\n",
        "\n",
        "Therefore, you can return the whole\n",
        "named entity The New York Times as the subject of the main action in the sentence.\n",
        "\n",
        "Let's extract the indexes of the words covered by the NE.\n"
      ],
      "metadata": {
        "id": "6HXD3JgcBYa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_span(sent, entity):\n",
        "  indexes = []\n",
        "  for ent in sent.ents:\n",
        "    if ent.text == entity:\n",
        "      for i in range(int(ent.start), int(ent.end)):\n",
        "        indexes.append(i)\n",
        "  return indexes"
      ],
      "metadata": {
        "id": "ulp5nJ5XFZaI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's extract information about the main participants of the action."
      ],
      "metadata": {
        "id": "p11yl_p8G5Tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_information(sent, entity, indexes):\n",
        "  actions = []\n",
        "  action = \"\"\n",
        "  participant1 = \"\"\n",
        "  participant2 = \"\"\n",
        "\n",
        "  for token in sent:\n",
        "    # Identify the main verb expressing the main action in the sentence\n",
        "    if token.pos_==\"VERB\" and token.dep_==\"ROOT\":  \n",
        "        subj_ind = -1\n",
        "        obj_ind = -1\n",
        "        action = token.text\n",
        "        children = [child for child in token.children]   \n",
        "        for child1 in children:\n",
        "            if child1.dep_==\"nsubj\":\n",
        "                participant1 = child1.text\n",
        "                # Find the subject via the nsubj relation and store it as participant1 and its index as subj_ind\n",
        "                subj_ind = int(child1.i)\n",
        "            if child1.dep_==\"prep\":\n",
        "                participant2 = \"\"\n",
        "                child1_children = [child for child in child1.children]\n",
        "                for child2 in child1_children:\n",
        "                    if child2.pos_ == \"NOUN\" or child2.pos_ == \"PROPN\":\n",
        "                        participant2 = child2.text\n",
        "                        # Search for the indirect object as the second participant and store it as participant2 and its index as obj_ind\n",
        "                        obj_ind = int(child2.i)\n",
        "                if not participant2==\"\":\n",
        "                    if subj_ind in indexes:\n",
        "                        actions.append(entity + \" \" + action + \" \" + child1.text + \" \" + participant2)\n",
        "                    elif obj_ind in indexes:\n",
        "                        actions.append(participant1 + \" \" + action + \" \" + child1.text + \" \" + entity)\n",
        "\n",
        "            if child1.dep_==\"dobj\" and (child1.pos_ == \"NOUN\" or child1.pos_ == \"PROPN\"):\n",
        "                participant2 = child1.text\n",
        "                obj_ind = int(child1.i)\n",
        "                if subj_ind in indexes:\n",
        "                    actions.append(entity + \" \" + action + \" \" + participant2)\n",
        "                elif obj_ind in indexes:\n",
        "                    actions.append(participant1 + \" \" + action + \" \" + entity)\n",
        "                \n",
        "  if not len(actions)==0:\n",
        "      print (f\"\\nSentence = {sent}\")\n",
        "      for item in actions:\n",
        "          print(item)"
      ],
      "metadata": {
        "id": "sWhlcWB1G7C0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s apply this code to your texts extracted from the news articles.\n",
        "\n",
        "So, let's to extract information on the specific entity."
      ],
      "metadata": {
        "id": "SXXKkMQEIFXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def entity_detector(processed_docs, entity, ent_type):\n",
        "  output_sentences = []\n",
        "  for doc in processed_docs:\n",
        "    for sent in doc.sents:\n",
        "      if entity in [ent.text for ent in sent.ents if ent.label_ == ent_type]:\n",
        "        output_sentences.append(sent)\n",
        "  return output_sentences"
      ],
      "metadata": {
        "id": "wmjxojU9IbMz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity = \"Apple\"\n",
        "ent_sentences = entity_detector(processed_docs, entity, \"ORG\")\n",
        "print(len(ent_sentences))\n",
        "\n",
        "for sent in ent_sentences:\n",
        "  indexes = extract_span(sent, entity)\n",
        "  extract_information(sent, entity, indexes)"
      ],
      "metadata": {
        "id": "WrhBqMDCKhaI",
        "outputId": "884b7b4e-765f-4316-85af-11b2fb26bf11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "61\n",
            "\n",
            "Sentence = Apple, complying with what it said was a request from Chinese authorities, removed news apps created by The New York Times from its app store in China late last month.\n",
            "Apple removed apps\n",
            "\n",
            "Sentence = Apple removed both the   and   apps from the app store in China on Dec. 23.\n",
            "Apple removed apps\n",
            "Apple removed from store\n",
            "Apple removed on Dec.\n",
            "\n",
            "Sentence = Apple has previously removed other, less prominent media apps from its China store.\n",
            "Apple removed apps\n",
            "\n",
            "Sentence = It puts Apple and Google in a difficult position.\n",
            "It puts Apple\n",
            "\n",
            "Sentence = Russia required Apple and Google to remove the LinkedIn app from their local stores.\n",
            "Russia required Apple\n",
            "\n",
            "Sentence = On Friday, Apple, its longtime partner, sued Qualcomm over what it said was $1 billion in withheld rebates.\n",
            "Apple sued Qualcomm\n",
            "\n",
            "Sentence = Apple sued three days after the  Federal Trade Commission accused Qualcomm of using anticompetitive practices to guarantee its high royalty payments for advanced wireless technology.\n",
            "Apple sued days\n",
            "\n",
            "Sentence = Apple returned to growth, reporting    revenue largely thanks to the latest iPhones.\n",
            "Apple returned to growth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main content of the sentences is concisely summarized by the tuples consisting of\n",
        "the main action and its two participants, so if you were interested in extracting only the\n",
        "sentences that have such informative content and that directly answer questions “What did Apple do to X?” or “What did Y do to Apple?”\n",
        "\n",
        "Now, let's extract information on named entities consisting of multiple words."
      ],
      "metadata": {
        "id": "O_-EGWhSLa-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entity = \"The New York Times\"\n",
        "sentences = [\"The New York Times wrote about Apple\"]\n",
        "\n",
        "for sent in sentences:\n",
        "  doc = nlp(sent)\n",
        "  indexes = extract_span(doc, entity)\n",
        "  print(indexes)\n",
        "  extract_information(doc, entity, indexes)"
      ],
      "metadata": {
        "id": "hNwpYuiRLxN2",
        "outputId": "7cf4c81b-ccf4-4979-e78e-a754824093f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3]\n",
            "\n",
            "Sentence = The New York Times wrote about Apple\n",
            "The New York Times wrote about Apple\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Named Entities Visualization"
      ],
      "metadata": {
        "id": "XbMDsFYGMMdI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ENpoEOM6MOcR"
      }
    }
  ]
}