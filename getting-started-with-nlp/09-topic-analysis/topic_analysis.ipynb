{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topic-analysis.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPSOogHHGuFkJFIhvxr1Vdf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-research-and-practice/blob/main/getting-started-with-nlp/09-topic-analysis/topic_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Topic Analysis"
      ],
      "metadata": {
        "id": "KNfBwCp922nA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, you will work with one more application of\n",
        "this powerful framework – topic classification. \n",
        "\n",
        "Let’s start with a scenario: suppose you work\n",
        "as a content manager for a large news platform. Your platform hosts texts from a wide\n",
        "variety of authors and mainly specializes in the following set of well-established topics:\n",
        "“Politics”, “Finance”, “Science”, “Sports”, and “Arts”. Your task is to decide, for every\n",
        "incoming article, which topic it belongs to and post it under the relevant tab on the platform.\n",
        "\n",
        "This scenario relates to the task that we can broadly define as topic analysis.\n",
        "\n",
        "Imagine\n",
        "that the content on your platform doesn’t stay the same all the time – new topics may\n",
        "emerge in the data. \n",
        "\n",
        "Unfortunately, if you wanted to train a classification model to cover\n",
        "these topics in a supervised manner, you will need labeled articles for these new topics as\n",
        "well. \n",
        "\n",
        "Data availability is the major bottleneck for supervised machine learning. \n",
        "\n",
        "Therefore, you need to learn about alternative ways of topic discovery and need to apply two unsupervised machine learning algorithms – \n",
        "\n",
        "* clustering\n",
        "* Latent Dirichlet Allocation\n",
        "\n",
        "\n",
        "<img src='https://github.com/rahiakela/natural-language-processing-research-and-practice/blob/main/getting-started-with-nlp/09-topic-analysis/images/1.png?raw=1' width='600'/>"
      ],
      "metadata": {
        "id": "qbKLnvHa3DKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "TrK8SygW4mos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "XN5nbKtW4n1X"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset"
      ],
      "metadata": {
        "id": "dZaXUkIc5q2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(subsets, cats):\n",
        "  dataset = fetch_20newsgroups(subset=subsets, categories=cats, remove=(\"headers\", \"footers\", \"quotes\"), shuffle=True)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "j4GtrPgf5r9C"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\"comp.windows.x\", \"misc.forsale\", \"rec.autos\", \"rec.motorcycles\", \"rec.sport.baseball\"]\n",
        "categories += [\"rec.sport.hockey\", \"sci.crypt\", \"sci.med\", \"sci.space\", \"talk.politics.mideast\"]\n",
        "\n",
        "newsgroups_train = load_dataset(\"train\", categories)\n",
        "newsgroups_test = load_dataset(\"test\", categories)"
      ],
      "metadata": {
        "id": "--Qi6MSi6PbA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's check our uploaded data subsets\n",
        "def check_data(dataset):\n",
        "  print(list(dataset.target_names))\n",
        "  print(dataset.filenames.shape)\n",
        "  print(dataset.target.shape)\n",
        "\n",
        "  if dataset.filenames.shape[0] == dataset.target.shape[0]:\n",
        "    print(\"Equal sizes for data and targets\")\n",
        "  \n",
        "  print(dataset.filenames[0])\n",
        "  print(dataset.data[0])\n",
        "  print(dataset.target[:10])"
      ],
      "metadata": {
        "id": "_0w5aZyU7Yj0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_data(newsgroups_train)\n",
        "print(\"\\n***\\n\")\n",
        "check_data(newsgroups_test)"
      ],
      "metadata": {
        "id": "6A8TOcAT8STP",
        "outputId": "80ea9a86-7357-4dd0-db65-9faf307e7570",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.med', 'sci.space', 'talk.politics.mideast']\n",
            "(5913,)\n",
            "(5913,)\n",
            "Equal sizes for data and targets\n",
            "/root/scikit_learn_data/20news_home/20news-bydate-train/rec.sport.baseball/102665\n",
            "I have posted the logos of the NL East teams to alt.binaries.pictures.misc \n",
            " Hopefully, I'll finish the series up next week with the NL West.\n",
            "\n",
            " Darren\n",
            "\n",
            "[4 3 9 7 4 3 0 5 7 8]\n",
            "\n",
            "***\n",
            "\n",
            "['comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.med', 'sci.space', 'talk.politics.mideast']\n",
            "(3937,)\n",
            "(3937,)\n",
            "Equal sizes for data and targets\n",
            "/root/scikit_learn_data/20news_home/20news-bydate-test/misc.forsale/76785\n",
            "As the title says. I would like to sell my Star LV2010 9 pin printer.\n",
            "Its a narrow colum dot matrix, supports both parallel and serial\n",
            "interfacing, prints at 200 characters per second, has a 16K buffer, \n",
            "and is very dependable...\n",
            "\n",
            "Drop some mail if your interested in it. $55 Plus shipping get the\n",
            "printer, and 6 extra srink-wraped ribbons, parallel connection\n",
            "cable, power cord, manual, and ONE sheet of paper (smile)...\n",
            "[1 7 2 5 3 5 7 3 0 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Naïve Bayes classifier"
      ],
      "metadata": {
        "id": "MOZUHcvP9whf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic classification\n",
        "task on the basis of word occurrences boils down to recognizing which topic a text may\n",
        "belong to based on which words are used in this text.\n",
        "\n",
        "TF-IDF is a technique that allows you to downweigh terms\n",
        "that occur frequently across many documents and upvalue terms that occur frequently only\n",
        "in some documents but not across the whole collection."
      ],
      "metadata": {
        "id": "ZnquC4b197i6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply TfidfVectorizer and convert texts into vectors\n",
        "def text2vec(vectorizer, train_set, test_set):\n",
        "  # estimates tf-idf weights on the training data and then applies them to the test data\n",
        "  vectors_train = vectorizer.fit_transform(train_set.data)\n",
        "  vectors_test = vectorizer.transform(test_set.data)\n",
        "  return vectors_train, vectors_test"
      ],
      "metadata": {
        "id": "-N-AXf91sHYR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "\n",
        "vectors_train, vectors_test = text2vec(vectorizer, newsgroups_train, newsgroups_test)\n",
        "\n",
        "print(vectors_train.shape)\n",
        "print(vectors_test.shape)\n",
        "print(vectors_train[0])\n",
        "print(vectorizer.get_feature_names_out()[33404])"
      ],
      "metadata": {
        "id": "XPYmS0AJsZ7r",
        "outputId": "c18c5a96-eb1e-4ad5-e819-e0590166205b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5913, 52746)\n",
            "(3937, 52746)\n",
            "  (0, 15218)\t0.31618146678372416\n",
            "  (0, 50534)\t0.20153071455804605\n",
            "  (0, 50435)\t0.1817612919269656\n",
            "  (0, 42031)\t0.1891577831889085\n",
            "  (0, 20349)\t0.2372918776268056\n",
            "  (0, 29215)\t0.14244326085583361\n",
            "  (0, 24214)\t0.23045715683316248\n",
            "  (0, 31546)\t0.21952696479551445\n",
            "  (0, 36274)\t0.23637098993673133\n",
            "  (0, 9616)\t0.2606508810838842\n",
            "  (0, 6736)\t0.23045715683316248\n",
            "  (0, 46098)\t0.18751137951875305\n",
            "  (0, 17820)\t0.1996672692556469\n",
            "  (0, 33404)\t0.47274197987346267\n",
            "  (0, 29330)\t0.32348469409130415\n",
            "  (0, 36985)\t0.1806134526365663\n",
            "nl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.get_feature_names_out()[15218])"
      ],
      "metadata": {
        "id": "XKfsG0aytzlG",
        "outputId": "12253c57-4839-4732-9a38-6d4efcb99a49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "darren\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.get_feature_names_out()[50534])"
      ],
      "metadata": {
        "id": "cNnb2YdFt3JX",
        "outputId": "25aa3bd4-0b5f-40c5-f0a5-12f55b45c152",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "west\n"
          ]
        }
      ]
    }
  ]
}