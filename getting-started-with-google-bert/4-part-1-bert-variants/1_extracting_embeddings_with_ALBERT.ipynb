{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-extracting-embeddings-with-ALBERT.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOS0Kf1eFbV86ms8G74Nwm9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/getting-started-with-google-bert/blob/main/4-part-1-bert-variants/1_extracting_embeddings_with_ALBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-01JQRyEyPyK"
      },
      "source": [
        "## Extracting embeddings with ALBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNSMwh6xyy5q"
      },
      "source": [
        "One of the challenges with BERT is that it consists of millions of parameters. BERT-base consists of 110 million parameters, which makes it harder to train, and it also has a high inference time. Increasing the model size gives us good results but it puts a limitation on the computational resources. To combat this, ALBERT was introduced. ALBERT is a lite\n",
        "version of BERT with fewer parameters compared to BERT. It uses the following two techniques to reduce the number of parameters:\n",
        "\n",
        "- Cross-layer parameter sharing\n",
        "- Factorized embedding layer parameterization\n",
        "\n",
        "By using the preceding two techniques, we can reduce the training time and inference time of the BERT model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5ET21JvgzCh"
      },
      "source": [
        "###Cross-layer parameter sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uiykKIcg0na"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWWvPi3M1HOG"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUvximYa1IeA"
      },
      "source": [
        "%%capture\n",
        "!pip install torch==1.4.0\n",
        "!pip install nlp==0.4.0\n",
        "!pip install transformers==3.5.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjC56MC91X7K"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from transformers import BertForQuestionAnswering, BertTokenizer, Trainer, TrainingArguments\n",
        "from nlp import load_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnlpOwrUoirp"
      },
      "source": [
        "## Question-answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb5ptuQDok1F"
      },
      "source": [
        "In a question-answering task, we are given a question along with a paragraph containing an answer to the question. Our goal is to extract the answer from the paragraph for the given question.\n",
        "\n",
        "The input to the BERT model will be a question-paragraph pair. That is, we feed a question and a paragraph containing the answer to the question to BERT and it has to extract the answer from the paragraph. So, essentially, BERT has to return the text span that contains the answer from the paragraph. \n",
        "\n",
        "Let's understand this with an example â€“ consider the following question-paragraph pair:\n",
        "\n",
        "```\n",
        "Question = \"What is the immune system?\"\n",
        "\n",
        "Paragraph = \"The immune system is a system of many biological structures and processes within an organism that protects against disease. To function properly, an immune system must detect a wide variety of agents, known as pathogens, from viruses to parasitic worms, and distinguish them from\n",
        "the organism's own healthy tissue.\"\n",
        "```\n",
        "\n",
        "Now, our model has to extract an answer from the paragraph; it essentially has to return the text span containing the answer. So, it should return the following:\n",
        "\n",
        "```\n",
        "Answer = \"a system of many biological structures and processes within an organism that protects against disease\"\n",
        "```\n",
        "\n",
        "To do this, our model has to understand the starting and ending index of the text span containing the answer in the given paragraph. For example, take the question, \"What is the immune system?\" If our model understands that the answer to this question starts from index 4 (\"a\") and ends at index 21\n",
        "(\"disease\"), then we can get the answer as shown here:\n",
        "\n",
        "```\n",
        "Paragraph = \"The immune system is **a system of many system of many biological structures and processes within an organism that protects against disease\"** biological structures and processes within an organism that protects against disease. To function properly, an immune system must detect a wide variety of agents, known as pathogens, from viruses to parasitic worms, and distinguish them from the organism's own healthy tissue.\"\n",
        "```\n",
        "\n",
        "If we get the probability of each token (word) in the paragraph of being the starting and ending token (word) of the answer, then we can easily extract the answer, right? Yes, but how we can achieve this? To do this, we use two vectors called the start vector $S$ and the end vector $E$. The values of the start and end vectors will be learned during training.\n",
        "\n",
        "First, we compute the probability of each token (word) in the paragraph being the starting token of the answer.\n",
        "\n",
        "To compute this probability, for each token $i$, we compute the dot product between the representation of the token $R_i$ and the start vector $S$. Next, we apply the softmax function to the dot product $S.R_i$ and obtain the probability:\n",
        "\n",
        "$$ P_i = \\frac{e^{S.R_i}}{\\sum_j{e^{S.R_j}}} $$\n",
        "\n",
        "Next, we compute the starting index by selecting the index of the token that has a high probability of being the starting token.\n",
        "\n",
        "In a very similar fashion, we compute the probability of each token (word) in the paragraph being the ending token of the answer. To compute this probability, for each token $i$, we compute the dot product between the representation of the token $R_i$ and the end vector $E$.\n",
        "\n",
        "Next, we apply the softmax function to the dot product $E.R_i$ and obtain the probability:\n",
        "\n",
        "$$ P_i = \\frac{e^{E.R_i}}{\\sum_j{e^{E.R_j}}} $$\n",
        "\n",
        "Next, we compute the ending index by selecting the index of the token that has a high probability of being the ending token. Now, we can select the text span that contains the answer using the starting and ending index.\n",
        "\n",
        "As shown, first, we tokenize the question-paragraph pair and feed\n",
        "the tokens to the pre-trained BERT model, which returns the embeddings of all the tokens.\n",
        "As shown, $R_1$ to $R_N$ denotes the embeddings of the tokens in the question and $R^_1$ to $R_M$ denotes the embedding of the tokens in the paragraph.\n",
        "\n",
        "After computing the embedding, we compute the dot product with the start/end vectors, apply the softmax function, and obtain the probabilities of each token in the paragraph being the start/end word as shown here:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/getting-started-with-google-bert/fine-tuning-question-answering.png?raw=1' width='800'/>\n",
        "\n",
        "We can see how we compute the probability of each token in the paragraph being the start/end word. Next, we select the text span containing the answer\n",
        "using the starting and ending indexes with the highest probability.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH_-9Aopnb9e"
      },
      "source": [
        "## Loading the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33eV-4LbkgR4"
      },
      "source": [
        "We use the `bert-large-uncased-whole-wordmasking-fine-tuned-squad` model, which is fine-tuned on the **Stanford Question- Answering Dataset (SQUAD)**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikDxtyh3klMH"
      },
      "source": [
        "model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6leXTpfblQ0O"
      },
      "source": [
        "Next, we download and load the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmN71BNhlRZJ"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al2LYQzflVPG"
      },
      "source": [
        "Now that we have downloaded the model and tokenizer, let's preprocess the input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADJ2dXfx1pln"
      },
      "source": [
        "## Preprocessing the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1_XY_RKwYOd"
      },
      "source": [
        "First, we define the input to BERT, which is the question and paragraph text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az0W44ZuxRZF"
      },
      "source": [
        "question = \"What is the immune system?\"\n",
        "paragraph = \"The immune system is a system of many biological structures and processes within an organism that protects against disease. To function properly, an immune system must detect a wide variety of agents, known as pathogens, from viruses to parasitic worms, and distinguish them from the organism's own healthy tissue.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVzOd4STxa4U"
      },
      "source": [
        "Add a `[CLS]` token to the beginning of the question and an `[SEP]` token to the end of both the question and the paragraph:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFF-_v_LxVCG"
      },
      "source": [
        "question = \"[CLS] \" + question + \"[SEP]\"\n",
        "paragraph = paragraph + \"[SEP]\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QXcW3j8zzgQ"
      },
      "source": [
        "Now, tokenize the question and paragraph:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAIFr8yAuBZR"
      },
      "source": [
        "question_tokens = tokenizer.tokenize(question)\n",
        "paragraph_tokens = tokenizer.tokenize(paragraph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3dzeZzo0Ka_"
      },
      "source": [
        "Combine the question and paragraph tokens and convert them to `input_ids`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBkzUYatznD-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c2576bb-5444-41d2-d90c-29ef17820db8"
      },
      "source": [
        "tokens = question_tokens + paragraph_tokens\n",
        "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(input_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[101, 2054, 2003, 1996, 11311, 2291, 1029, 102, 1996, 11311, 2291, 2003, 1037, 2291, 1997, 2116, 6897, 5090, 1998, 6194, 2306, 2019, 15923, 2008, 18227, 2114, 4295, 1012, 2000, 3853, 7919, 1010, 2019, 11311, 2291, 2442, 11487, 1037, 2898, 3528, 1997, 6074, 1010, 2124, 2004, 26835, 2015, 1010, 2013, 18191, 2000, 26045, 16253, 1010, 1998, 10782, 2068, 2013, 1996, 15923, 1005, 1055, 2219, 7965, 8153, 1012, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSyMTQyN0fRe"
      },
      "source": [
        "Next, we define `segment_ids`. \n",
        "\n",
        "Now, `segment_ids` will be 0 for all the tokens of the question and 1 for all the tokens of the paragraph:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dizML_zt0cDL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d6aa249-1888-4d8a-f750-101956a8ed82"
      },
      "source": [
        "segment_ids = [0] * len(question_tokens)\n",
        "segment_ids += [1] * len(paragraph_tokens)\n",
        "print(segment_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyjU-l1t1DNb"
      },
      "source": [
        "Now we convert `input_ids` and `segment_ids` to tensors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrsEh8iF08wI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7266d5c6-8e65-48db-fa6b-3f939d43c14c"
      },
      "source": [
        "input_ids = torch.tensor([input_ids])\n",
        "segment_ids = torch.tensor([segment_ids])\n",
        "\n",
        "print(input_ids)\n",
        "print(segment_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[  101,  2054,  2003,  1996, 11311,  2291,  1029,   102,  1996, 11311,\n",
            "          2291,  2003,  1037,  2291,  1997,  2116,  6897,  5090,  1998,  6194,\n",
            "          2306,  2019, 15923,  2008, 18227,  2114,  4295,  1012,  2000,  3853,\n",
            "          7919,  1010,  2019, 11311,  2291,  2442, 11487,  1037,  2898,  3528,\n",
            "          1997,  6074,  1010,  2124,  2004, 26835,  2015,  1010,  2013, 18191,\n",
            "          2000, 26045, 16253,  1010,  1998, 10782,  2068,  2013,  1996, 15923,\n",
            "          1005,  1055,  2219,  7965,  8153,  1012,   102]])\n",
            "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc9syOQ91djD"
      },
      "source": [
        "Now that we have processed the input, let's feed it to the model and get the result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCiUOZrh1eEh"
      },
      "source": [
        "## Getting the answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBz-CzcQxBqV"
      },
      "source": [
        "We feed the `input_ids` and `segment_ids` to the model which return the start score and end score for all of the tokens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpVMLRsO1ZZO"
      },
      "source": [
        "start_scores, end_scores = model(input_ids, token_type_ids=segment_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A-PsDEg2rcR"
      },
      "source": [
        "Now, we select the `start_index` which is the index of the token which has a maximum start score and `end_index` which is the index of the token which has a maximum end score:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N-invI82nOI"
      },
      "source": [
        "start_index = torch.argmax(start_scores)\n",
        "end_index = torch.argmax(end_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLce21yd4QOr"
      },
      "source": [
        "That's it! Now, we print the text span between the start and end indexes as our answer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75F4BMsO3BuJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf9d3ae3-bfea-4a61-8df3-b137128cb8d2"
      },
      "source": [
        "print(\" \".join(tokens[start_index: end_index + 1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a system of many biological structures and processes within an organism that protects against disease\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}