{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine-translator.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOSk6tsTABsvc8HYniQWLd+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-research-and-practice/blob/main/real-world-natural-language-processing/6-sequence-to-sequence-models/machine_translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKJ5NY0ZJiPD"
      },
      "source": [
        "##Machine Translator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3nTcGA3Jkzy"
      },
      "source": [
        "In this notebook, we are going to build a working MT system. Instead of writing any\n",
        "Python code to do that, we’ll make the most of existing MT frameworks. A number of\n",
        "open source frameworks make it easier to build MT systems, including [Moses](http://www.statmt.org/moses/) for SMT and [OpenNMT](http://opennmt.net/) for NMT.\n",
        "\n",
        "But, we will use [Fairseq](https://github.com/pytorch/fairseq), an NMT\n",
        "toolkit developed by Facebook that is becoming more and more popular among NLP\n",
        "practitioners these days.\n",
        "\n",
        "The following aspects make Fairseq a good choice for developing\n",
        "an NMT system quickly:\n",
        "\n",
        "- it is a modern framework that comes with a number\n",
        "of predefined state-of-the-art NMT models that you can use out of the box;\n",
        "- it is very extensible, meaning you can quickly implement your own model by following their API;\n",
        "- it is very fast, supporting multi-GPU and distributed training by default.\n",
        "\n",
        "Thanks to its powerful models, you can build a decent quality NMT system within a couple of hours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OypcEydKKfC2"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLVwz4BMKgHT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a9bdcb5-3e55-4895-bc3c-2b73f5200eca"
      },
      "source": [
        "!pip -q install fairseq"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 90 kB 8.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 145 kB 46.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 48.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 44.3 MB/s \n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO2ncsqWKu3k"
      },
      "source": [
        "Let's download and expand the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ee7uVfRKxU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce9fcc38-4716-4fa1-c608-b841578c6bcc"
      },
      "source": [
        "%%shell\n",
        "\n",
        "mkdir -p data/mt\n",
        "wget https://realworldnlpbook.s3.amazonaws.com/data/mt/tatoeba.eng_spa.zip\n",
        "unzip tatoeba.eng_spa.zip -d data/mt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-16 06:27:51--  https://realworldnlpbook.s3.amazonaws.com/data/mt/tatoeba.eng_spa.zip\n",
            "Resolving realworldnlpbook.s3.amazonaws.com (realworldnlpbook.s3.amazonaws.com)... 52.216.225.240\n",
            "Connecting to realworldnlpbook.s3.amazonaws.com (realworldnlpbook.s3.amazonaws.com)|52.216.225.240|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19148555 (18M) [application/zip]\n",
            "Saving to: ‘tatoeba.eng_spa.zip’\n",
            "\n",
            "tatoeba.eng_spa.zip 100%[===================>]  18.26M  44.4MB/s    in 0.4s    \n",
            "\n",
            "2021-11-16 06:27:52 (44.4 MB/s) - ‘tatoeba.eng_spa.zip’ saved [19148555/19148555]\n",
            "\n",
            "Archive:  tatoeba.eng_spa.zip\n",
            "  inflating: data/mt/tatoeba.eng_spa.train.tok.en  \n",
            "  inflating: data/mt/tatoeba.eng_spa.train.tok.es  \n",
            "  inflating: data/mt/tatoeba.eng_spa.train.tsv  \n",
            "  inflating: data/mt/tatoeba.eng_spa.valid.tok.en  \n",
            "  inflating: data/mt/tatoeba.eng_spa.valid.tok.es  \n",
            "  inflating: data/mt/tatoeba.eng_spa.valid.tsv  \n",
            "  inflating: data/mt/tatoeba.eng_spa.tsv  \n",
            "  inflating: data/mt/tatoeba.eng_spa.test.tsv  \n",
            "  inflating: data/mt/tatoeba.eng_spa.test.tok.en  \n",
            "  inflating: data/mt/tatoeba.eng_spa.test.tok.es  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGqZISQeLc-A"
      },
      "source": [
        "The\n",
        "corpus consists of approximately 200,000 English sentences and their Spanish translations.\n",
        "I went ahead and already formatted the dataset so that you can use it without worrying about obtaining the data, tokenizing the text, and so on. The dataset is\n",
        "already split into train, validate, and test subsets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "788vSarZLf0U"
      },
      "source": [
        "##Preparing the datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qah-tp9yLibl"
      },
      "source": [
        "As we know, MT systems (both SMT and NMT) are machine learning\n",
        "models and thus are trained from data. The development process of MT systems looks similar to any other modern NLP systems.\n",
        "\n",
        "First, the training portion of the parallel corpus is preprocessed and used to train a set of NMT model candidates. \n",
        "\n",
        "Next, the validation portion is used to choose the best-performing model\n",
        "out of all the candidates. This process is called model selection.\n",
        "\n",
        "Finally, the best model is tested on the test portion of the dataset to obtain\n",
        "evaluation metrics, which reflect how good the model is.\n",
        "\n",
        "<img src='https://github.com/rahiakela/natural-language-processing-research-and-practice/blob/main/real-world-natural-language-processing/6-sequence-to-sequence-models/images/1.png?raw=1' width='800'/>\n",
        "\n",
        "The first step in MT development is preprocessing the dataset. But before preprocessing, you need to convert the dataset into an easy-to-use format, which is usually plain text in NLP.\n",
        "\n",
        "In practice, the raw data for training MT systems come in many different\n",
        "formats, for example, plain text files (if you are lucky), XML formats of proprietary\n",
        "software, PDF files, and database records. Your first job is to format the raw files so\n",
        "that source sentences and their target translations are aligned sentence by sentence.\n",
        "\n",
        "The resulting file is often a TSV file where each line is a tab-separated sentence pair, which looks like the following:\n",
        "\n",
        "```\n",
        "Let's try something.                  Permíteme intentarlo.\n",
        "Muiriel is 20 now.                    Ahora, Muiriel tiene 20 años.\n",
        "I just don't know what to say.        No sé qué decir.\n",
        "You are in my way.                    Estás en mi camino.\n",
        "Sometimes he can be a strange guy.    A veces él puede ser un chico raro.\n",
        "…\n",
        "```\n",
        "\n",
        "After the translations are aligned, the parallel corpus is fed into the preprocessing\n",
        "pipeline. Specific operations applied in this process differ from application to application,\n",
        "and from language to language, but the following steps are most common:\n",
        "\n",
        "- Filtering\n",
        "- Cleaning\n",
        "- Tokenization\n",
        "\n",
        "The Tatoeba dataset you downloaded and expanded earlier has already gone\n",
        "through all this preprocessing pipeline. Now you are ready to hand the dataset over to Fairseq. \n",
        "\n",
        "The first step is to tell Fairseq to convert the input files to the binary format so that the training script can read them easily, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsIsnLVUbbBn"
      },
      "source": [
        "!fairseq-preprocess --source-lang es --target-lang en \\\n",
        "      --trainpref data/mt/tatoeba.eng_spa.train.tok \\\n",
        "      --validpref data/mt/tatoeba.eng_spa.valid.tok \\\n",
        "      --testpref data/mt/tatoeba.eng_spa.test.tok \\\n",
        "      --destdir data/mt-bin \\\n",
        "      --thresholdsrc 3 \\\n",
        "      --thresholdtgt 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvAVsVMOdB2U"
      },
      "source": [
        "When this succeeds, you should see a message Wrote preprocessed data to `data/mt-bin` on your terminal. \n",
        "\n",
        "You should also find the following group of files under the `data/mt-bin` directory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sz70-f3SVr7_",
        "outputId": "3fb17470-765a-4f45-e929-e25737380301"
      },
      "source": [
        "!ls data/mt-bin/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict.en.txt\t   test.es-en.en.idx   train.es-en.en.idx  valid.es-en.en.idx\n",
            "dict.es.txt\t   test.es-en.es.bin   train.es-en.es.bin  valid.es-en.es.bin\n",
            "preprocess.log\t   test.es-en.es.idx   train.es-en.es.idx  valid.es-en.es.idx\n",
            "test.es-en.en.bin  train.es-en.en.bin  valid.es-en.en.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkk9K7Qndui-"
      },
      "source": [
        "One of the key functionalities of this preprocessing step is to build the vocabulary (called the dictionary in Fairseq), which is a mapping from vocabulary items (usually words) to their IDs. \n",
        "\n",
        "Notice the two dictionary files in the directory, dict.en.txt and\n",
        "dict.es.txt. MT deals with two languages, so the system needs to maintain two\n",
        "mappings, one for each language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ658gIcd6E9"
      },
      "source": [
        "##Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF6Ow5xqd6tc"
      },
      "source": [
        "Now that the train data is converted into the binary format, you are ready to train the MT model. \n",
        "\n",
        "At this point, you need to know only that you are training a model using the\n",
        "data stored in the directory specified by the first parameter (data/mt-bin) using an LSTM architecture (--arch lstm) with a bunch of other hyperparameters, and saving the results in data/mt-ckpt (short for “checkpoint”).\n",
        "\n",
        "Invoke the fairseq-train command with the directory where the\n",
        "binary files are located, along with several hyperparameters, as shown next:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb7QK6dWeOQ4"
      },
      "source": [
        "!fairseq-train data/mt-bin --arch lstm \\\n",
        "    --share-decoder-input-output-embed \\\n",
        "    --optimizer adam \\\n",
        "    --lr 1.0e-3 \\\n",
        "    --max-tokens 4096 \\\n",
        "    --save-dir data/mt-ckpt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxyjH4LDevB7"
      },
      "source": [
        "When you run this command, your terminal will show two types of progress bars\n",
        "alternatively—one for training and another for validating.\n",
        "\n",
        "For each epoch, the training process alternates two\n",
        "stages: training and validation. An epoch, a concept used in machine learning, means one pass through the entire train data.\n",
        "\n",
        "In the training stage, the loss is calculated using the training data, then the model parameters are adjusted in such a way that the new set of parameters lowers the loss. \n",
        "\n",
        "In the validation stage, the model parameters\n",
        "are fixed, and a separate dataset (validation set) is used to measure how well the model is performing against the dataset.\n",
        "\n",
        "<img src='https://github.com/rahiakela/natural-language-processing-research-and-practice/blob/main/real-world-natural-language-processing/6-sequence-to-sequence-models/images/2.png?raw=1' width='800'/>\n",
        "\n",
        "As the training continues, the train loss becomes smaller and smaller and gradually\n",
        "approaches zero, because this is exactly what we told the optimizer to do: decrease the\n",
        "loss as much as possible. Checking whether the train loss is decreasing steadily epoch\n",
        "after epoch is a good “sanity check” that your model and the training pipeline are working as expected.\n",
        "\n",
        "On the other hand, if you look at the validation loss, it goes down at first for several\n",
        "epochs, but after a certain point, it gradually goes back up, forming a U-shaped\n",
        "curve—a typical sign of overfitting. After several epochs of training, your model fits\n",
        "the train set so well that it begins to lose its generalizability on the validation set.\n",
        "\n",
        "If you see your validation loss starting to creep up, there’s little point keeping the training process running, because chances are, your model has already overfitted to the data to some extent. A common practice in such a situation, called early stopping, is to terminate the training. \n",
        "\n",
        "Specifically, if your validation loss is not improving for a certain\n",
        "number of epochs, you stop the training and use the model at the point when the\n",
        "validation loss was the lowest. The number of epochs you wait until the training is terminated is called patience. \n",
        "\n",
        "In practice, the metric you care about the most (such as\n",
        "BLEU) is used for early stopping instead of the validation loss.\n",
        "\n",
        "The graph indicates that the validation loss is lowest around epoch 8, so you can stop (by pressing `Ctrl + C`) the fairseq-train command after around 10 epochs; otherwise, the command would keep running indefinitely. Fairseq will automatically save the best model parameters (in terms of the validation loss) to the `checkpoint_best.pt` file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OX3UjR3jbs6"
      },
      "source": [
        "##Running the translator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYA8hKC_jcW5"
      },
      "source": [
        "After the model is trained, you can invoke the fairseq-interactive command to\n",
        "run your MT model on any input in an interactive way. \n",
        "\n",
        "After you see the prompt Type the input sentence and press return, try typing\n",
        "(or copying and pasting) the following Spanish sentences one by one:\n",
        "\n",
        "```\n",
        "Buenos días !\n",
        "¡ Hola !\n",
        "¿ Dónde está el baño ?\n",
        "¿ Hay habitaciones libres ?\n",
        "¿ Acepta tarjeta de crédito ?\n",
        "La cuenta , por favor .\n",
        "```\n",
        "\n",
        "You can run the command by specifying the binary file location and the model parameter file as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbN6sBSajktw",
        "outputId": "c2a46b6d-cec7-43db-d981-7a53052df5f0"
      },
      "source": [
        "!fairseq-interactive data/mt-bin \\\n",
        "    --path data/mt-ckpt/checkpoint_best.pt \\\n",
        "    --beam 5 \\\n",
        "    --source-lang es \\\n",
        "    --target-lang en"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-11-16 07:04:39 | INFO | fairseq_cli.interactive | Namespace(all_gather_list_size=16384, batch_size=1, batch_size_valid=None, beam=5, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, buffer_size=1, checkpoint_shard_count=1, checkpoint_suffix='', constraints=None, cpu=False, criterion='cross_entropy', curriculum=0, data='data/mt-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoding_format=None, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, empty_cache_freq=0, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', input='-', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, left_pad_source='True', left_pad_target='False', lenpen=1, lm_path=None, lm_weight=0.0, load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, memory_efficient_bf16=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', model_parallel_size=1, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, no_seed_provided=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer=None, path='data/mt-ckpt/checkpoint_best.pt', pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, prefix_size=0, print_alignment=False, print_step=False, profile=False, quantization_config_path=None, quiet=False, remove_bpe=None, replace_unk=None, required_batch_size_multiple=8, required_seq_len_multiple=1, results_path=None, retain_dropout=False, retain_dropout_modules=None, retain_iter_history=False, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, scoring='bleu', seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='es', target_lang='en', task='translation', temperature=1.0, tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, zero_sharding='none')\n",
            "2021-11-16 07:04:39 | INFO | fairseq.tasks.translation | [es] dictionary: 16832 types\n",
            "2021-11-16 07:04:39 | INFO | fairseq.tasks.translation | [en] dictionary: 11416 types\n",
            "2021-11-16 07:04:39 | INFO | fairseq_cli.interactive | loading model(s) from data/mt-ckpt/checkpoint_best.pt\n",
            "2021-11-16 07:04:41 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2021-11-16 07:04:41 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "¡ Buenos días !\n",
            "/usr/local/lib/python3.7/dist-packages/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/usr/local/lib/python3.7/dist-packages/fairseq/sequence_generator.py:651: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = idx // beam_size\n",
            "S-0\t¡ Buenos días !\n",
            "W-0\t0.101\tseconds\n",
            "H-0\t-0.25427359342575073\tGood morning !\n",
            "D-0\t-0.25427359342575073\tGood morning !\n",
            "P-0\t-0.8047 -0.1836 -0.0283 -0.0005\n",
            "¡ Hola !\n",
            "S-1\t¡ Hola !\n",
            "W-1\t0.019\tseconds\n",
            "H-1\t-0.11692231893539429\tHi !\n",
            "D-1\t-0.11692231893539429\tHi !\n",
            "P-1\t-0.3233 -0.0170 -0.0104\n",
            "¿ Dónde está el baño ?\n",
            "S-2\t¿ Dónde está el baño ?\n",
            "W-2\t0.032\tseconds\n",
            "H-2\t-0.30176666378974915\tWhere &apos;s the toilet ?\n",
            "D-2\t-0.30176666378974915\tWhere &apos;s the toilet ?\n",
            "P-2\t-0.0150 -0.3637 -0.0017 -1.4270 -0.0022 -0.0009\n",
            "¿ Hay habitaciones libres ?\n",
            "S-3\t¿ Hay habitaciones libres ?\n",
            "W-3\t0.038\tseconds\n",
            "H-3\t-0.593152642250061\tIs there a free rooms ?\n",
            "D-3\t-0.593152642250061\tIs there a free rooms ?\n",
            "P-3\t-0.1604 -0.0116 -2.1812 -1.0646 -0.7068 -0.0264 -0.0012\n",
            "¿ Acepta tarjeta de crédito ?\n",
            "S-4\t¿ Acepta tarjeta de crédito ?\n",
            "W-4\t0.037\tseconds\n",
            "H-4\t-0.07972174882888794\tDo you accept credit card ?\n",
            "D-4\t-0.07972174882888794\tDo you accept credit card ?\n",
            "P-4\t-0.0551 -0.0633 -0.3996 -0.0241 -0.0143 -0.0015 -0.0002\n",
            "La cuenta , por favor .\n",
            "S-5\tLa cuenta , por favor .\n",
            "W-5\t0.033\tseconds\n",
            "H-5\t-0.3058212697505951\tThe bill , please .\n",
            "D-5\t-0.3058212697505951\tThe bill , please .\n",
            "P-5\t-1.5694 -0.2349 -0.0128 -0.0026 -0.0147 -0.0004\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-interactive\", line 8, in <module>\n",
            "    sys.exit(cli_main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/interactive.py\", line 307, in cli_main\n",
            "    distributed_utils.call_main(args, main)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fairseq/distributed_utils.py\", line 301, in call_main\n",
            "    main(args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/interactive.py\", line 203, in main\n",
            "    for inputs in buffered_read(args.input, args.buffer_size):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/interactive.py\", line 42, in buffered_read\n",
            "    for src_str in h:\n",
            "  File \"/usr/lib/python3.7/fileinput.py\", line 252, in __next__\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb-zZJFak5b-"
      },
      "source": [
        "Most of the output sentences here are almost perfect, except the fourth one (I would translate to \"is there free rooms?\"). Even considering the fact that these sentences are all simple examples you can find in any travel Spanish phrasebook, this is not a bad start for a system built within an hour!"
      ]
    }
  ]
}