{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine-translator.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOSk6tsTABsvc8HYniQWLd+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-research-and-practice/blob/main/real-world-natural-language-processing/6-sequence-to-sequence-models/machine_translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKJ5NY0ZJiPD"
      },
      "source": [
        "##Machine Translator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3nTcGA3Jkzy"
      },
      "source": [
        "In this notebook, we are going to build a working MT system. Instead of writing any\n",
        "Python code to do that, we’ll make the most of existing MT frameworks. A number of\n",
        "open source frameworks make it easier to build MT systems, including [Moses](http://www.statmt.org/moses/) for SMT and [OpenNMT](http://opennmt.net/) for NMT.\n",
        "\n",
        "But, we will use [Fairseq](https://github.com/pytorch/fairseq), an NMT\n",
        "toolkit developed by Facebook that is becoming more and more popular among NLP\n",
        "practitioners these days.\n",
        "\n",
        "The following aspects make Fairseq a good choice for developing\n",
        "an NMT system quickly:\n",
        "\n",
        "- it is a modern framework that comes with a number\n",
        "of predefined state-of-the-art NMT models that you can use out of the box;\n",
        "- it is very extensible, meaning you can quickly implement your own model by following their API;\n",
        "- it is very fast, supporting multi-GPU and distributed training by default.\n",
        "\n",
        "Thanks to its powerful models, you can build a decent quality NMT system within a couple of hours."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OypcEydKKfC2"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLVwz4BMKgHT",
        "outputId": "0a9bdcb5-3e55-4895-bc3c-2b73f5200eca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip -q install fairseq"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 90 kB 8.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 145 kB 46.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 48.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 74 kB 3.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 44.3 MB/s \n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO2ncsqWKu3k"
      },
      "source": [
        "Let's download and expand the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ee7uVfRKxU2",
        "outputId": "ce9fcc38-4716-4fa1-c608-b841578c6bcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "mkdir -p data/mt\n",
        "wget https://realworldnlpbook.s3.amazonaws.com/data/mt/tatoeba.eng_spa.zip\n",
        "unzip tatoeba.eng_spa.zip -d data/mt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-16 06:27:51--  https://realworldnlpbook.s3.amazonaws.com/data/mt/tatoeba.eng_spa.zip\n",
            "Resolving realworldnlpbook.s3.amazonaws.com (realworldnlpbook.s3.amazonaws.com)... 52.216.225.240\n",
            "Connecting to realworldnlpbook.s3.amazonaws.com (realworldnlpbook.s3.amazonaws.com)|52.216.225.240|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19148555 (18M) [application/zip]\n",
            "Saving to: ‘tatoeba.eng_spa.zip’\n",
            "\n",
            "tatoeba.eng_spa.zip 100%[===================>]  18.26M  44.4MB/s    in 0.4s    \n",
            "\n",
            "2021-11-16 06:27:52 (44.4 MB/s) - ‘tatoeba.eng_spa.zip’ saved [19148555/19148555]\n",
            "\n",
            "Archive:  tatoeba.eng_spa.zip\n",
            "  inflating: data/mt/tatoeba.eng_spa.train.tok.en  \n",
            "  inflating: data/mt/tatoeba.eng_spa.train.tok.es  \n",
            "  inflating: data/mt/tatoeba.eng_spa.train.tsv  \n",
            "  inflating: data/mt/tatoeba.eng_spa.valid.tok.en  \n",
            "  inflating: data/mt/tatoeba.eng_spa.valid.tok.es  \n",
            "  inflating: data/mt/tatoeba.eng_spa.valid.tsv  \n",
            "  inflating: data/mt/tatoeba.eng_spa.tsv  \n",
            "  inflating: data/mt/tatoeba.eng_spa.test.tsv  \n",
            "  inflating: data/mt/tatoeba.eng_spa.test.tok.en  \n",
            "  inflating: data/mt/tatoeba.eng_spa.test.tok.es  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGqZISQeLc-A"
      },
      "source": [
        "The\n",
        "corpus consists of approximately 200,000 English sentences and their Spanish translations.\n",
        "I went ahead and already formatted the dataset so that you can use it without worrying about obtaining the data, tokenizing the text, and so on. The dataset is\n",
        "already split into train, validate, and test subsets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "788vSarZLf0U"
      },
      "source": [
        "##Preparing the datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qah-tp9yLibl"
      },
      "source": [
        "As we know, MT systems (both SMT and NMT) are machine learning\n",
        "models and thus are trained from data. The development process of MT systems looks similar to any other modern NLP systems.\n",
        "\n",
        "First, the training portion of the parallel corpus is preprocessed and used to train a set of NMT model candidates. \n",
        "\n",
        "Next, the validation portion is used to choose the best-performing model\n",
        "out of all the candidates. This process is called model selection.\n",
        "\n",
        "Finally, the best model is tested on the test portion of the dataset to obtain\n",
        "evaluation metrics, which reflect how good the model is.\n",
        "\n",
        "<img src='https://github.com/rahiakela/natural-language-processing-research-and-practice/blob/main/real-world-natural-language-processing/6-sequence-to-sequence-models/images/1.png?raw=1' width='800'/>\n",
        "\n",
        "The first step in MT development is preprocessing the dataset. But before preprocessing, you need to convert the dataset into an easy-to-use format, which is usually plain text in NLP.\n",
        "\n",
        "In practice, the raw data for training MT systems come in many different\n",
        "formats, for example, plain text files (if you are lucky), XML formats of proprietary\n",
        "software, PDF files, and database records. Your first job is to format the raw files so\n",
        "that source sentences and their target translations are aligned sentence by sentence.\n",
        "\n",
        "The resulting file is often a TSV file where each line is a tab-separated sentence pair, which looks like the following:\n",
        "\n",
        "```\n",
        "Let's try something.                  Permíteme intentarlo.\n",
        "Muiriel is 20 now.                    Ahora, Muiriel tiene 20 años.\n",
        "I just don't know what to say.        No sé qué decir.\n",
        "You are in my way.                    Estás en mi camino.\n",
        "Sometimes he can be a strange guy.    A veces él puede ser un chico raro.\n",
        "…\n",
        "```\n",
        "\n",
        "After the translations are aligned, the parallel corpus is fed into the preprocessing\n",
        "pipeline. Specific operations applied in this process differ from application to application,\n",
        "and from language to language, but the following steps are most common:\n",
        "\n",
        "- Filtering\n",
        "- Cleaning\n",
        "- Tokenization\n",
        "\n",
        "The Tatoeba dataset you downloaded and expanded earlier has already gone\n",
        "through all this preprocessing pipeline. Now you are ready to hand the dataset over to Fairseq. \n",
        "\n",
        "The first step is to tell Fairseq to convert the input files to the binary format so that the training script can read them easily, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsIsnLVUbbBn"
      },
      "source": [
        "!fairseq-preprocess --source-lang es --target-lang en \\\n",
        "      --trainpref data/mt/tatoeba.eng_spa.train.tok \\\n",
        "      --validpref data/mt/tatoeba.eng_spa.valid.tok \\\n",
        "      --testpref data/mt/tatoeba.eng_spa.test.tok \\\n",
        "      --destdir data/mt-bin \\\n",
        "      --thresholdsrc 3 \\\n",
        "      --thresholdtgt 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvAVsVMOdB2U"
      },
      "source": [
        "When this succeeds, you should see a message Wrote preprocessed data to `data/mt-bin` on your terminal. \n",
        "\n",
        "You should also find the following group of files under the `data/mt-bin` directory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz70-f3SVr7_",
        "outputId": "3fb17470-765a-4f45-e929-e25737380301",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!ls data/mt-bin/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict.en.txt\t   test.es-en.en.idx   train.es-en.en.idx  valid.es-en.en.idx\n",
            "dict.es.txt\t   test.es-en.es.bin   train.es-en.es.bin  valid.es-en.es.bin\n",
            "preprocess.log\t   test.es-en.es.idx   train.es-en.es.idx  valid.es-en.es.idx\n",
            "test.es-en.en.bin  train.es-en.en.bin  valid.es-en.en.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkk9K7Qndui-"
      },
      "source": [
        "One of the key functionalities of this preprocessing step is to build the vocabulary (called the dictionary in Fairseq), which is a mapping from vocabulary items (usually words) to their IDs. \n",
        "\n",
        "Notice the two dictionary files in the directory, dict.en.txt and\n",
        "dict.es.txt. MT deals with two languages, so the system needs to maintain two\n",
        "mappings, one for each language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ658gIcd6E9"
      },
      "source": [
        "##Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF6Ow5xqd6tc"
      },
      "source": [
        "Now that the train data is converted into the binary format, you are ready to train the MT model. \n",
        "\n",
        "At this point, you need to know only that you are training a model using the\n",
        "data stored in the directory specified by the first parameter (data/mt-bin) using an LSTM architecture (--arch lstm) with a bunch of other hyperparameters, and saving the results in data/mt-ckpt (short for “checkpoint”).\n",
        "\n",
        "Invoke the fairseq-train command with the directory where the\n",
        "binary files are located, along with several hyperparameters, as shown next:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb7QK6dWeOQ4",
        "outputId": "95a0b402-c959-47fc-f3fb-98b0261e4842",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!fairseq-train data/mt-bin --arch lstm \\\n",
        "    --share-decoder-input-output-embed \\\n",
        "    --optimizer adam \\\n",
        "    --lr 1.0e-3 \\\n",
        "    --max-tokens 4096 \\\n",
        "    --save-dir data/mt-ckpt"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-11-16 06:41:09 | INFO | fairseq_cli.train | Namespace(adam_betas='(0.9, 0.999)', adam_eps=1e-08, adaptive_softmax_cutoff='10000,50000,200000', all_gather_list_size=16384, arch='lstm', batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, cpu=False, criterion='cross_entropy', curriculum=0, data='data/mt-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoder_attention='1', decoder_dropout_in=0.1, decoder_dropout_out=0.1, decoder_embed_dim=512, decoder_embed_path=None, decoder_freeze_embed=False, decoder_hidden_size=512, decoder_layers=1, decoder_out_embed_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', dropout=0.1, empty_cache_freq=0, encoder_bidirectional=False, encoder_dropout_in=0.1, encoder_dropout_out=0.1, encoder_embed_dim=512, encoder_embed_path=None, encoder_freeze_embed=False, encoder_hidden_size=512, encoder_layers=1, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, left_pad_source='True', left_pad_target='False', load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.001], lr_scheduler='fixed', lr_shrink=0.1, max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1.0, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='data/mt-ckpt', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_time_hours=0, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, weight_decay=0.0, zero_sharding='none')\n",
            "2021-11-16 06:41:09 | INFO | fairseq.tasks.translation | [es] dictionary: 16832 types\n",
            "2021-11-16 06:41:09 | INFO | fairseq.tasks.translation | [en] dictionary: 11416 types\n",
            "2021-11-16 06:41:09 | INFO | fairseq.data.data_utils | loaded 19581 examples from: data/mt-bin/valid.es-en.es\n",
            "2021-11-16 06:41:09 | INFO | fairseq.data.data_utils | loaded 19581 examples from: data/mt-bin/valid.es-en.en\n",
            "2021-11-16 06:41:09 | INFO | fairseq.tasks.translation | data/mt-bin valid es-en 19581 examples\n",
            "2021-11-16 06:41:09 | INFO | fairseq_cli.train | LSTMModel(\n",
            "  (encoder): LSTMEncoder(\n",
            "    (dropout_in_module): FairseqDropout()\n",
            "    (dropout_out_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(16832, 512, padding_idx=1)\n",
            "    (lstm): LSTM(512, 512)\n",
            "  )\n",
            "  (decoder): LSTMDecoder(\n",
            "    (dropout_in_module): FairseqDropout()\n",
            "    (dropout_out_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(11416, 512, padding_idx=1)\n",
            "    (layers): ModuleList(\n",
            "      (0): LSTMCell(1024, 512)\n",
            "    )\n",
            "    (attention): AttentionLayer(\n",
            "      (input_proj): Linear(in_features=512, out_features=512, bias=False)\n",
            "      (output_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2021-11-16 06:41:09 | INFO | fairseq_cli.train | task: translation (TranslationTask)\n",
            "2021-11-16 06:41:09 | INFO | fairseq_cli.train | model: lstm (LSTMModel)\n",
            "2021-11-16 06:41:09 | INFO | fairseq_cli.train | criterion: cross_entropy (CrossEntropyCriterion)\n",
            "2021-11-16 06:41:09 | INFO | fairseq_cli.train | num. model params: 20500480 (num. trained: 20500480)\n",
            "2021-11-16 06:41:24 | INFO | fairseq.trainer | detected shared parameter: decoder.attention.input_proj.bias <- decoder.attention.output_proj.bias\n",
            "2021-11-16 06:41:24 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2021-11-16 06:41:24 | INFO | fairseq.utils | rank   0: capabilities =  3.7  ; total memory = 11.173 GB ; name = Tesla K80                               \n",
            "2021-11-16 06:41:24 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2021-11-16 06:41:24 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2021-11-16 06:41:24 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and max sentences per GPU = None\n",
            "2021-11-16 06:41:24 | INFO | fairseq.trainer | no existing checkpoint found data/mt-ckpt/checkpoint_last.pt\n",
            "2021-11-16 06:41:24 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2021-11-16 06:41:24 | INFO | fairseq.data.data_utils | loaded 156647 examples from: data/mt-bin/train.es-en.es\n",
            "2021-11-16 06:41:24 | INFO | fairseq.data.data_utils | loaded 156647 examples from: data/mt-bin/train.es-en.en\n",
            "2021-11-16 06:41:24 | INFO | fairseq.tasks.translation | data/mt-bin train es-en 156647 examples\n",
            "epoch 001:   0% 0/389 [00:00<?, ?it/s]2021-11-16 06:41:24 | INFO | fairseq.trainer | begin training epoch 1\n",
            "/usr/local/lib/python3.7/dist-packages/fairseq/utils.py:342: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  \"amp_C fused kernels unavailable, disabling multi_tensor_l2norm; \"\n",
            "epoch 001: 100% 388/389 [01:20<00:00,  5.18it/s, loss=5.584, ppl=47.97, wps=17379.3, ups=4.78, wpb=3632.3, bsz=384.1, num_updates=300, lr=0.001, gnorm=0.878, train_wall=21, wall=62]2021-11-16 06:42:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/55 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   2% 1/55 [00:00<00:08,  6.51it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   5% 3/55 [00:00<00:05,  9.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   9% 5/55 [00:00<00:04, 11.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  13% 7/55 [00:00<00:03, 13.07it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  16% 9/55 [00:00<00:03, 14.25it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 11/55 [00:00<00:03, 14.40it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  24% 13/55 [00:00<00:02, 14.56it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  27% 15/55 [00:01<00:02, 13.92it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  31% 17/55 [00:01<00:02, 13.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  35% 19/55 [00:01<00:02, 12.79it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  38% 21/55 [00:01<00:02, 13.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  42% 23/55 [00:01<00:02, 13.99it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  45% 25/55 [00:01<00:02, 14.09it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  49% 27/55 [00:02<00:01, 14.15it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  53% 29/55 [00:02<00:01, 14.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  56% 31/55 [00:02<00:01, 14.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  60% 33/55 [00:02<00:01, 14.41it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  64% 35/55 [00:02<00:01, 14.41it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 37/55 [00:02<00:01, 14.39it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  71% 39/55 [00:02<00:01, 14.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  75% 41/55 [00:03<00:01, 13.84it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  78% 43/55 [00:03<00:00, 13.78it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  82% 45/55 [00:03<00:00, 13.32it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  85% 47/55 [00:03<00:00, 13.14it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 49/55 [00:03<00:00, 12.82it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  93% 51/55 [00:03<00:00, 12.48it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  96% 53/55 [00:03<00:00, 11.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 55/55 [00:04<00:00, 10.11it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-11-16 06:42:49 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 4.239 | ppl 18.88 | wps 43201.9 | wpb 3315.7 | bsz 356 | num_updates 389\n",
            "2021-11-16 06:42:49 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-11-16 06:43:03 | INFO | fairseq.checkpoint_utils | saved checkpoint data/mt-ckpt/checkpoint1.pt (epoch 1 @ 389 updates, score 4.239) (writing took 13.731925856999851 seconds)\n",
            "2021-11-16 06:43:03 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2021-11-16 06:43:03 | INFO | train | epoch 001 | loss 6.206 | ppl 73.84 | wps 14725.9 | ups 3.94 | wpb 3734.3 | bsz 402.7 | num_updates 389 | lr 0.001 | gnorm 1.048 | train_wall 80 | wall 99\n",
            "epoch 002:   0% 0/389 [00:00<?, ?it/s]2021-11-16 06:43:03 | INFO | fairseq.trainer | begin training epoch 2\n",
            "epoch 002: 100% 388/389 [01:21<00:00,  5.08it/s, loss=3.006, ppl=8.04, wps=17357.3, ups=4.73, wpb=3671.3, bsz=409.2, num_updates=700, lr=0.001, gnorm=0.566, train_wall=21, wall=165]2021-11-16 06:44:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/55 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   2% 1/55 [00:00<00:08,  6.09it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   5% 3/55 [00:00<00:05, 10.03it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   9% 5/55 [00:00<00:04, 12.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  13% 7/55 [00:00<00:03, 12.46it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  16% 9/55 [00:00<00:03, 12.56it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 11/55 [00:00<00:03, 12.52it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24% 13/55 [00:01<00:03, 12.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  27% 15/55 [00:01<00:02, 13.67it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  31% 17/55 [00:01<00:02, 14.05it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  35% 19/55 [00:01<00:02, 14.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  38% 21/55 [00:01<00:02, 14.43it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  42% 23/55 [00:01<00:02, 14.41it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  45% 25/55 [00:01<00:02, 14.72it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  49% 27/55 [00:02<00:01, 14.47it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  53% 29/55 [00:02<00:01, 14.45it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  56% 31/55 [00:02<00:01, 14.27it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  60% 33/55 [00:02<00:01, 14.05it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  64% 35/55 [00:02<00:01, 14.18it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  67% 37/55 [00:02<00:01, 14.27it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  71% 39/55 [00:02<00:01, 13.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  75% 41/55 [00:03<00:01, 13.77it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  78% 43/55 [00:03<00:00, 13.69it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  82% 45/55 [00:03<00:00, 13.26it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  85% 47/55 [00:03<00:00, 13.07it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  89% 49/55 [00:03<00:00, 12.82it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  93% 51/55 [00:03<00:00, 12.43it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  96% 53/55 [00:04<00:00, 11.78it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 55/55 [00:04<00:00, 10.10it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-11-16 06:44:30 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 2.677 | ppl 6.4 | wps 43160.5 | wpb 3315.7 | bsz 356 | num_updates 778 | best_loss 2.677\n",
            "2021-11-16 06:44:30 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-11-16 06:44:44 | INFO | fairseq.checkpoint_utils | saved checkpoint data/mt-ckpt/checkpoint2.pt (epoch 2 @ 778 updates, score 2.677) (writing took 14.309260341000027 seconds)\n",
            "2021-11-16 06:44:44 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2021-11-16 06:44:44 | INFO | train | epoch 002 | loss 3.346 | ppl 10.17 | wps 14408.2 | ups 3.86 | wpb 3734.3 | bsz 402.7 | num_updates 778 | lr 0.001 | gnorm 0.628 | train_wall 81 | wall 200\n",
            "epoch 003:   0% 0/389 [00:00<?, ?it/s]2021-11-16 06:44:44 | INFO | fairseq.trainer | begin training epoch 3\n",
            "epoch 003: 100% 388/389 [01:21<00:00,  4.37it/s, loss=2.144, ppl=4.42, wps=18544.1, ups=4.86, wpb=3818.1, bsz=419.2, num_updates=1100, lr=0.001, gnorm=0.479, train_wall=20, wall=268]2021-11-16 06:46:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/55 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   2% 1/55 [00:00<00:10,  5.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   5% 3/55 [00:00<00:05,  9.31it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   9% 5/55 [00:00<00:04, 12.09it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  13% 7/55 [00:00<00:04, 11.68it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  16% 9/55 [00:00<00:03, 13.06it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  20% 11/55 [00:00<00:03, 13.69it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  24% 13/55 [00:01<00:02, 14.18it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  27% 15/55 [00:01<00:02, 14.52it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  31% 17/55 [00:01<00:02, 14.78it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  35% 19/55 [00:01<00:02, 14.79it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  38% 21/55 [00:01<00:02, 14.85it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  42% 23/55 [00:01<00:02, 14.87it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  45% 25/55 [00:01<00:01, 15.09it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  49% 27/55 [00:01<00:01, 14.71it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  53% 29/55 [00:02<00:01, 14.69it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  56% 31/55 [00:02<00:01, 14.70it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  60% 33/55 [00:02<00:01, 14.53it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  64% 35/55 [00:02<00:01, 14.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  67% 37/55 [00:02<00:01, 14.47it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  71% 39/55 [00:02<00:01, 14.05it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  75% 41/55 [00:02<00:01, 13.75it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  78% 43/55 [00:03<00:00, 13.61it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  82% 45/55 [00:03<00:00, 13.15it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  85% 47/55 [00:03<00:00, 13.03it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  89% 49/55 [00:03<00:00, 12.73it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  93% 51/55 [00:03<00:00, 12.44it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  96% 53/55 [00:03<00:00, 11.79it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 55/55 [00:04<00:00, 10.06it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-11-16 06:46:10 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 2.188 | ppl 4.56 | wps 43951.6 | wpb 3315.7 | bsz 356 | num_updates 1167 | best_loss 2.188\n",
            "2021-11-16 06:46:10 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-11-16 06:46:25 | INFO | fairseq.checkpoint_utils | saved checkpoint data/mt-ckpt/checkpoint3.pt (epoch 3 @ 1167 updates, score 2.188) (writing took 14.573856439000338 seconds)\n",
            "2021-11-16 06:46:25 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2021-11-16 06:46:25 | INFO | train | epoch 003 | loss 2.286 | ppl 4.88 | wps 14408.9 | ups 3.86 | wpb 3734.3 | bsz 402.7 | num_updates 1167 | lr 0.001 | gnorm 0.509 | train_wall 81 | wall 301\n",
            "epoch 004:   0% 0/389 [00:00<?, ?it/s]2021-11-16 06:46:25 | INFO | fairseq.trainer | begin training epoch 4\n",
            "epoch 004: 100% 388/389 [01:21<00:00,  5.12it/s, loss=1.734, ppl=3.33, wps=18237, ups=4.86, wpb=3756.3, bsz=409, num_updates=1500, lr=0.001, gnorm=0.434, train_wall=20, wall=371]2021-11-16 06:47:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/55 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   2% 1/55 [00:00<00:09,  5.78it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   5% 3/55 [00:00<00:06,  8.47it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   9% 5/55 [00:00<00:04, 11.40it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  13% 7/55 [00:00<00:04, 11.96it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  16% 9/55 [00:00<00:03, 12.67it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  20% 11/55 [00:00<00:03, 12.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  24% 13/55 [00:01<00:03, 12.84it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  27% 15/55 [00:01<00:03, 13.16it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  31% 17/55 [00:01<00:02, 14.00it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  35% 19/55 [00:01<00:02, 13.93it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  38% 21/55 [00:01<00:02, 14.05it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  42% 23/55 [00:01<00:02, 14.29it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  45% 25/55 [00:01<00:02, 14.70it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  49% 27/55 [00:02<00:01, 14.58it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  53% 29/55 [00:02<00:01, 14.53it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  56% 31/55 [00:02<00:01, 14.49it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  60% 33/55 [00:02<00:01, 14.48it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  64% 35/55 [00:02<00:01, 14.67it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  67% 37/55 [00:02<00:01, 14.46it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  71% 39/55 [00:02<00:01, 14.10it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  75% 41/55 [00:03<00:01, 13.80it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  78% 43/55 [00:03<00:00, 13.72it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  82% 45/55 [00:03<00:00, 13.18it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  85% 47/55 [00:03<00:00, 13.05it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  89% 49/55 [00:03<00:00, 12.74it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  93% 51/55 [00:03<00:00, 12.39it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  96% 53/55 [00:04<00:00, 11.80it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 55/55 [00:04<00:00, 10.05it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-11-16 06:47:51 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 1.933 | ppl 3.82 | wps 43012.9 | wpb 3315.7 | bsz 356 | num_updates 1556 | best_loss 1.933\n",
            "2021-11-16 06:47:51 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-11-16 06:48:06 | INFO | fairseq.checkpoint_utils | saved checkpoint data/mt-ckpt/checkpoint4.pt (epoch 4 @ 1556 updates, score 1.933) (writing took 14.68649861699987 seconds)\n",
            "2021-11-16 06:48:06 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2021-11-16 06:48:06 | INFO | train | epoch 004 | loss 1.803 | ppl 3.49 | wps 14393.8 | ups 3.85 | wpb 3734.3 | bsz 402.7 | num_updates 1556 | lr 0.001 | gnorm 0.458 | train_wall 81 | wall 402\n",
            "epoch 005:   0% 0/389 [00:00<?, ?it/s]2021-11-16 06:48:06 | INFO | fairseq.trainer | begin training epoch 5\n",
            "epoch 005: 100% 388/389 [01:21<00:00,  4.66it/s, loss=1.486, ppl=2.8, wps=18411, ups=4.86, wpb=3786.9, bsz=419.9, num_updates=1900, lr=0.001, gnorm=0.409, train_wall=20, wall=474]2021-11-16 06:49:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/55 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   2% 1/55 [00:00<00:08,  6.20it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   5% 3/55 [00:00<00:05, 10.21it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   9% 5/55 [00:00<00:03, 12.72it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  13% 7/55 [00:00<00:03, 12.67it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  16% 9/55 [00:00<00:03, 12.95it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  20% 11/55 [00:00<00:03, 12.68it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  24% 13/55 [00:01<00:03, 13.03it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  27% 15/55 [00:01<00:02, 13.64it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  31% 17/55 [00:01<00:02, 14.03it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  35% 19/55 [00:01<00:02, 13.65it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  38% 21/55 [00:01<00:02, 14.01it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  42% 23/55 [00:01<00:02, 14.25it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  45% 25/55 [00:01<00:02, 14.65it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  49% 27/55 [00:02<00:01, 14.45it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  53% 29/55 [00:02<00:01, 14.32it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  56% 31/55 [00:02<00:01, 14.42it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  60% 33/55 [00:02<00:01, 14.48it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  64% 35/55 [00:02<00:01, 14.68it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  67% 37/55 [00:02<00:01, 14.54it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  71% 39/55 [00:02<00:01, 13.78it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  75% 41/55 [00:03<00:01, 13.66it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  78% 43/55 [00:03<00:00, 13.67it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  82% 45/55 [00:03<00:00, 13.17it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  85% 47/55 [00:03<00:00, 13.01it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  89% 49/55 [00:03<00:00, 12.73it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  93% 51/55 [00:03<00:00, 12.39it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  96% 53/55 [00:04<00:00, 11.78it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 55/55 [00:04<00:00, 10.10it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-11-16 06:49:32 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 1.818 | ppl 3.53 | wps 43197.6 | wpb 3315.7 | bsz 356 | num_updates 1945 | best_loss 1.818\n",
            "2021-11-16 06:49:32 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-11-16 06:49:46 | INFO | fairseq.checkpoint_utils | saved checkpoint data/mt-ckpt/checkpoint5.pt (epoch 5 @ 1945 updates, score 1.818) (writing took 14.54081500700022 seconds)\n",
            "2021-11-16 06:49:46 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2021-11-16 06:49:46 | INFO | train | epoch 005 | loss 1.523 | ppl 2.87 | wps 14426.8 | ups 3.86 | wpb 3734.3 | bsz 402.7 | num_updates 1945 | lr 0.001 | gnorm 0.428 | train_wall 81 | wall 502\n",
            "epoch 006:   0% 0/389 [00:00<?, ?it/s]2021-11-16 06:49:46 | INFO | fairseq.trainer | begin training epoch 6\n",
            "epoch 006: 100% 388/389 [01:21<00:00,  5.22it/s, loss=1.359, ppl=2.57, wps=16961.9, ups=4.56, wpb=3716.1, bsz=400.8, num_updates=2300, lr=0.001, gnorm=0.419, train_wall=22, wall=578]2021-11-16 06:51:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/55 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   2% 1/55 [00:00<00:09,  5.90it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   5% 3/55 [00:00<00:04, 11.11it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   9% 5/55 [00:00<00:03, 13.36it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  13% 7/55 [00:00<00:03, 13.22it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  16% 9/55 [00:00<00:03, 14.26it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  20% 11/55 [00:00<00:03, 14.54it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  24% 13/55 [00:00<00:02, 14.76it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  27% 15/55 [00:01<00:02, 14.92it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  31% 17/55 [00:01<00:02, 15.08it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  35% 19/55 [00:01<00:02, 15.00it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  38% 21/55 [00:01<00:02, 15.00it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  42% 23/55 [00:01<00:02, 14.91it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  45% 25/55 [00:01<00:02, 14.99it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  49% 27/55 [00:01<00:01, 14.83it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  53% 29/55 [00:02<00:01, 14.76it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  56% 31/55 [00:02<00:01, 14.76it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  60% 33/55 [00:02<00:01, 14.61it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  64% 35/55 [00:02<00:01, 14.73it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  67% 37/55 [00:02<00:01, 14.63it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  71% 39/55 [00:02<00:01, 14.13it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  75% 41/55 [00:02<00:01, 13.85it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  78% 43/55 [00:03<00:00, 13.76it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  82% 45/55 [00:03<00:00, 13.31it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  85% 47/55 [00:03<00:00, 13.11it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  89% 49/55 [00:03<00:00, 12.82it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  93% 51/55 [00:03<00:00, 12.50it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  96% 53/55 [00:03<00:00, 11.90it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 55/55 [00:04<00:00, 10.15it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-11-16 06:51:12 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 1.78 | ppl 3.43 | wps 44783.7 | wpb 3315.7 | bsz 356 | num_updates 2334 | best_loss 1.78\n",
            "2021-11-16 06:51:12 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-11-16 06:51:27 | INFO | fairseq.checkpoint_utils | saved checkpoint data/mt-ckpt/checkpoint6.pt (epoch 6 @ 2334 updates, score 1.78) (writing took 14.366349248999995 seconds)\n",
            "2021-11-16 06:51:27 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2021-11-16 06:51:27 | INFO | train | epoch 006 | loss 1.329 | ppl 2.51 | wps 14463.2 | ups 3.87 | wpb 3734.3 | bsz 402.7 | num_updates 2334 | lr 0.001 | gnorm 0.406 | train_wall 81 | wall 603\n",
            "epoch 007:   0% 0/389 [00:00<?, ?it/s]2021-11-16 06:51:27 | INFO | fairseq.trainer | begin training epoch 7\n",
            "epoch 007: 100% 388/389 [01:21<00:00,  4.51it/s, loss=1.269, ppl=2.41, wps=17320.5, ups=4.62, wpb=3749.4, bsz=424.6, num_updates=2700, lr=0.001, gnorm=0.491, train_wall=22, wall=680]2021-11-16 06:52:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/55 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   2% 1/55 [00:00<00:09,  5.57it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   5% 3/55 [00:00<00:05,  9.41it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   9% 5/55 [00:00<00:04, 11.93it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  13% 7/55 [00:00<00:04, 11.79it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  16% 9/55 [00:00<00:03, 12.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  20% 11/55 [00:00<00:03, 13.63it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  24% 13/55 [00:01<00:02, 14.16it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  27% 15/55 [00:01<00:02, 14.52it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  31% 17/55 [00:01<00:02, 14.81it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  35% 19/55 [00:01<00:02, 14.81it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  38% 21/55 [00:01<00:02, 14.82it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  42% 23/55 [00:01<00:02, 14.80it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  45% 25/55 [00:01<00:01, 15.09it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  49% 27/55 [00:01<00:01, 14.92it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  53% 29/55 [00:02<00:01, 14.80it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  56% 31/55 [00:02<00:01, 14.78it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  60% 33/55 [00:02<00:01, 14.74it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  64% 35/55 [00:02<00:01, 14.83it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  67% 37/55 [00:02<00:01, 14.71it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  71% 39/55 [00:02<00:01, 14.24it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  75% 41/55 [00:02<00:01, 13.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  78% 43/55 [00:03<00:00, 13.65it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  82% 45/55 [00:03<00:00, 13.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  85% 47/55 [00:03<00:00, 13.08it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  89% 49/55 [00:03<00:00, 12.82it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  93% 51/55 [00:03<00:00, 12.47it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  96% 53/55 [00:03<00:00, 11.80it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset: 100% 55/55 [00:04<00:00, 10.10it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-11-16 06:52:53 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 1.758 | ppl 3.38 | wps 44087.9 | wpb 3315.7 | bsz 356 | num_updates 2723 | best_loss 1.758\n",
            "2021-11-16 06:52:53 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-11-16 06:53:07 | INFO | fairseq.checkpoint_utils | saved checkpoint data/mt-ckpt/checkpoint7.pt (epoch 7 @ 2723 updates, score 1.758) (writing took 14.367629615000169 seconds)\n",
            "2021-11-16 06:53:07 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2021-11-16 06:53:07 | INFO | train | epoch 007 | loss 1.205 | ppl 2.31 | wps 14452.4 | ups 3.87 | wpb 3734.3 | bsz 402.7 | num_updates 2723 | lr 0.001 | gnorm 0.414 | train_wall 81 | wall 703\n",
            "epoch 008:   0% 0/389 [00:00<?, ?it/s]2021-11-16 06:53:07 | INFO | fairseq.trainer | begin training epoch 8\n",
            "epoch 008: 100% 388/389 [01:21<00:00,  4.94it/s, loss=1.132, ppl=2.19, wps=17931.7, ups=4.88, wpb=3671.9, bsz=390.4, num_updates=3100, lr=0.001, gnorm=0.388, train_wall=20, wall=783]2021-11-16 06:54:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/55 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   2% 1/55 [00:00<00:09,  5.94it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   5% 3/55 [00:00<00:05,  9.85it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   9% 5/55 [00:00<00:03, 12.56it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  13% 7/55 [00:00<00:03, 13.79it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  16% 9/55 [00:00<00:03, 14.49it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  20% 11/55 [00:00<00:03, 14.37it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  24% 13/55 [00:00<00:02, 14.31it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  27% 15/55 [00:01<00:02, 14.44it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  31% 17/55 [00:01<00:02, 14.76it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  35% 19/55 [00:01<00:02, 14.77it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  38% 21/55 [00:01<00:02, 14.85it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  42% 23/55 [00:01<00:02, 14.84it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  45% 25/55 [00:01<00:01, 15.07it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  49% 27/55 [00:01<00:01, 14.87it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  53% 29/55 [00:02<00:01, 14.78it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  56% 31/55 [00:02<00:01, 14.55it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  60% 33/55 [00:02<00:01, 14.60it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  64% 35/55 [00:02<00:01, 14.69it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  67% 37/55 [00:02<00:01, 14.45it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  71% 39/55 [00:02<00:01, 14.03it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  75% 41/55 [00:02<00:01, 13.71it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  78% 43/55 [00:03<00:00, 13.61it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  82% 45/55 [00:03<00:00, 13.27it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  85% 47/55 [00:03<00:00, 13.14it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  89% 49/55 [00:03<00:00, 12.80it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  93% 51/55 [00:03<00:00, 12.41it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  96% 53/55 [00:03<00:00, 11.80it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100% 55/55 [00:04<00:00, 10.09it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-11-16 06:54:33 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 1.729 | ppl 3.31 | wps 44417.1 | wpb 3315.7 | bsz 356 | num_updates 3112 | best_loss 1.729\n",
            "2021-11-16 06:54:33 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-11-16 06:54:48 | INFO | fairseq.checkpoint_utils | saved checkpoint data/mt-ckpt/checkpoint8.pt (epoch 8 @ 3112 updates, score 1.729) (writing took 14.549158382000314 seconds)\n",
            "2021-11-16 06:54:48 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2021-11-16 06:54:48 | INFO | train | epoch 008 | loss 1.096 | ppl 2.14 | wps 14423.9 | ups 3.86 | wpb 3734.3 | bsz 402.7 | num_updates 3112 | lr 0.001 | gnorm 0.389 | train_wall 81 | wall 804\n",
            "epoch 009:   0% 0/389 [00:00<?, ?it/s]2021-11-16 06:54:48 | INFO | fairseq.trainer | begin training epoch 9\n",
            "epoch 009: 100% 388/389 [01:21<00:00,  4.85it/s, loss=0.993, ppl=1.99, wps=18037.4, ups=4.74, wpb=3805.9, bsz=414.2, num_updates=3500, lr=0.001, gnorm=0.364, train_wall=21, wall=886]2021-11-16 06:56:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/55 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   2% 1/55 [00:00<00:09,  5.81it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   5% 3/55 [00:00<00:05,  8.72it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   9% 5/55 [00:00<00:04, 11.19it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  13% 7/55 [00:00<00:04, 10.73it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  16% 9/55 [00:00<00:03, 12.48it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  20% 11/55 [00:00<00:03, 13.32it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  24% 13/55 [00:01<00:03, 12.47it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  27% 15/55 [00:01<00:03, 13.09it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  31% 17/55 [00:01<00:02, 13.77it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  35% 19/55 [00:01<00:02, 14.18it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  38% 21/55 [00:01<00:02, 14.43it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  42% 23/55 [00:01<00:02, 14.53it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  45% 25/55 [00:01<00:02, 14.65it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  49% 27/55 [00:02<00:01, 14.55it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  53% 29/55 [00:02<00:01, 14.57it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  56% 31/55 [00:02<00:01, 14.62it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  60% 33/55 [00:02<00:01, 14.60it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  64% 35/55 [00:02<00:01, 14.72it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  67% 37/55 [00:02<00:01, 14.59it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  71% 39/55 [00:02<00:01, 14.22it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  75% 41/55 [00:03<00:01, 13.95it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  78% 43/55 [00:03<00:00, 13.80it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  82% 45/55 [00:03<00:00, 13.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  85% 47/55 [00:03<00:00, 13.16it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  89% 49/55 [00:03<00:00, 12.84it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  93% 51/55 [00:03<00:00, 12.45it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  96% 53/55 [00:04<00:00, 11.81it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset: 100% 55/55 [00:04<00:00, 10.15it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-11-16 06:56:14 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 1.741 | ppl 3.34 | wps 43101.9 | wpb 3315.7 | bsz 356 | num_updates 3501 | best_loss 1.729\n",
            "2021-11-16 06:56:14 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-11-16 06:56:23 | INFO | fairseq.checkpoint_utils | saved checkpoint data/mt-ckpt/checkpoint9.pt (epoch 9 @ 3501 updates, score 1.741) (writing took 8.335975699999835 seconds)\n",
            "2021-11-16 06:56:23 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2021-11-16 06:56:23 | INFO | train | epoch 009 | loss 0.984 | ppl 1.98 | wps 15355.7 | ups 4.11 | wpb 3734.3 | bsz 402.7 | num_updates 3501 | lr 0.001 | gnorm 0.379 | train_wall 81 | wall 899\n",
            "epoch 010:   0% 0/389 [00:00<?, ?it/s]2021-11-16 06:56:23 | INFO | fairseq.trainer | begin training epoch 10\n",
            "epoch 010: 100% 388/389 [01:21<00:00,  4.99it/s, loss=0.958, ppl=1.94, wps=17338.3, ups=4.78, wpb=3627.9, bsz=389.6, num_updates=3800, lr=0.001, gnorm=0.4, train_wall=21, wall=962]2021-11-16 06:57:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/55 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   2% 1/55 [00:00<00:09,  5.85it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   5% 3/55 [00:00<00:05,  8.72it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   9% 5/55 [00:00<00:04, 11.39it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  13% 7/55 [00:00<00:04, 11.45it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  16% 9/55 [00:00<00:03, 12.96it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  20% 11/55 [00:00<00:03, 13.65it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  24% 13/55 [00:01<00:02, 14.16it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  27% 15/55 [00:01<00:02, 14.50it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  31% 17/55 [00:01<00:02, 14.77it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  35% 19/55 [00:01<00:02, 14.46it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  38% 21/55 [00:01<00:02, 14.54it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  42% 23/55 [00:01<00:02, 14.52it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  45% 25/55 [00:01<00:02, 14.80it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  49% 27/55 [00:02<00:01, 14.42it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  53% 29/55 [00:02<00:01, 14.35it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  56% 31/55 [00:02<00:01, 14.45it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  60% 33/55 [00:02<00:01, 14.38it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  64% 35/55 [00:02<00:01, 14.61it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  67% 37/55 [00:02<00:01, 14.49it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  71% 39/55 [00:02<00:01, 14.11it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  75% 41/55 [00:02<00:01, 13.90it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  78% 43/55 [00:03<00:00, 13.79it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  82% 45/55 [00:03<00:00, 13.26it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  85% 47/55 [00:03<00:00, 13.06it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  89% 49/55 [00:03<00:00, 12.78it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  93% 51/55 [00:03<00:00, 12.45it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  96% 53/55 [00:03<00:00, 11.84it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset: 100% 55/55 [00:04<00:00, 10.11it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-11-16 06:57:49 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 1.754 | ppl 3.37 | wps 43554.5 | wpb 3315.7 | bsz 356 | num_updates 3890 | best_loss 1.729\n",
            "2021-11-16 06:57:49 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-11-16 06:57:57 | INFO | fairseq.checkpoint_utils | saved checkpoint data/mt-ckpt/checkpoint10.pt (epoch 10 @ 3890 updates, score 1.754) (writing took 8.28342342399992 seconds)\n",
            "2021-11-16 06:57:57 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2021-11-16 06:57:57 | INFO | train | epoch 010 | loss 0.901 | ppl 1.87 | wps 15328.1 | ups 4.1 | wpb 3734.3 | bsz 402.7 | num_updates 3890 | lr 0.001 | gnorm 0.377 | train_wall 82 | wall 993\n",
            "epoch 011:   0% 0/389 [00:00<?, ?it/s]2021-11-16 06:57:57 | INFO | fairseq.trainer | begin training epoch 11\n",
            "epoch 011: 100% 388/389 [01:21<00:00,  5.04it/s, loss=0.827, ppl=1.77, wps=18384.8, ups=4.89, wpb=3760.8, bsz=428.2, num_updates=4200, lr=0.001, gnorm=0.368, train_wall=20, wall=1059]2021-11-16 06:59:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/55 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   2% 1/55 [00:00<00:08,  6.07it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   5% 3/55 [00:00<00:04, 11.22it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   9% 5/55 [00:00<00:03, 13.19it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  13% 7/55 [00:00<00:03, 14.17it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  16% 9/55 [00:00<00:03, 13.97it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  20% 11/55 [00:00<00:03, 14.63it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  24% 13/55 [00:00<00:03, 13.91it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  27% 15/55 [00:01<00:02, 13.42it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  31% 17/55 [00:01<00:02, 14.20it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  35% 19/55 [00:01<00:02, 14.45it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  38% 21/55 [00:01<00:02, 14.62it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  42% 23/55 [00:01<00:02, 14.58it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  45% 25/55 [00:01<00:02, 14.94it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  49% 27/55 [00:01<00:01, 14.77it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  53% 29/55 [00:02<00:01, 14.77it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  56% 31/55 [00:02<00:01, 14.78it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  60% 33/55 [00:02<00:01, 14.70it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  64% 35/55 [00:02<00:01, 14.72it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  67% 37/55 [00:02<00:01, 14.52it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  71% 39/55 [00:02<00:01, 14.08it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  75% 41/55 [00:02<00:01, 13.86it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  78% 43/55 [00:03<00:00, 13.90it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  82% 45/55 [00:03<00:00, 13.27it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  85% 47/55 [00:03<00:00, 13.15it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  89% 49/55 [00:03<00:00, 12.84it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  93% 51/55 [00:03<00:00, 12.48it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  96% 53/55 [00:03<00:00, 11.86it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset: 100% 55/55 [00:04<00:00, 10.25it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-11-16 06:59:23 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 1.759 | ppl 3.38 | wps 44430.9 | wpb 3315.7 | bsz 356 | num_updates 4279 | best_loss 1.729\n",
            "2021-11-16 06:59:23 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-11-16 06:59:32 | INFO | fairseq.checkpoint_utils | saved checkpoint data/mt-ckpt/checkpoint11.pt (epoch 11 @ 4279 updates, score 1.759) (writing took 8.458246281999891 seconds)\n",
            "2021-11-16 06:59:32 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2021-11-16 06:59:32 | INFO | train | epoch 011 | loss 0.838 | ppl 1.79 | wps 15400.9 | ups 4.12 | wpb 3734.3 | bsz 402.7 | num_updates 4279 | lr 0.001 | gnorm 0.383 | train_wall 81 | wall 1088\n",
            "epoch 012:   0% 0/389 [00:00<?, ?it/s]2021-11-16 06:59:32 | INFO | fairseq.trainer | begin training epoch 12\n",
            "epoch 012: 100% 388/389 [01:21<00:00,  4.89it/s, loss=0.782, ppl=1.72, wps=17043.4, ups=4.59, wpb=3716.6, bsz=399.5, num_updates=4600, lr=0.001, gnorm=0.389, train_wall=22, wall=1155]2021-11-16 07:00:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/55 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   2% 1/55 [00:00<00:09,  5.75it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   5% 3/55 [00:00<00:05,  9.45it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   9% 5/55 [00:00<00:04, 12.02it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  13% 7/55 [00:00<00:04, 11.55it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  16% 9/55 [00:00<00:03, 13.14it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  20% 11/55 [00:00<00:03, 13.84it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  24% 13/55 [00:01<00:02, 14.20it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  27% 15/55 [00:01<00:02, 14.44it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  31% 17/55 [00:01<00:02, 14.75it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  35% 19/55 [00:01<00:02, 14.67it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  38% 21/55 [00:01<00:02, 14.72it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  42% 23/55 [00:01<00:02, 14.77it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  45% 25/55 [00:01<00:01, 15.04it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  49% 27/55 [00:01<00:01, 14.73it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  53% 29/55 [00:02<00:01, 14.61it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  56% 31/55 [00:02<00:01, 14.21it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  60% 33/55 [00:02<00:01, 14.16it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  64% 35/55 [00:02<00:01, 14.38it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  67% 37/55 [00:02<00:01, 14.40it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  71% 39/55 [00:02<00:01, 14.02it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  75% 41/55 [00:02<00:01, 13.58it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  78% 43/55 [00:03<00:00, 13.64it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  82% 45/55 [00:03<00:00, 13.20it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  85% 47/55 [00:03<00:00, 13.07it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  89% 49/55 [00:03<00:00, 12.75it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  93% 51/55 [00:03<00:00, 12.40it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  96% 53/55 [00:03<00:00, 11.79it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset: 100% 55/55 [00:04<00:00, 10.13it/s]\u001b[A\n",
            "                                                                        \u001b[A2021-11-16 07:00:58 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 1.793 | ppl 3.46 | wps 43748.7 | wpb 3315.7 | bsz 356 | num_updates 4668 | best_loss 1.729\n",
            "2021-11-16 07:00:58 | INFO | fairseq_cli.train | begin save checkpoint\n",
            "2021-11-16 07:01:07 | INFO | fairseq.checkpoint_utils | saved checkpoint data/mt-ckpt/checkpoint12.pt (epoch 12 @ 4668 updates, score 1.793) (writing took 8.445605586000056 seconds)\n",
            "2021-11-16 07:01:07 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2021-11-16 07:01:07 | INFO | train | epoch 012 | loss 0.761 | ppl 1.69 | wps 15315.6 | ups 4.1 | wpb 3734.3 | bsz 402.7 | num_updates 4668 | lr 0.001 | gnorm 0.365 | train_wall 82 | wall 1183\n",
            "epoch 013:   0% 0/389 [00:00<?, ?it/s]2021-11-16 07:01:07 | INFO | fairseq.trainer | begin training epoch 13\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-train\", line 8, in <module>\n",
            "    sys.exit(cli_main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/train.py\", line 352, in cli_main\n",
            "    distributed_utils.call_main(args, main)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fairseq/distributed_utils.py\", line 301, in call_main\n",
            "    main(args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/train.py\", line 125, in main\n",
            "    valid_losses, should_stop = train(args, trainer, task, epoch_itr)\n",
            "  File \"/usr/lib/python3.7/contextlib.py\", line 74, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/train.py\", line 208, in train\n",
            "    log_output = trainer.train_step(samples)\n",
            "  File \"/usr/lib/python3.7/contextlib.py\", line 74, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fairseq/trainer.py\", line 486, in train_step\n",
            "    ignore_grad=is_dummy_batch,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fairseq/tasks/fairseq_task.py\", line 420, in train_step\n",
            "    optimizer.backward(loss)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fairseq/optim/fairseq_optimizer.py\", line 95, in backward\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\", line 307, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 156, in backward\n",
            "    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxyjH4LDevB7"
      },
      "source": [
        "When you run this command, your terminal will show two types of progress bars\n",
        "alternatively—one for training and another for validating.\n",
        "\n",
        "For each epoch, the training process alternates two\n",
        "stages: training and validation. An epoch, a concept used in machine learning, means one pass through the entire train data.\n",
        "\n",
        "In the training stage, the loss is calculated using the training data, then the model parameters are adjusted in such a way that the new set of parameters lowers the loss. \n",
        "\n",
        "In the validation stage, the model parameters\n",
        "are fixed, and a separate dataset (validation set) is used to measure how well the model is performing against the dataset.\n",
        "\n",
        "<img src='https://github.com/rahiakela/natural-language-processing-research-and-practice/blob/main/real-world-natural-language-processing/6-sequence-to-sequence-models/images/2.png?raw=1' width='800'/>\n",
        "\n",
        "As the training continues, the train loss becomes smaller and smaller and gradually\n",
        "approaches zero, because this is exactly what we told the optimizer to do: decrease the\n",
        "loss as much as possible. Checking whether the train loss is decreasing steadily epoch\n",
        "after epoch is a good “sanity check” that your model and the training pipeline are working as expected.\n",
        "\n",
        "On the other hand, if you look at the validation loss, it goes down at first for several\n",
        "epochs, but after a certain point, it gradually goes back up, forming a U-shaped\n",
        "curve—a typical sign of overfitting. After several epochs of training, your model fits\n",
        "the train set so well that it begins to lose its generalizability on the validation set.\n",
        "\n",
        "If you see your validation loss starting to creep up, there’s little point keeping the training process running, because chances are, your model has already overfitted to the data to some extent. A common practice in such a situation, called early stopping, is to terminate the training. \n",
        "\n",
        "Specifically, if your validation loss is not improving for a certain\n",
        "number of epochs, you stop the training and use the model at the point when the\n",
        "validation loss was the lowest. The number of epochs you wait until the training is terminated is called patience. \n",
        "\n",
        "In practice, the metric you care about the most (such as\n",
        "BLEU) is used for early stopping instead of the validation loss.\n",
        "\n",
        "The graph indicates that the validation loss is lowest around epoch 8, so you can stop (by pressing `Ctrl + C`) the fairseq-train command after around 10 epochs; otherwise, the command would keep running indefinitely. Fairseq will automatically save the best model parameters (in terms of the validation loss) to the `checkpoint_best.pt` file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OX3UjR3jbs6"
      },
      "source": [
        "##Running the translator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYA8hKC_jcW5"
      },
      "source": [
        "After the model is trained, you can invoke the fairseq-interactive command to\n",
        "run your MT model on any input in an interactive way. \n",
        "\n",
        "After you see the prompt Type the input sentence and press return, try typing\n",
        "(or copying and pasting) the following Spanish sentences one by one:\n",
        "\n",
        "```\n",
        "Buenos días !\n",
        "¡ Hola !\n",
        "¿ Dónde está el baño ?\n",
        "¿ Hay habitaciones libres ?\n",
        "¿ Acepta tarjeta de crédito ?\n",
        "La cuenta , por favor .\n",
        "```\n",
        "\n",
        "You can run the command by specifying the binary file location and the model parameter file as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbN6sBSajktw",
        "outputId": "c2a46b6d-cec7-43db-d981-7a53052df5f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!fairseq-interactive data/mt-bin \\\n",
        "    --path data/mt-ckpt/checkpoint_best.pt \\\n",
        "    --beam 5 \\\n",
        "    --source-lang es \\\n",
        "    --target-lang en"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-11-16 07:04:39 | INFO | fairseq_cli.interactive | Namespace(all_gather_list_size=16384, batch_size=1, batch_size_valid=None, beam=5, bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, buffer_size=1, checkpoint_shard_count=1, checkpoint_suffix='', constraints=None, cpu=False, criterion='cross_entropy', curriculum=0, data='data/mt-bin', data_buffer_size=10, dataset_impl=None, ddp_backend='c10d', decoding_format=None, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, distributed_wrapper='DDP', diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, empty_cache_freq=0, eval_bleu=False, eval_bleu_args=None, eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', input='-', iter_decode_eos_penalty=0.0, iter_decode_force_max_iter=False, iter_decode_max_iter=10, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, left_pad_source='True', left_pad_target='False', lenpen=1, lm_path=None, lm_weight=0.0, load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr_scheduler='fixed', lr_shrink=0.1, match_source_len=False, max_len_a=0, max_len_b=200, max_source_positions=1024, max_target_positions=1024, max_tokens=None, max_tokens_valid=None, memory_efficient_bf16=False, memory_efficient_fp16=False, min_len=1, min_loss_scale=0.0001, model_overrides='{}', model_parallel_size=1, nbest=1, no_beamable_mm=False, no_early_stop=False, no_progress_bar=False, no_repeat_ngram_size=0, no_seed_provided=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer=None, path='data/mt-ckpt/checkpoint_best.pt', pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, prefix_size=0, print_alignment=False, print_step=False, profile=False, quantization_config_path=None, quiet=False, remove_bpe=None, replace_unk=None, required_batch_size_multiple=8, required_seq_len_multiple=1, results_path=None, retain_dropout=False, retain_dropout_modules=None, retain_iter_history=False, sacrebleu=False, sampling=False, sampling_topk=-1, sampling_topp=-1.0, score_reference=False, scoring='bleu', seed=1, shard_id=0, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='es', target_lang='en', task='translation', temperature=1.0, tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unkpen=0, unnormalized=False, upsample_primary=1, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_updates=0, zero_sharding='none')\n",
            "2021-11-16 07:04:39 | INFO | fairseq.tasks.translation | [es] dictionary: 16832 types\n",
            "2021-11-16 07:04:39 | INFO | fairseq.tasks.translation | [en] dictionary: 11416 types\n",
            "2021-11-16 07:04:39 | INFO | fairseq_cli.interactive | loading model(s) from data/mt-ckpt/checkpoint_best.pt\n",
            "2021-11-16 07:04:41 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2021-11-16 07:04:41 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "¡ Buenos días !\n",
            "/usr/local/lib/python3.7/dist-packages/fairseq/search.py:140: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  beams_buf = indices_buf // vocab_size\n",
            "/usr/local/lib/python3.7/dist-packages/fairseq/sequence_generator.py:651: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  unfin_idx = idx // beam_size\n",
            "S-0\t¡ Buenos días !\n",
            "W-0\t0.101\tseconds\n",
            "H-0\t-0.25427359342575073\tGood morning !\n",
            "D-0\t-0.25427359342575073\tGood morning !\n",
            "P-0\t-0.8047 -0.1836 -0.0283 -0.0005\n",
            "¡ Hola !\n",
            "S-1\t¡ Hola !\n",
            "W-1\t0.019\tseconds\n",
            "H-1\t-0.11692231893539429\tHi !\n",
            "D-1\t-0.11692231893539429\tHi !\n",
            "P-1\t-0.3233 -0.0170 -0.0104\n",
            "¿ Dónde está el baño ?\n",
            "S-2\t¿ Dónde está el baño ?\n",
            "W-2\t0.032\tseconds\n",
            "H-2\t-0.30176666378974915\tWhere &apos;s the toilet ?\n",
            "D-2\t-0.30176666378974915\tWhere &apos;s the toilet ?\n",
            "P-2\t-0.0150 -0.3637 -0.0017 -1.4270 -0.0022 -0.0009\n",
            "¿ Hay habitaciones libres ?\n",
            "S-3\t¿ Hay habitaciones libres ?\n",
            "W-3\t0.038\tseconds\n",
            "H-3\t-0.593152642250061\tIs there a free rooms ?\n",
            "D-3\t-0.593152642250061\tIs there a free rooms ?\n",
            "P-3\t-0.1604 -0.0116 -2.1812 -1.0646 -0.7068 -0.0264 -0.0012\n",
            "¿ Acepta tarjeta de crédito ?\n",
            "S-4\t¿ Acepta tarjeta de crédito ?\n",
            "W-4\t0.037\tseconds\n",
            "H-4\t-0.07972174882888794\tDo you accept credit card ?\n",
            "D-4\t-0.07972174882888794\tDo you accept credit card ?\n",
            "P-4\t-0.0551 -0.0633 -0.3996 -0.0241 -0.0143 -0.0015 -0.0002\n",
            "La cuenta , por favor .\n",
            "S-5\tLa cuenta , por favor .\n",
            "W-5\t0.033\tseconds\n",
            "H-5\t-0.3058212697505951\tThe bill , please .\n",
            "D-5\t-0.3058212697505951\tThe bill , please .\n",
            "P-5\t-1.5694 -0.2349 -0.0128 -0.0026 -0.0147 -0.0004\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-interactive\", line 8, in <module>\n",
            "    sys.exit(cli_main())\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/interactive.py\", line 307, in cli_main\n",
            "    distributed_utils.call_main(args, main)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fairseq/distributed_utils.py\", line 301, in call_main\n",
            "    main(args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/interactive.py\", line 203, in main\n",
            "    for inputs in buffered_read(args.input, args.buffer_size):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/interactive.py\", line 42, in buffered_read\n",
            "    for src_str in h:\n",
            "  File \"/usr/lib/python3.7/fileinput.py\", line 252, in __next__\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb-zZJFak5b-"
      },
      "source": [
        "Most of the output sentences here are almost perfect, except the fourth one (I would translate to \"is there free rooms?\"). Even considering the fact that these sentences are all simple examples you can find in any travel Spanish phrasebook, this is not a bad start for a system built within an hour!"
      ]
    }
  ]
}