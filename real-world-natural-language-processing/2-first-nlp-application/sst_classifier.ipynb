{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "92b35821ede83672ccb97912c62ccda6ab0e80a152a70e9f1912db9acba38245"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "sst_classifier.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-research-and-practice/blob/main/real-world-natural-language-processing/2-first-nlp-application/sst_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WddxoP8EUqMa"
      },
      "source": [
        "##Introducing sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY1dIJOyUrLh"
      },
      "source": [
        "In a scenario, you wanted to extract users’ subjective opinions\n",
        "from online survey results. You have a collection of textual data in response to a\n",
        "free-response question, but you are missing the answers to the “How do you like our\n",
        "product?” question, which you’d like to recover from the text. \n",
        "\n",
        "This task is called sentiment\n",
        "analysis, which is a text analytic technique used in the automatic identification\n",
        "and categorization of subjective information within text. The technique is widely used\n",
        "in quantifying opinions, emotions, and so on that are written in an unstructured way\n",
        "and, thus, hard to quantify otherwise. Sentiment analysis is applied to a wide variety of\n",
        "textual resources such as survey, reviews, and social media posts.\n",
        "\n",
        "In machine learning, classification means categorizing something into a set of predefined,\n",
        "discrete categories. One of the most basic tasks in sentiment analysis is the\n",
        "classification of polarity, that is, to classify whether the expressed opinion is positive,\n",
        "negative, or neutral.\n",
        "\n",
        "<img src='images/1.png?raw=1' width='800'/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3awuZjufUswB"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fnup-v9Utwe"
      },
      "source": [
        "!pip -q install allennlp==2.5.0\n",
        "!pip -q install allennlp-models==2.5.0\n",
        "!git -q clone https://github.com/mhagiwara/realworldnlp.git\n",
        "%cd realworldnlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4DSCWXyT-t1"
      },
      "source": [
        "from itertools import chain\n",
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from allennlp.data.data_loaders import MultiProcessDataLoader\n",
        "from allennlp.data.samplers import BucketBatchSampler\n",
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.models import Model\n",
        "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
        "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.nn.util import get_text_field_mask\n",
        "from allennlp.training import GradientDescentTrainer\n",
        "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
        "from allennlp_models.classification.dataset_readers.stanford_sentiment_tree_bank import \\\n",
        "    StanfordSentimentTreeBankDatasetReader\n",
        "\n",
        "from realworldnlp.predictors import SentenceClassifierPredictor"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nl8jH0qT-t4"
      },
      "source": [
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 128"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXrGSlJYWGhT"
      },
      "source": [
        "##What is a dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BOqsIXpWJWF"
      },
      "source": [
        "In NLP, records in a dataset are usually some type of linguistic units, such as words,\n",
        "sentences, or documents. A dataset of natural language texts is called a corpus (plural: corpora).\n",
        "\n",
        "If a dataset contains a collection of sentences annotated\n",
        "with their parse trees, the dataset is called a treebank. The most famous example\n",
        "of this is [Penn Treebank (PTB)](http://realworldnlpbook.com/ch2.html#ptb), which\n",
        "has been serving as the de facto standard dataset for training and evaluating NLP tasks\n",
        "such as part-of-speech (POS) tagging and parsing.\n",
        "\n",
        "A closely related term to a record is an instance. In machine learning, an instance is\n",
        "a basic unit for which the prediction is made.\n",
        "\n",
        "<img src='images/2.png?raw=1' width='800'/>\n",
        "\n",
        "Finally, a label is a piece of information\n",
        "attached to some linguistic unit in a dataset.\n",
        "\n",
        "Labels are usually used as training signals (i.e., answers for\n",
        "the training algorithm) in a supervised machine learning setting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_0dcxQzW_Mv"
      },
      "source": [
        "###Train, validation, and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwDpv3eXXADM"
      },
      "source": [
        "A train (or training) set is the main dataset used to train the NLP/ML models.\n",
        "Instances from the train set are usually fed to the ML training pipeline directly and\n",
        "used to learn parameters of the model.\n",
        "\n",
        "A validation set (also called a dev or development set) is used for model selection. Model\n",
        "selection is a process where appropriate NLP/ML models are selected among all possible\n",
        "models that can be trained using the train set, and here’s why it’s necessary.\n",
        "\n",
        "<img src='images/3.png?raw=1' width='800'/>\n",
        "\n",
        "In summary, when training NLP models, use a train set to train your model candidates,\n",
        "use a validation set to choose good ones, and use a test set to evaluate them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2448HzDeZN2A"
      },
      "source": [
        "###Loading SST datasets using AllenNLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhVJIo0-ZpSa"
      },
      "source": [
        "AllenNLP already supports an abstraction called DatasetReader, which takes care of\n",
        "reading a dataset from the original format (be it raw text or some exotic XML-based\n",
        "format) and returns it as a collection of instances. \n",
        "\n",
        "We are going to use Stanford-\n",
        "SentimentTreeBankDatasetReader(), which is a type of DatasetReader that\n",
        "specifically deals with SST datasets, as shown here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HCU9zYWZpvh"
      },
      "source": [
        "reader = StanfordSentimentTreeBankDatasetReader()\n",
        "train_path = 'https://s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/train.txt'\n",
        "dev_path = 'https://s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/dev.txt'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqwzFXDzaQEf"
      },
      "source": [
        "##Using word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHY1UZcRaZyF"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZSznlqOT-t5"
      },
      "source": [
        "# Model in AllenNLP represents a model that is trained.\n",
        "class LstmClassifier(Model):\n",
        "    def __init__(self,\n",
        "                 embedder: TextFieldEmbedder,\n",
        "                 encoder: Seq2VecEncoder,\n",
        "                 vocab: Vocabulary,\n",
        "                 positive_label: str = '4') -> None:\n",
        "        super().__init__(vocab)\n",
        "        # We need the embeddings to convert word IDs to their vector representations\n",
        "        self.embedder = embedder\n",
        "\n",
        "        self.encoder = encoder\n",
        "\n",
        "        # After converting a sequence of vectors to a single vector, we feed it into\n",
        "        # a fully-connected linear layer to reduce the dimension to the total number of labels.\n",
        "        self.linear = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
        "                                      out_features=vocab.get_vocab_size('labels'))\n",
        "\n",
        "        # Monitor the metrics - we use accuracy, as well as prec, rec, f1 for 4 (very positive)\n",
        "        positive_index = vocab.get_token_index(positive_label, namespace='labels')\n",
        "        self.accuracy = CategoricalAccuracy()\n",
        "        self.f1_measure = F1Measure(positive_index)\n",
        "\n",
        "        # We use the cross entropy loss because this is a classification task.\n",
        "        # Note that PyTorch's CrossEntropyLoss combines softmax and log likelihood loss,\n",
        "        # which makes it unnecessary to add a separate softmax layer.\n",
        "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Instances are fed to forward after batching.\n",
        "    # Fields are passed through arguments with the same name.\n",
        "    def forward(self,\n",
        "                tokens: Dict[str, torch.Tensor],\n",
        "                label: torch.Tensor = None) -> torch.Tensor:\n",
        "        # In deep NLP, when sequences of tensors in different lengths are batched together,\n",
        "        # shorter sequences get padded with zeros to make them equal length.\n",
        "        # Masking is the process to ignore extra zeros added by padding\n",
        "        mask = get_text_field_mask(tokens)\n",
        "\n",
        "        # Forward pass\n",
        "        embeddings = self.embedder(tokens)\n",
        "        encoder_out = self.encoder(embeddings, mask)\n",
        "        logits = self.linear(encoder_out)\n",
        "\n",
        "        # In AllenNLP, the output of forward() is a dictionary.\n",
        "        # Your output dictionary must contain a \"loss\" key for your model to be trained.\n",
        "        output = {\"logits\": logits}\n",
        "        if label is not None:\n",
        "            self.accuracy(logits, label)\n",
        "            self.f1_measure(logits, label)\n",
        "            output[\"loss\"] = self.loss_function(logits, label)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "        return {'accuracy': self.accuracy.get_metric(reset),\n",
        "                **self.f1_measure.get_metric(reset)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "briDlExuT-t9"
      },
      "source": [
        "reader = StanfordSentimentTreeBankDatasetReader()\n",
        "train_path = 'https://s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/train.txt'\n",
        "dev_path = 'https://s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/dev.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxsPtK49T-t_"
      },
      "source": [
        "sampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"tokens\"])\n",
        "train_data_loader = MultiProcessDataLoader(reader, train_path, batch_sampler=sampler)\n",
        "dev_data_loader = MultiProcessDataLoader(reader, dev_path, batch_sampler=sampler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LooUpnU2T-uB"
      },
      "source": [
        "# You can optionally specify the minimum count of tokens/labels.\n",
        "# `min_count={'tokens':3}` here means that any tokens that appear less than three times\n",
        "# will be ignored and not included in the vocabulary.\n",
        "vocab = Vocabulary.from_instances(chain(train_data_loader.iter_instances(), dev_data_loader.iter_instances()),\n",
        "                                  min_count={'tokens': 3})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGf41U04T-uD"
      },
      "source": [
        "train_data_loader.index_with(vocab)\n",
        "dev_data_loader.index_with(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkesGTvKT-uF"
      },
      "source": [
        "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
        "                            embedding_dim=EMBEDDING_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzASIMNjT-uH"
      },
      "source": [
        "# BasicTextFieldEmbedder takes a dict - we need an embedding just for tokens,\n",
        "# not for labels, which are used as-is as the \"answer\" of the sentence classification\n",
        "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v5saY-ET-uI"
      },
      "source": [
        "# Seq2VecEncoder is a neural network abstraction that takes a sequence of something\n",
        "# (usually a sequence of embedded word vectors), processes it, and returns a single\n",
        "# vector. Oftentimes this is an RNN-based architecture (e.g., LSTM or GRU), but\n",
        "# AllenNLP also supports CNNs and other simple architectures (for example,\n",
        "# just averaging over the input vectors).\n",
        "encoder = PytorchSeq2VecWrapper(\n",
        "    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeZKijW6T-uJ"
      },
      "source": [
        "model = LstmClassifier(word_embeddings, encoder, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amXIIBtyT-uK"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFtFXJhNT-uL"
      },
      "source": [
        "trainer = GradientDescentTrainer(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_data_loader,\n",
        "    validation_data_loader=dev_data_loader,\n",
        "    patience=10,\n",
        "    num_epochs=20,\n",
        "    cuda_device=-1)\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_xHCNXNT-uM"
      },
      "source": [
        "predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
        "logits = predictor.predict('This is the best movie ever!')['logits']\n",
        "label_id = np.argmax(logits)\n",
        "\n",
        "print(model.vocab.get_token_from_index(label_id, 'labels'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oGErwRTT-uM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}