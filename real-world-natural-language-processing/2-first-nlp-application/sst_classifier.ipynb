{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "92b35821ede83672ccb97912c62ccda6ab0e80a152a70e9f1912db9acba38245"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "sst_classifier.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-research-and-practice/blob/main/real-world-natural-language-processing/2-first-nlp-application/sst_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WddxoP8EUqMa"
      },
      "source": [
        "##Introducing sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY1dIJOyUrLh"
      },
      "source": [
        "In a scenario, you wanted to extract users’ subjective opinions\n",
        "from online survey results. You have a collection of textual data in response to a\n",
        "free-response question, but you are missing the answers to the “How do you like our\n",
        "product?” question, which you’d like to recover from the text. \n",
        "\n",
        "This task is called sentiment\n",
        "analysis, which is a text analytic technique used in the automatic identification\n",
        "and categorization of subjective information within text. The technique is widely used\n",
        "in quantifying opinions, emotions, and so on that are written in an unstructured way\n",
        "and, thus, hard to quantify otherwise. Sentiment analysis is applied to a wide variety of\n",
        "textual resources such as survey, reviews, and social media posts.\n",
        "\n",
        "In machine learning, classification means categorizing something into a set of predefined,\n",
        "discrete categories. One of the most basic tasks in sentiment analysis is the\n",
        "classification of polarity, that is, to classify whether the expressed opinion is positive,\n",
        "negative, or neutral.\n",
        "\n",
        "<img src='https://github.com/rahiakela/natural-language-processing-research-and-practice/blob/main/real-world-natural-language-processing/2-first-nlp-application/images/1.png?raw=1' width='800'/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3awuZjufUswB"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fnup-v9Utwe"
      },
      "source": [
        "!pip -q install allennlp==2.5.0\n",
        "!pip -q install allennlp-models==2.5.0\n",
        "!git clone https://github.com/mhagiwara/realworldnlp.git\n",
        "%cd realworldnlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4DSCWXyT-t1"
      },
      "source": [
        "from itertools import chain\n",
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from allennlp.data.data_loaders import MultiProcessDataLoader\n",
        "from allennlp.data.samplers import BucketBatchSampler\n",
        "from allennlp.data.vocabulary import Vocabulary\n",
        "from allennlp.models import Model\n",
        "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
        "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding\n",
        "from allennlp.nn.util import get_text_field_mask\n",
        "from allennlp.training import GradientDescentTrainer\n",
        "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
        "from allennlp_models.classification.dataset_readers.stanford_sentiment_tree_bank import StanfordSentimentTreeBankDatasetReader\n",
        "\n",
        "from realworldnlp.predictors import SentenceClassifierPredictor"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nl8jH0qT-t4"
      },
      "source": [
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 128"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXrGSlJYWGhT"
      },
      "source": [
        "##What is a dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BOqsIXpWJWF"
      },
      "source": [
        "In NLP, records in a dataset are usually some type of linguistic units, such as words,\n",
        "sentences, or documents. A dataset of natural language texts is called a corpus (plural: corpora).\n",
        "\n",
        "If a dataset contains a collection of sentences annotated\n",
        "with their parse trees, the dataset is called a treebank. The most famous example\n",
        "of this is [Penn Treebank (PTB)](http://realworldnlpbook.com/ch2.html#ptb), which\n",
        "has been serving as the de facto standard dataset for training and evaluating NLP tasks\n",
        "such as part-of-speech (POS) tagging and parsing.\n",
        "\n",
        "A closely related term to a record is an instance. In machine learning, an instance is\n",
        "a basic unit for which the prediction is made.\n",
        "\n",
        "<img src='https://github.com/rahiakela/natural-language-processing-research-and-practice/blob/main/real-world-natural-language-processing/2-first-nlp-application/images/2.png?raw=1' width='800'/>\n",
        "\n",
        "Finally, a label is a piece of information\n",
        "attached to some linguistic unit in a dataset.\n",
        "\n",
        "Labels are usually used as training signals (i.e., answers for\n",
        "the training algorithm) in a supervised machine learning setting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_0dcxQzW_Mv"
      },
      "source": [
        "###Train, validation, and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwDpv3eXXADM"
      },
      "source": [
        "A train (or training) set is the main dataset used to train the NLP/ML models.\n",
        "Instances from the train set are usually fed to the ML training pipeline directly and\n",
        "used to learn parameters of the model.\n",
        "\n",
        "A validation set (also called a dev or development set) is used for model selection. Model\n",
        "selection is a process where appropriate NLP/ML models are selected among all possible\n",
        "models that can be trained using the train set, and here’s why it’s necessary.\n",
        "\n",
        "<img src='https://github.com/rahiakela/natural-language-processing-research-and-practice/blob/main/real-world-natural-language-processing/2-first-nlp-application/images/3.png?raw=1' width='800'/>\n",
        "\n",
        "In summary, when training NLP models, use a train set to train your model candidates,\n",
        "use a validation set to choose good ones, and use a test set to evaluate them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2448HzDeZN2A"
      },
      "source": [
        "###Loading SST datasets using AllenNLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhVJIo0-ZpSa"
      },
      "source": [
        "AllenNLP already supports an abstraction called DatasetReader, which takes care of\n",
        "reading a dataset from the original format (be it raw text or some exotic XML-based\n",
        "format) and returns it as a collection of instances. \n",
        "\n",
        "We are going to use Stanford-\n",
        "SentimentTreeBankDatasetReader(), which is a type of DatasetReader that\n",
        "specifically deals with SST datasets, as shown here:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HCU9zYWZpvh"
      },
      "source": [
        "reader = StanfordSentimentTreeBankDatasetReader()\n",
        "train_path = 'https://s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/train.txt'\n",
        "dev_path = 'https://s3.amazonaws.com/realworldnlpbook/data/stanfordSentimentTreebank/trees/dev.txt'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqwzFXDzaQEf"
      },
      "source": [
        "##Using word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHY1UZcRaZyF"
      },
      "source": [
        "Word embeddings are one of the most important concepts in modern NLP. Technically,\n",
        "an embedding is a continuous vector representation of something that is usually discrete.\n",
        "A word embedding is a continuous vector representation of a word.\n",
        "\n",
        "In simpler terms, word embeddings are a way to represent\n",
        "each word with a 300-element array (or an array of any other size) filled with\n",
        "nonzero float numbers.\n",
        "\n",
        "Can we think of some sort of numerical scale where words are represented as points, so that semantically closer words (e.g., “dog” and “cat,” which are both animals) are also geometrically closer?\n",
        "\n",
        "<img src='https://github.com/rahiakela/natural-language-processing-research-and-practice/blob/main/real-world-natural-language-processing/2-first-nlp-application/images/4.png?raw=1' width='800'/>\n",
        "\n",
        "Because computers are really good at dealing with multidimensional\n",
        "spaces (because you can just represent points by arrays), you can simply keep doing\n",
        "this until you have a sufficient number of dimensions. \n",
        "\n",
        "Let’s have three dimensions. In\n",
        "this 3-D space, you can represent those three words as follows:\n",
        "\n",
        "```python\n",
        "vec(\"cat\") = [0.7, 0.5, 0.1]\n",
        "vec(\"dog\") = [0.8, 0.3, 0.1]\n",
        "vec(\"pizza\") = [0.1, 0.2, 0.8]\n",
        "```\n",
        "\n",
        "The x -axis (the first element) here represents some concept of “animal-ness” and\n",
        "the z -axis (the third dimension) corresponds to “food-ness.” (I’m making these numbers\n",
        "up, but you get the point.) \n",
        "\n",
        "This is essentially what word embeddings are. You just\n",
        "embedded those words in a three-dimensional space. By using those vectors, you\n",
        "already “know” how the basic building blocks of the language work.\n",
        "\n",
        "<img src='https://github.com/rahiakela/natural-language-processing-research-and-practice/blob/main/real-world-natural-language-processing/2-first-nlp-application/images/5.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz-akuB2yAdj"
      },
      "source": [
        "###Using word embeddings for sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O_O7MjTyCMy"
      },
      "source": [
        "First, we create dataset loaders that take care of loading data and passing it to the training pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZQQYCvMyddt",
        "outputId": "2a4c380a-d392-497f-b713-99cfc33050b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sampler = BucketBatchSampler(batch_size=32, sorting_keys=[\"tokens\"])\n",
        "train_data_loader = MultiProcessDataLoader(reader, train_path, batch_sampler=sampler)\n",
        "dev_data_loader = MultiProcessDataLoader(reader, dev_path, batch_sampler=sampler)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading instances: 0it [00:00, ?it/s]\n",
            "downloading:   0%|          | 0/2160058 [00:00<?, ?B/s]\u001b[A\n",
            "downloading:   2%|1         | 43008/2160058 [00:00<00:08, 259252.81B/s]\u001b[A\n",
            "downloading:  10%|#         | 217088/2160058 [00:00<00:02, 708270.16B/s]\u001b[A\n",
            "downloading: 100%|##########| 2160058/2160058 [00:00<00:00, 3601735.96B/s]\n",
            "loading instances: 8544it [00:03, 2305.34it/s]\n",
            "loading instances: 0it [00:00, ?it/s]\n",
            "downloading:   0%|          | 0/280825 [00:00<?, ?B/s]\u001b[A\n",
            "downloading:  18%|#7        | 50176/280825 [00:00<00:00, 306658.21B/s]\u001b[A\n",
            "downloading: 100%|##########| 280825/280825 [00:00<00:00, 811960.36B/s]\n",
            "loading instances: 1101it [00:01, 782.93it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaQayp6IyqbB"
      },
      "source": [
        "AllenNLP provides a useful Vocabulary class that manages mappings from some linguistic\n",
        "units (such as characters, words, and labels) to their IDs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HGLwo7G1uvi",
        "outputId": "e31aaa53-7e7a-45b6-86a6-059460e49c53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# You can optionally specify the minimum count of tokens/labels.\n",
        "# `min_count={'tokens':3}` here means that any tokens that appear less than three times will be ignored and not included in the vocabulary.\n",
        "vocab = Vocabulary.from_instances(chain(train_data_loader.iter_instances(), dev_data_loader.iter_instances()),\n",
        "                                  min_count={\"tokens\": 3})"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "building vocab: 9645it [00:00, 57063.47it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGf41U04T-uD"
      },
      "source": [
        "train_data_loader.index_with(vocab)\n",
        "dev_data_loader.index_with(vocab)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEjRacMdyyhY"
      },
      "source": [
        "Then, you need to initialize an Embedding instance, which takes care of converting IDs to embeddings.The size (dimension) of the\n",
        "embeddings is determined by `EMBEDDING_DIM`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OECpEWW23oeN"
      },
      "source": [
        "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size(\"tokens\"), embedding_dim=EMBEDDING_DIM)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdcbBO_zy7el"
      },
      "source": [
        "Finally, you need to specify which index names correspond to which embeddings and pass it to `BasicTextFieldEmbedder` as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqCiY4qD30IC"
      },
      "source": [
        "# BasicTextFieldEmbedder takes a dict - we need an embedding just for tokens, not for labels, which are used as-is as the \"answer\" of the sentence classification\n",
        "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5uzggD34FeA"
      },
      "source": [
        "Now you can use word_embeddings to convert words to their embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWDnH1D64HZE"
      },
      "source": [
        "##Neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyxZsHlN4Pi5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v5saY-ET-uI"
      },
      "source": [
        "# Seq2VecEncoder is a neural network abstraction that takes a sequence of something\n",
        "# (usually a sequence of embedded word vectors), processes it, and returns a single\n",
        "# vector. Oftentimes this is an RNN-based architecture (e.g., LSTM or GRU), but\n",
        "# AllenNLP also supports CNNs and other simple architectures (for example,\n",
        "# just averaging over the input vectors).\n",
        "encoder = PytorchSeq2VecWrapper(\n",
        "    torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeZKijW6T-uJ"
      },
      "source": [
        "model = LstmClassifier(word_embeddings, encoder, vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amXIIBtyT-uK"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFtFXJhNT-uL"
      },
      "source": [
        "trainer = GradientDescentTrainer(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    data_loader=train_data_loader,\n",
        "    validation_data_loader=dev_data_loader,\n",
        "    patience=10,\n",
        "    num_epochs=20,\n",
        "    cuda_device=-1)\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_xHCNXNT-uM"
      },
      "source": [
        "predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
        "logits = predictor.predict('This is the best movie ever!')['logits']\n",
        "label_id = np.argmax(logits)\n",
        "\n",
        "print(model.vocab.get_token_from_index(label_id, 'labels'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oGErwRTT-uM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}