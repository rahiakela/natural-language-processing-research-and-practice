{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "convolutional-neural-network-sentiment-analysis.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-in-action/blob/master/7-getting-words-in-order-with-convolutional-neural-networks/convolutional_neural_network_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO1preWdp2QJ"
      },
      "source": [
        "# Convolutional Neural Network for Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfJtCbhbqTT1"
      },
      "source": [
        "Let’s take a look at convolution in Python with the example convolutional neural network classifier provided in the Keras documentation. They have crafted a onedimensional convolutional net to examine the IMDB movie review dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8dMuJFaqjr2"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKtslBeDg78u"
      },
      "source": [
        "[1] * 10 ** 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shf2mXZKrZg4"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import backend as keras_backend\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Conv1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "import re\n",
        "import tqdm\n",
        "\n",
        "import requests"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCCTu_fmUskA"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# dowload w2v pretrained model\n",
        "wget https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz\n",
        "\n",
        "# unzip the dowloaded file\n",
        "gzip -d GoogleNews-vectors-negative300.bin.gz\n",
        "\n",
        "# dowload IMDB dataset\n",
        "wget https://www.dropbox.com/s/yviic64qv84x73j/aclImdb_v1.tar.gz\n",
        "\n",
        "# unzip IMDB dataset\n",
        "gzip -d aclImdb_v1.tar.gz\n",
        "tar -xvf aclImdb_v1.tar\n",
        "\n",
        "# remove tar file\n",
        "rm -rf aclImdb_v1.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swxx7Tmxr0np"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQymxGOrr3X5"
      },
      "source": [
        "Each data point is prelabeled with a 0 (negative sentiment) or a 1 (positive sentiment).you’re going to swap out their example IMDB movie review dataset\n",
        "for one in raw text, so you can get your hands dirty with the preprocessing of the text as well. And then you’ll see if you can use this trained network to classify text it has never seen before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipv8IOS3v6AK"
      },
      "source": [
        "### Preprocessing the loaded documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjVYBXqrwT7V"
      },
      "source": [
        "The reviews in the train folder are broken up into text files in either the pos or neg folders. You’ll first need to read those in Python with their appropriate label and then shuffle the deck so the samples aren’t all positive and then all negative. Training with the sorted labels will skew training toward whatever comes last, especially when you use certain hyperparameters, such as momentum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtmvAhqfv5Mo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c83c09-86d6-4fd9-e22d-206aa727524a"
      },
      "source": [
        "import glob\n",
        "from random import shuffle\n",
        "\n",
        "def pre_process_data(filepath):\n",
        "  '''\n",
        "  This is dependent on your training data source but we will try to generalize it as best as possible.\n",
        "  '''\n",
        "  positive_path = os.path.join(filepath, 'pos')\n",
        "  negative_path = os.path.join(filepath, 'neg')\n",
        "\n",
        "  pos_label = 1\n",
        "  neg_label = 0\n",
        "\n",
        "  dataset = []\n",
        "\n",
        "  for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
        "    with open(filename, 'r') as f:\n",
        "      dataset.append((pos_label, f.read()))\n",
        "\n",
        "  for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
        "    with open(filename, 'r') as f:\n",
        "      dataset.append((neg_label, f.read()))\n",
        "\n",
        "  shuffle(dataset)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "dataset = pre_process_data('./aclImdb/train')\n",
        "print(dataset[:5])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 'The End of Violence and certainly the Million Dollar hotel hinted at the idea the Wenders has lost his vision, his ability to tell compelling stories through a map of the moving picture. The Land of Plenty seals the coffin, I\\'m afraid, by being a vastly unimaginative, obviously sentimental and cliché\\'d film. The characters are entirely flat and stereotyped, the writing, plot and direction are amateurish, at best. For the first time in quite a while, I was impatient for the film to end so I could get on with my life. The war-torn delirium of the uncle, the patriotic abstract gazing at the sky at the conclusion...it all just struck me as being so simple and pathetic, hardly the work of a filmmaker who once made some compelling magic on screen. What happened? The days of experimentation, perceptive writing and interesting filming possibilities are long behind him, I\\'m afraid. Let\\'s hope he finds his inspiration again... At the Toronto film festival, which is where I saw the film, Wenders was there to introduce it. Completely lacking in humility, he offered us the following: \"I hope...no, wait...I KNOW you\\'re going to enjoy the next two hours.\" I\\'m afraid he couldn\\'t be more wrong...'), (1, \"The best film about marriage and family. This is a very interesting reflections to the couples that will be come to the dangerous and paradoxical fascinating world of marriage and family. This decision could be the better or the worst in our lives and the life of our kids. The real intrusion or help of 'friends' -or executioner if we leave-. The real role of families: they can help or they can destroy us. The mad priest who possibly is not much mad telling what could happen according the statistics and the reality. A couple who thinks in a 'special' marriage, live a painful story in their future own history.<br /><br />Who likes contract marriage? Nobody, after the priest tells their own history\\x85 if they leave the future in another hands, if they don't know WHAT is the marriage. That the problems are true, that the life demand a real engage, guaranties, from each one. That the real victims of the divorce are kids, with real name \\x96Andrea in the film- or names. That the abortion is only an easy exit: sadness, regrets and unhappiness will be there after abortion. That the state and social security thinks every time less in a real problems of the families. The gossip of the 'friends', the infidelity because of weakness and desperation of Steffania because Tomasso lives his life as if he were alone.<br /><br />Maybe someone could think that this film is a pessimistic film, but not. Steffania and Tomasso, in the deep of their hearts, they like a beautiful marriage and family, if not, Why they like marriage? A truly and beautiful marriage depends only of the couple: of each one of their decisions, of each one actions in their lives. The family could be a place where each one feel loved because being his or her, only by existing. The screenplay is wonderful. The performances are great: Steffania and Tomasso, ¡the almost cynical priest! An excellent direction and script. The colors and the management of the cameras, superb.\"), (1, 'If one would see a René Clair film with the kind of distracted semi-attention which is the rule in TV watching - one might be better off doing something different.<br /><br />Watching \"Le Million\" with all attention focused upon what takes place before eyes and ears will reveal a wealth of delightful details which keep this musical comedy going from the beginning to the end with its explosion of joy.<br /><br />In the Danish newspaper Berlingske Tidende a journalist once wrote: \"In my younger days I saw a film which made me feel like dancing all the way home from the cinema. This film is on TV tonight - see it!\"'), (0, 'The ultimate goal of Big Brother, that we know what to think before we think it, has been realized. Is it some kind of miracle, or sinister joke, that people don mental straight jackets of their own volition, twist themselves into contorted shapes, and grin like apes? Movies, art, no longer risk the unknown, but are forgone conclusions, drained of life.<br /><br />\"The Notorious Bettie Page\" is a bland case history, fit for a freshman college feminism course. Its lesson is schematic, right-angled and linear: \"See how women are objectified, exploited, abused, then tossed on the trash heap, by a male-dominated society.\"<br /><br />Bettie Page, supposedly, was the \"pin-up queen of the 1950\\'s,\" the ass millions of men ejaculated to. (All reviewers repeat that phrase, \"pin-up queen of the 1950\\'s,\" like a choir of monkeys.) Her history as an American sex bomb is familiar: Southern, abused by her father, raped, etc. In this movie she is a naïf, an innocent unaware of the prurient interests she serves and shamelessly profits from. Although she believes in Jesus, she enjoys frolicking nude before a camera lens -- just the wholesome girl-next-door sex-slave American males supposedly fantasize.<br /><br />From the mouth of writer-director Mary Harron herself, Oxford-educated AND ex-punker (do you smell the combined rot of privilege and \"hipness\" as I do?): \"I feel that without feminism, I wouldn\\'t be doing this. ... I don\\'t make feminist films in the sense that I don\\'t make anything ideological. But I do find that women get my films better.\" What a cozy clique.<br /><br />The movie merely goes through the motions of telling the story of a human life, it\\'s subject and purpose having been eulogized and interred well before the movie began. Ms. Page has a boyfriend, but we are shown next to nothing about their relationship. In fact, there are no intimate or detailed relationships in the film. <br /><br />One can\\'t ignore its smug simplicity. In New York, where Ms. Page tries her best to fit into and appease a man\\'s world, letting herself be tied up in the ropes of bondage and tightly laced into the black leather boots and bodices of S & M, the movie is black and white. But down in Miami, where she goes to get away from it all, gleefully takes off her clothes, and is photographed by a \"liberated\" female, the movie turns into color.<br /><br />Like hell Harron doesn\\'t \"make feminist films,\" doesn\\'t \"make anything ideological.\" Ideology has become so internalized, so assumed, so programmed, that it\\'s almost invisible. Big Brother must be smiling.'), (1, 'Great entertainment from start to the end. Wonderful performances by Belushi, Beach, Dalton & Railsback. Some twists and many action scenes. The movie was made for me! Funny lines in the screenplay, good music. Dalton as the tough sheriff and Railsback as \"redneck-villain\". I must recommend this film to every action-adventure fan! 10/10')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W76Jm0mbGm3o"
      },
      "source": [
        "### Data tokenization and vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT3S4Gd3FCvO"
      },
      "source": [
        "The next step is to tokenize and vectorize the data. You’ll use the Google News pretrained `Word2vec` vectors, so download those directly from Google.\n",
        "\n",
        "You’ll use gensim to unpack the vectors, You can\n",
        "experiment with the limit argument to the `load_word2vec_format` method; a\n",
        "higher number will get you more vectors to play with, but memory quickly becomes an issue and return on investment drops quickly in really high values for limit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slErmWLvxsca"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, limit=200000)\n",
        "\n",
        "def tokenize_and_vectorize(dataset):\n",
        "  tokenizer = TreebankWordTokenizer()\n",
        "  vectorized_data = []\n",
        "\n",
        "  for sample in dataset:\n",
        "    tokens = tokenizer.tokenize(sample[1])\n",
        "    sample_vecs = []\n",
        "    for token in tokens:\n",
        "      try:\n",
        "        sample_vecs.append(word_vectors[token])\n",
        "      except KeyError:\n",
        "        pass    # No matching token in the Google w2v vocab\n",
        "\n",
        "    vectorized_data.append(sample_vecs)\n",
        "\n",
        "  return vectorized_data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1Z3bpwfIfyq"
      },
      "source": [
        "You also need to collect the target values—0 for a negative review, 1 for a positive review—in the same order as the training samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC1qJoaNH5gF"
      },
      "source": [
        "def collect_expected(dataset):\n",
        "  '''Peel of the target values from the dataset'''\n",
        "  expected = []\n",
        "  for sample in dataset:\n",
        "    expected.append(sample[0])\n",
        "  \n",
        "  return expected"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAwhZvflImSK"
      },
      "source": [
        "And then you simply pass your data into those functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu5lfSiGIMwd"
      },
      "source": [
        "vectorized_data = tokenize_and_vectorize(dataset)\n",
        "expected = collect_expected(dataset)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u65UIysaI_cP"
      },
      "source": [
        "### Train/Test splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EgeyjZgJHu4"
      },
      "source": [
        "Next you’ll split the prepared data into a training set and a test set. You’re just going to split your imported dataset 80/20, but this ignores the folder of test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuO_XoY4JQd7"
      },
      "source": [
        "split_point = int(len(vectorized_data) * .8)\n",
        "\n",
        "x_train = vectorized_data[:split_point]\n",
        "y_train = expected[:split_point]\n",
        "x_test = vectorized_data[split_point:]\n",
        "y_test = expected[split_point:]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERwTnAjrKccV"
      },
      "source": [
        "### CNN parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ_9-wsmKReT"
      },
      "source": [
        "The next sets most of the hyperparameters for the net."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2Y0InHSKqGV"
      },
      "source": [
        "maxlen = 400          # holds the maximum review length\n",
        "batch_size = 32       # How many samples to show the net before backpropagating the error and updating the weights\n",
        "embedding_dims = 300  # Length of the token vectors you’ll create for passing into the convnet\n",
        "filters = 250         # Number of filters you’ll train\n",
        "kernel_size = 3       # Filters width; actual filters will each be a matrix of weights of size: embedding_dims x kernel_size, or 50 x 3 in your case\n",
        "hidden_dims = 250     # Number of neurons in the plain feed forward net at the end of the chain\n",
        "epochs = 2            # Number of times we will pass the entire training dataset through the network"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qAveehRMnBK"
      },
      "source": [
        "### Padding and truncating token sequence(sequences of vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di-Sa4YOMshT"
      },
      "source": [
        "Keras has a preprocessing helper method, `pad_sequences`, that in theory could be\n",
        "used to pad your input data, but unfortunately it works only with sequences of scalars, and you have sequences of vectors. \n",
        "\n",
        "Let’s write a helper function of your own to pad your input data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v51zAgevM0Kq"
      },
      "source": [
        "def pad_trunc(data, maxlen):\n",
        "  '''For a given dataset pad with zero vectors or truncate to maxlen'''\n",
        "  new_data = []\n",
        "\n",
        "  # Create a vector of 0's the length of our word vectors\n",
        "  zero_vector = []\n",
        "  for _ in range(len(data[0][0])):\n",
        "    zero_vector.append(0.0)\n",
        "  #zero_vector = [0.0 for _ in range(len(data[0][0]))]\n",
        "\n",
        "  for sample in data:\n",
        "    if len(sample) > maxlen:\n",
        "        temp = sample[:maxlen]\n",
        "    elif len(sample) < maxlen:\n",
        "        temp = sample\n",
        "        additional_elems = maxlen - len(sample)\n",
        "        for _ in range(additional_elems):\n",
        "            temp.append(zero_vector)\n",
        "    else:\n",
        "        temp = sample\n",
        "    new_data.append(temp)\n",
        "  \n",
        "  return new_data"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKWpu1TYODk2"
      },
      "source": [
        "Then you need to pass your train and test data into the padder/truncator. After that you can convert it to numpy arrays to make Keras happy. This is a tensor with the shape (number of samples, sequence length, word vector length) that you need for your CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGkviQPlFb_j"
      },
      "source": [
        "x_train = pad_trunc(x_train, maxlen)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNQZ3p6g7Wng"
      },
      "source": [
        "x_test = pad_trunc(x_test, maxlen)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6qc3GwKNxGx"
      },
      "source": [
        "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCFtBiGpaQ96"
      },
      "source": [
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mj96pjMOvxX"
      },
      "source": [
        "Phew; finally you’re ready to build a neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8CB1D4XOwQm"
      },
      "source": [
        "## Convolutional neural network architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ildkqgWOyny"
      },
      "source": [
        "Sequential is one of the base classes for neural networks in Keras. From here you can start to layer on the magic.\n",
        "\n",
        "The first piece you add is a convolutional layer. In this case, you assume that it’s okay that the output is of smaller dimension than the input, and you set the padding to 'valid'. Each filter will start its pass with its leftmost edge at the start of the sentence and stop with its rightmost edge on the last token.\n",
        "\n",
        "Each shift (stride) in the convolution will be one token. The kernel (window\n",
        "width) you already set to three tokens.And you’re using the 'relu' activation\n",
        "function. At each step, you’ll multiply the filter weight times the value in the\n",
        "three tokens it’s looking at (element-wise), sum up those answers, and pass them through if they’re greater than 0, else you output 0. That last passthrough of positive values and 0s is the rectified linear units activation function or ReLU.\n",
        "\n",
        "```python\n",
        "model = Sequential()\n",
        "# Add one Conv1D layer, which will learn word group filters of size kernel_size.\n",
        "model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1, input_shape=(maxlen, embedding_dims)))\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JRIDUL3uZbW"
      },
      "source": [
        "### Pooling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aCxJgxwuahb"
      },
      "source": [
        "Pooling is the convolutional neural network’s path to dimensionality reduction. In some ways, you’re speeding up the process by allowing for parallelization of the computation.\n",
        "\n",
        "The key idea is you’re going to evenly divide the output of each filter into a subsection. Then for each of those subsections, you’ll select or compute a representative value. And then you set the original output aside and use the collections of representative values as the input to the next layers.\n",
        "\n",
        "Usually, discarding data wouldn’t be the best course of action. But it turns out, it’s a path toward learning higher order representations of the source data. The filters are being trained to find patterns. The patterns are revealed in relationships between words and their neighbors! Just the kind of subtle   information you set out to find.\n",
        "\n",
        "In image processing, the first layers will tend to learn to be edge detectors, places where pixel densities rapidly shift from one side to the other. Later layers learn concepts like shape and texture. And layers after that may learn “content” or “meaning.” Similar processes will happen with text.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/pooling-layers.PNG?raw=1' width='800'/>\n",
        "\n",
        "You have two choices for pooling:\n",
        "* Average Pooling: Average is the more intuitive of the two in that by taking the average of the subset of values you would in theory retain the most data.\n",
        "* Max Pooling: has an interesting property, in that by taking the largest activation value for the given region, the network sees\n",
        "that subsection’s most prominent feature. The network has a path toward learning\n",
        "what it should look at, regardless of exact pixel-level position!\n",
        "\n",
        "In addition to dimensionality reduction and the computational savings that come\n",
        "with it, you gain something else special: **location invariance**. If an original input element is jostled slightly in position in a similar but distinct input sample, the max pooling layer will still output something similar. This is a huge boon in the image recognition world, and it serves a similar purpose in natural language processing.\n",
        "\n",
        "In Keras, you’re using the GlobalMaxPooling1D layer.\n",
        "\n",
        "```python\n",
        "model.add(GlobalMaxPooling1D())\n",
        "```\n",
        "Now for each input sample you have a 1D vector that the network thinks is a good representation of that input sample. This is a semantic representation of the input—a crude one to be sure. And it will only be semantic in the context of the training target, which is sentiment. There won’t be an encoding of the content of the movie being reviewed, say, just an encoding of its sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-Gqkg8I0B_b"
      },
      "source": [
        "### Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdBO-Z8T0DI3"
      },
      "source": [
        "Dropout is a special technique developed to prevent overfitting in neural networks. It isn’t specific to natural language processing, but it does work well here.\n",
        "\n",
        "The idea is that on each training pass, if you “turn off” a certain percentage of the input going to the next layer, randomly chosen on each pass, the model will be less likely to learn the specifics of the training set, “overfitting,” and instead learn more nuanced representations of the patterns in the data and thereby be able to generalize and make accurate predictions when it sees completely novel data.\n",
        "\n",
        "The parameter passed into the Dropout layer in Keras is the percentage of the inputs to randomly turn off. In this example, only 80% of the embedding data, randomly chosen for each training sample, will pass into the next layer as it is. The rest will go in as 0s. A 20% dropout setting is common, but a dropout of up to 50% can have good results.\n",
        "\n",
        "```python\n",
        "# You start with a vanilla fully connected hidden layer and then tack on dropout and ReLU.\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkF0zyt4s6IC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cfb9e556-5431-4931-8e99-eceed55453a7"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# we add a Convolution1D, which will learn filters word group filters of size filter_length\n",
        "model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1, input_shape=(maxlen, embedding_dims)))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "# vanilla hidden layer\n",
        "model.add(Dense(hidden_dims))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "# compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5aPTrww8HZ3"
      },
      "source": [
        "You would like to save the model state after training.\n",
        "Because you aren’t going to hold the model in memory for now, you can grab its\n",
        "structure in a JSON file and save the trained weights in another file for later reinstantiation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viQIQimB8enz"
      },
      "source": [
        "model_structure = model.to_json()   # Note that this doesn’t save the weights of the network, only the structure.\n",
        "\n",
        "# Save your trained model before you lose it!\n",
        "with open('cnn_model.json', 'w') as json_file:\n",
        "  json_file.write(model_structure)\n",
        "model.save_weights('cnn_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhheVs8B-k6A"
      },
      "source": [
        "Now your trained model will be persisted on disk; should it converge, you won’t have to train it again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpssFGkjFrIi"
      },
      "source": [
        "## Loading saved model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0-q-H2FFuXl"
      },
      "source": [
        "After you have a trained model, you can then pass in a novel sample and see what the network thinks. This could be an incoming chat message or tweet to your bot; in your case, it’ll be a made-up example.\n",
        "\n",
        "First, reinstate your trained model, if it’s no longer in memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vT0JsbEF3Ou"
      },
      "source": [
        "from tf.keras.models import model_from_json\n",
        "\n",
        "with open('cnn_model.json', 'r') as json_file:\n",
        "  json_string = json_file.read()\n",
        "\n",
        "model = model_from_json(json_string)\n",
        "model.load_weights('cnn_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5ccRG3FOg_j"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ko8dLd8N0Mr"
      },
      "source": [
        "Let’s make up a sentence with an obvious negative sentiment and see what the network has to say about it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDoFDTjjN1st"
      },
      "source": [
        "sample_1 = \"\"\"\n",
        "I'm hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  \n",
        "The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ2hHRP4OKhG"
      },
      "source": [
        "With the model pretrained, testing a new sample is quick. The are still thousands and\n",
        "thousands of calculations to do, but for each sample you only need one forward pass\n",
        "and no backpropagation to get a result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chBtRjF7OAlw"
      },
      "source": [
        "# You pass a dummy value in the first element of the tuple just because\n",
        "# your helper expects it from the way you processed the initial data.\n",
        "# That value won’t ever see the network, so it can be anything.\n",
        "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
        "\n",
        "# Tokenize returns a list of the data (length 1 here)\n",
        "test_vec_list = pad_trunc(vec_list, maxlen)\n",
        "\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "model.predict(test_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTHN4qssPkN7"
      },
      "source": [
        "model.predict_classes(test_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d4_Zx0hQP8Y"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtKEj7dPQVSd"
      },
      "source": [
        "We touched briefly on the output of the convolutional layers (before you step into\n",
        "the feedforward layer). This semantic representation is an important artifact. It’s in many\n",
        "ways a numerical representation of the thought and details of the input text. Specifically\n",
        "in this case, it’s a representation of the thought and details through the lens of sentiment\n",
        "analysis, as all the “learning” that happened was in response to whether the\n",
        "sample was labeled as a positive or negative sentiment. The vector that was generated\n",
        "by training on a set that was labeled for another specific topic and classified as such\n",
        "would contain much different information. Using the intermediary vector directly\n",
        "from a convolutional neural net isn’t common, but other neural network architectures where the details of that intermediary\n",
        "vector become important, and in some cases are the end goal itself.\n",
        "\n",
        "Why would you choose a CNN for your NLP classification task? The main benefit it\n",
        "provides is efficiency. In many ways, because of the pooling layers and the limits created\n",
        "by filter size (though you can make your filters large if you wish), you’re throwing\n",
        "away a good deal of information. But that doesn’t mean they aren’t useful models. As\n",
        "you’ve seen, they were able to efficiently detect and predict sentiment over a relatively\n",
        "large dataset, and even though you relied on the Word2vec embeddings, CNNs can\n",
        "perform on much less rich embeddings without mapping the entire language.\n",
        "\n"
      ]
    }
  ]
}