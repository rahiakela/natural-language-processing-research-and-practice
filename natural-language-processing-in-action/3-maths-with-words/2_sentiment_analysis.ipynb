{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-sentiment-analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO4yc4Vc1AkBKRqTZrW9uuo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-in-action/blob/3-build-vocabulary-using-word-tokenization/2_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "970zBnEkmw9E",
        "colab_type": "text"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA4hBJ8Mm4mH",
        "colab_type": "text"
      },
      "source": [
        "Whether you use raw single-word tokens, n-grams, stems, or lemmas in your NLP pipeline, each of those tokens contains some information. An important part of this information is the word’s sentiment—the overall feeling or emotion that the word invokes. This sentiment analysis—measuring the sentiment of phrases or chunks of text—is a common application of NLP. In many companies it’s the main thing an NLP engineer is asked to do.\n",
        "\n",
        "An NLP pipeline can process a large quantity of user feedback\n",
        "quickly and objectively, with less chance for bias. And an NLP pipeline can output a numerical rating of the positivity or negativity or any other emotional quality of the text.\n",
        "\n",
        "There are two approaches to sentiment analysis:\n",
        "- **A rule-based algorithm composed by a human**\n",
        "- **A machine learning model learned from data by a machine**\n",
        "\n",
        "The first approach to sentiment analysis uses human-designed rules, sometimes called heuristics, to measure sentiment. A common rule-based approach to sentiment analysis is to find keywords in the text and map each one to numerical scores or weights in a dictionary or “mapping”—a Python dict, for example.\n",
        "\n",
        "The “rule” in your algorithm would be to add up these scores for each\n",
        "keyword in a document that you can find in your dictionary of sentiment scores. Of course you need to hand-compose this dictionary of keywords and their sentiment scores before you can run this algorithm on a body of text.\n",
        "\n",
        "The second approach, machine learning, relies on a labeled set of statements or documents to train a machine learning model to create those rules. A machine learning sentiment model is trained to process input text and output a numerical value for the sentiment you are trying to measure, like positivity or spamminess or trolliness. \n",
        "\n",
        "For the machine learning approach, you need a lot of data, text labeled with the “right” sentiment score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g03ueAasn3Fr",
        "colab_type": "text"
      },
      "source": [
        "## VADER—A rule-based sentiment analyzer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htvJrlyIn3_-",
        "colab_type": "text"
      },
      "source": [
        "Hutto and Gilbert at GA Tech came up with one of the first successful rule-based sentiment analysis algorithms. They called their algorithm **VADER**, for **V**alence **A**ware **D**ictionary for s**E**ntiment **R**easoning. Many NLP packages implement some form of this algorithm. The NLTK package has an implementation of the VADER algorithm in nltk.sentiment.vader. Hutto himself maintains the Python package vaderSentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxCbf1auoo4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's install VADER\n",
        "!pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1A5jM8ENoxjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j5l1pPwpADP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bdb94633-9df9-4ad8-c6ef-20806baee57f"
      },
      "source": [
        "# SentimentIntensityAnalyzer.lexicon contains that dictionary of tokens and their scores that we talked about.\n",
        "sa = SentimentIntensityAnalyzer()\n",
        "sa.lexicon"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'$:': -1.5,\n",
              " '%)': -0.4,\n",
              " '%-)': -1.5,\n",
              " '&-:': -0.4,\n",
              " '&:': -0.7,\n",
              " \"( '}{' )\": 1.6,\n",
              " '(%': -0.9,\n",
              " \"('-:\": 2.2,\n",
              " \"(':\": 2.3,\n",
              " '((-:': 2.1,\n",
              " '(*': 1.1,\n",
              " '(-%': -0.7,\n",
              " '(-*': 1.3,\n",
              " '(-:': 1.6,\n",
              " '(-:0': 2.8,\n",
              " '(-:<': -0.4,\n",
              " '(-:o': 1.5,\n",
              " '(-:O': 1.5,\n",
              " '(-:{': -0.1,\n",
              " '(-:|>*': 1.9,\n",
              " '(-;': 1.3,\n",
              " '(-;|': 2.1,\n",
              " '(8': 2.6,\n",
              " '(:': 2.2,\n",
              " '(:0': 2.4,\n",
              " '(:<': -0.2,\n",
              " '(:o': 2.5,\n",
              " '(:O': 2.5,\n",
              " '(;': 1.1,\n",
              " '(;<': 0.3,\n",
              " '(=': 2.2,\n",
              " '(?:': 2.1,\n",
              " '(^:': 1.5,\n",
              " '(^;': 1.5,\n",
              " '(^;0': 2.0,\n",
              " '(^;o': 1.9,\n",
              " '(o:': 1.6,\n",
              " \")':\": -2.0,\n",
              " \")-':\": -2.1,\n",
              " ')-:': -2.1,\n",
              " ')-:<': -2.2,\n",
              " ')-:{': -2.1,\n",
              " '):': -1.8,\n",
              " '):<': -1.9,\n",
              " '):{': -2.3,\n",
              " ');<': -2.6,\n",
              " '*)': 0.6,\n",
              " '*-)': 0.3,\n",
              " '*-:': 2.1,\n",
              " '*-;': 2.4,\n",
              " '*:': 1.9,\n",
              " '*<|:-)': 1.6,\n",
              " '*\\\\0/*': 2.3,\n",
              " '*^:': 1.6,\n",
              " ',-:': 1.2,\n",
              " \"---'-;-{@\": 2.3,\n",
              " '--<--<@': 2.2,\n",
              " '.-:': -1.2,\n",
              " '..###-:': -1.7,\n",
              " '..###:': -1.9,\n",
              " '/-:': -1.3,\n",
              " '/:': -1.3,\n",
              " '/:<': -1.4,\n",
              " '/=': -0.9,\n",
              " '/^:': -1.0,\n",
              " '/o:': -1.4,\n",
              " '0-8': 0.1,\n",
              " '0-|': -1.2,\n",
              " '0:)': 1.9,\n",
              " '0:-)': 1.4,\n",
              " '0:-3': 1.5,\n",
              " '0:03': 1.9,\n",
              " '0;^)': 1.6,\n",
              " '0_o': -0.3,\n",
              " '10q': 2.1,\n",
              " '1337': 2.1,\n",
              " '143': 3.2,\n",
              " '1432': 2.6,\n",
              " '14aa41': 2.4,\n",
              " '182': -2.9,\n",
              " '187': -3.1,\n",
              " '2g2b4g': 2.8,\n",
              " '2g2bt': -0.1,\n",
              " '2qt': 2.1,\n",
              " '3:(': -2.2,\n",
              " '3:)': 0.5,\n",
              " '3:-(': -2.3,\n",
              " '3:-)': -1.4,\n",
              " '4col': -2.2,\n",
              " '4q': -3.1,\n",
              " '5fs': 1.5,\n",
              " '8)': 1.9,\n",
              " '8-d': 1.7,\n",
              " '8-o': -0.3,\n",
              " '86': -1.6,\n",
              " '8d': 2.9,\n",
              " ':###..': -2.4,\n",
              " ':$': -0.2,\n",
              " ':&': -0.6,\n",
              " \":'(\": -2.2,\n",
              " \":')\": 2.3,\n",
              " \":'-(\": -2.4,\n",
              " \":'-)\": 2.7,\n",
              " ':(': -1.9,\n",
              " ':)': 2.0,\n",
              " ':*': 2.5,\n",
              " ':-###..': -2.5,\n",
              " ':-&': -0.5,\n",
              " ':-(': -1.5,\n",
              " ':-)': 1.3,\n",
              " ':-))': 2.8,\n",
              " ':-*': 1.7,\n",
              " ':-,': 1.1,\n",
              " ':-.': -0.9,\n",
              " ':-/': -1.2,\n",
              " ':-<': -1.5,\n",
              " ':-d': 2.3,\n",
              " ':-D': 2.3,\n",
              " ':-o': 0.1,\n",
              " ':-p': 1.5,\n",
              " ':-[': -1.6,\n",
              " ':-\\\\': -0.9,\n",
              " ':-c': -1.3,\n",
              " ':-|': -0.7,\n",
              " ':-||': -2.5,\n",
              " ':-Þ': 0.9,\n",
              " ':/': -1.4,\n",
              " ':3': 2.3,\n",
              " ':<': -2.1,\n",
              " ':>': 2.1,\n",
              " ':?)': 1.3,\n",
              " ':?c': -1.6,\n",
              " ':@': -2.5,\n",
              " ':d': 2.3,\n",
              " ':D': 2.3,\n",
              " ':l': -1.7,\n",
              " ':o': -0.4,\n",
              " ':p': 1.0,\n",
              " ':s': -1.2,\n",
              " ':[': -2.0,\n",
              " ':\\\\': -1.3,\n",
              " ':]': 2.2,\n",
              " ':^)': 2.1,\n",
              " ':^*': 2.6,\n",
              " ':^/': -1.2,\n",
              " ':^\\\\': -1.0,\n",
              " ':^|': -1.0,\n",
              " ':c': -2.1,\n",
              " ':c)': 2.0,\n",
              " ':o)': 2.1,\n",
              " ':o/': -1.4,\n",
              " ':o\\\\': -1.1,\n",
              " ':o|': -0.6,\n",
              " ':P': 1.4,\n",
              " ':{': -1.9,\n",
              " ':|': -0.4,\n",
              " ':}': 2.1,\n",
              " ':Þ': 1.1,\n",
              " ';)': 0.9,\n",
              " ';-)': 1.0,\n",
              " ';-*': 2.2,\n",
              " ';-]': 0.7,\n",
              " ';d': 0.8,\n",
              " ';D': 0.8,\n",
              " ';]': 0.6,\n",
              " ';^)': 1.4,\n",
              " '</3': -3.0,\n",
              " '<3': 1.9,\n",
              " '<:': 2.1,\n",
              " '<:-|': -1.4,\n",
              " '=)': 2.2,\n",
              " '=-3': 2.0,\n",
              " '=-d': 2.4,\n",
              " '=-D': 2.4,\n",
              " '=/': -1.4,\n",
              " '=3': 2.1,\n",
              " '=d': 2.3,\n",
              " '=D': 2.3,\n",
              " '=l': -1.2,\n",
              " '=\\\\': -1.2,\n",
              " '=]': 1.6,\n",
              " '=p': 1.3,\n",
              " '=|': -0.8,\n",
              " '>-:': -2.0,\n",
              " '>.<': -1.3,\n",
              " '>:': -2.1,\n",
              " '>:(': -2.7,\n",
              " '>:)': 0.4,\n",
              " '>:-(': -2.7,\n",
              " '>:-)': -0.4,\n",
              " '>:/': -1.6,\n",
              " '>:o': -1.2,\n",
              " '>:p': 1.0,\n",
              " '>:[': -2.1,\n",
              " '>:\\\\': -1.7,\n",
              " '>;(': -2.9,\n",
              " '>;)': 0.1,\n",
              " '>_>^': 2.1,\n",
              " '@:': -2.1,\n",
              " '@>-->--': 2.1,\n",
              " \"@}-;-'---\": 2.2,\n",
              " 'aas': 2.5,\n",
              " 'aayf': 2.7,\n",
              " 'afu': -2.9,\n",
              " 'alol': 2.8,\n",
              " 'ambw': 2.9,\n",
              " 'aml': 3.4,\n",
              " 'atab': -1.9,\n",
              " 'awol': -1.3,\n",
              " 'ayc': 0.2,\n",
              " 'ayor': -1.2,\n",
              " 'aug-00': 0.3,\n",
              " 'bfd': -2.7,\n",
              " 'bfe': -2.6,\n",
              " 'bff': 2.9,\n",
              " 'bffn': 1.0,\n",
              " 'bl': 2.3,\n",
              " 'bsod': -2.2,\n",
              " 'btd': -2.1,\n",
              " 'btdt': -0.1,\n",
              " 'bz': 0.4,\n",
              " 'b^d': 2.6,\n",
              " 'cwot': -2.3,\n",
              " \"d-':\": -2.5,\n",
              " 'd8': -3.2,\n",
              " 'd:': 1.2,\n",
              " 'd:<': -3.2,\n",
              " 'd;': -2.9,\n",
              " 'd=': 1.5,\n",
              " 'doa': -2.3,\n",
              " 'dx': -3.0,\n",
              " 'ez': 1.5,\n",
              " 'fav': 2.0,\n",
              " 'fcol': -1.8,\n",
              " 'ff': 1.8,\n",
              " 'ffs': -2.8,\n",
              " 'fkm': -2.4,\n",
              " 'foaf': 1.8,\n",
              " 'ftw': 2.0,\n",
              " 'fu': -3.7,\n",
              " 'fubar': -3.0,\n",
              " 'fwb': 2.5,\n",
              " 'fyi': 0.8,\n",
              " 'fysa': 0.4,\n",
              " 'g1': 1.4,\n",
              " 'gg': 1.2,\n",
              " 'gga': 1.7,\n",
              " 'gigo': -0.6,\n",
              " 'gj': 2.0,\n",
              " 'gl': 1.3,\n",
              " 'gla': 2.5,\n",
              " 'gn': 1.2,\n",
              " 'gr8': 2.7,\n",
              " 'grrr': -0.4,\n",
              " 'gt': 1.1,\n",
              " 'h&k': 2.3,\n",
              " 'hagd': 2.2,\n",
              " 'hagn': 2.2,\n",
              " 'hago': 1.2,\n",
              " 'hak': 1.9,\n",
              " 'hand': 2.2,\n",
              " 'heart': 3.2,\n",
              " 'hearts': 3.3,\n",
              " 'hho1/2k': 1.4,\n",
              " 'hhoj': 2.0,\n",
              " 'hhok': 0.9,\n",
              " 'hugz': 2.0,\n",
              " 'hi5': 1.9,\n",
              " 'idk': -0.4,\n",
              " 'ijs': 0.7,\n",
              " 'ilu': 3.4,\n",
              " 'iluaaf': 2.7,\n",
              " 'ily': 3.4,\n",
              " 'ily2': 2.6,\n",
              " 'iou': 0.7,\n",
              " 'iyq': 2.3,\n",
              " 'j/j': 2.0,\n",
              " 'j/k': 1.6,\n",
              " 'j/p': 1.4,\n",
              " 'j/t': -0.2,\n",
              " 'j/w': 1.0,\n",
              " 'j4f': 1.4,\n",
              " 'j4g': 1.7,\n",
              " 'jho': 0.8,\n",
              " 'jhomf': 1.0,\n",
              " 'jj': 1.0,\n",
              " 'jk': 0.9,\n",
              " 'jp': 0.8,\n",
              " 'jt': 0.9,\n",
              " 'jw': 1.6,\n",
              " 'jealz': -1.2,\n",
              " 'k4y': 2.3,\n",
              " 'kfy': 2.3,\n",
              " 'kia': -3.2,\n",
              " 'kk': 1.5,\n",
              " 'kmuf': 2.2,\n",
              " 'l': 2.0,\n",
              " 'l&r': 2.2,\n",
              " 'laoj': 1.3,\n",
              " 'lmao': 2.9,\n",
              " 'lmbao': 1.8,\n",
              " 'lmfao': 2.5,\n",
              " 'lmso': 2.7,\n",
              " 'lol': 1.8,\n",
              " 'lolz': 2.7,\n",
              " 'lts': 1.6,\n",
              " 'ly': 2.6,\n",
              " 'ly4e': 2.7,\n",
              " 'lya': 3.3,\n",
              " 'lyb': 3.0,\n",
              " 'lyl': 3.1,\n",
              " 'lylab': 2.7,\n",
              " 'lylas': 2.6,\n",
              " 'lylb': 1.6,\n",
              " 'm8': 1.4,\n",
              " 'mia': -1.2,\n",
              " 'mml': 2.0,\n",
              " 'mofo': -2.4,\n",
              " 'muah': 2.3,\n",
              " 'mubar': -1.0,\n",
              " 'musm': 0.9,\n",
              " 'mwah': 2.5,\n",
              " 'n1': 1.9,\n",
              " 'nbd': 1.3,\n",
              " 'nbif': -0.5,\n",
              " 'nfc': -2.7,\n",
              " 'nfw': -2.4,\n",
              " 'nh': 2.2,\n",
              " 'nimby': -0.8,\n",
              " 'nimjd': -0.7,\n",
              " 'nimq': -0.2,\n",
              " 'nimy': -1.4,\n",
              " 'nitl': -1.5,\n",
              " 'nme': -2.1,\n",
              " 'noyb': -0.7,\n",
              " 'np': 1.4,\n",
              " 'ntmu': 1.4,\n",
              " 'o-8': -0.5,\n",
              " 'o-:': -0.3,\n",
              " 'o-|': -1.1,\n",
              " 'o.o': -0.8,\n",
              " 'O.o': -0.6,\n",
              " 'o.O': -0.6,\n",
              " 'o:': -0.2,\n",
              " 'o:)': 1.5,\n",
              " 'o:-)': 2.0,\n",
              " 'o:-3': 2.2,\n",
              " 'o:3': 2.3,\n",
              " 'o:<': -0.3,\n",
              " 'o;^)': 1.6,\n",
              " 'ok': 1.2,\n",
              " 'o_o': -0.5,\n",
              " 'O_o': -0.5,\n",
              " 'o_O': -0.5,\n",
              " 'pita': -2.4,\n",
              " 'pls': 0.3,\n",
              " 'plz': 0.3,\n",
              " 'pmbi': 0.8,\n",
              " 'pmfji': 0.3,\n",
              " 'pmji': 0.7,\n",
              " 'po': -2.6,\n",
              " 'ptl': 2.6,\n",
              " 'pu': -1.1,\n",
              " 'qq': -2.2,\n",
              " 'qt': 1.8,\n",
              " 'r&r': 2.4,\n",
              " 'rofl': 2.7,\n",
              " 'roflmao': 2.5,\n",
              " 'rotfl': 2.6,\n",
              " 'rotflmao': 2.8,\n",
              " 'rotflmfao': 2.5,\n",
              " 'rotflol': 3.0,\n",
              " 'rotgl': 2.9,\n",
              " 'rotglmao': 1.8,\n",
              " 's:': -1.1,\n",
              " 'sapfu': -1.1,\n",
              " 'sete': 2.8,\n",
              " 'sfete': 2.7,\n",
              " 'sgtm': 2.4,\n",
              " 'slap': 0.6,\n",
              " 'slaw': 2.1,\n",
              " 'smh': -1.3,\n",
              " 'snafu': -2.5,\n",
              " 'sob': -1.0,\n",
              " 'swak': 2.3,\n",
              " 'tgif': 2.3,\n",
              " 'thks': 1.4,\n",
              " 'thx': 1.5,\n",
              " 'tia': 2.3,\n",
              " 'tmi': -0.3,\n",
              " 'tnx': 1.1,\n",
              " 'true': 1.8,\n",
              " 'tx': 1.5,\n",
              " 'txs': 1.1,\n",
              " 'ty': 1.6,\n",
              " 'tyvm': 2.5,\n",
              " 'urw': 1.9,\n",
              " 'vbg': 2.1,\n",
              " 'vbs': 3.1,\n",
              " 'vip': 2.3,\n",
              " 'vwd': 2.6,\n",
              " 'vwp': 2.1,\n",
              " 'wag': -0.2,\n",
              " 'wd': 2.7,\n",
              " 'wilco': 0.9,\n",
              " 'wp': 1.0,\n",
              " 'wtf': -2.8,\n",
              " 'wtg': 2.1,\n",
              " 'wth': -2.4,\n",
              " 'x-d': 2.6,\n",
              " 'x-p': 1.7,\n",
              " 'xd': 2.8,\n",
              " 'xlnt': 3.0,\n",
              " 'xoxo': 3.0,\n",
              " 'xoxozzz': 2.3,\n",
              " 'xp': 1.6,\n",
              " 'xqzt': 1.6,\n",
              " 'xtc': 0.8,\n",
              " 'yolo': 1.1,\n",
              " 'yoyo': 0.4,\n",
              " 'yvw': 1.6,\n",
              " 'yw': 1.8,\n",
              " 'ywia': 2.5,\n",
              " 'zzz': -1.2,\n",
              " '[-;': 0.5,\n",
              " '[:': 1.3,\n",
              " '[;': 1.0,\n",
              " '[=': 1.7,\n",
              " '\\\\-:': -1.0,\n",
              " '\\\\:': -1.0,\n",
              " '\\\\:<': -1.7,\n",
              " '\\\\=': -1.1,\n",
              " '\\\\^:': -1.3,\n",
              " '\\\\o/': 2.2,\n",
              " '\\\\o:': -1.2,\n",
              " ']-:': -2.1,\n",
              " ']:': -1.6,\n",
              " ']:<': -2.5,\n",
              " '^<_<': 1.4,\n",
              " '^urs': -2.8,\n",
              " 'abandon': -1.9,\n",
              " 'abandoned': -2.0,\n",
              " 'abandoner': -1.9,\n",
              " 'abandoners': -1.9,\n",
              " 'abandoning': -1.6,\n",
              " 'abandonment': -2.4,\n",
              " 'abandonments': -1.7,\n",
              " 'abandons': -1.3,\n",
              " 'abducted': -2.3,\n",
              " 'abduction': -2.8,\n",
              " 'abductions': -2.0,\n",
              " 'abhor': -2.0,\n",
              " 'abhorred': -2.4,\n",
              " 'abhorrent': -3.1,\n",
              " 'abhors': -2.9,\n",
              " 'abilities': 1.0,\n",
              " 'ability': 1.3,\n",
              " 'aboard': 0.1,\n",
              " 'absentee': -1.1,\n",
              " 'absentees': -0.8,\n",
              " 'absolve': 1.2,\n",
              " 'absolved': 1.5,\n",
              " 'absolves': 1.3,\n",
              " 'absolving': 1.6,\n",
              " 'abuse': -3.2,\n",
              " 'abused': -2.3,\n",
              " 'abuser': -2.6,\n",
              " 'abusers': -2.6,\n",
              " 'abuses': -2.6,\n",
              " 'abusing': -2.0,\n",
              " 'abusive': -3.2,\n",
              " 'abusively': -2.8,\n",
              " 'abusiveness': -2.5,\n",
              " 'abusivenesses': -3.0,\n",
              " 'accept': 1.6,\n",
              " 'acceptabilities': 1.6,\n",
              " 'acceptability': 1.1,\n",
              " 'acceptable': 1.3,\n",
              " 'acceptableness': 1.3,\n",
              " 'acceptably': 1.5,\n",
              " 'acceptance': 2.0,\n",
              " 'acceptances': 1.7,\n",
              " 'acceptant': 1.6,\n",
              " 'acceptation': 1.3,\n",
              " 'acceptations': 0.9,\n",
              " 'accepted': 1.1,\n",
              " 'accepting': 1.6,\n",
              " 'accepts': 1.3,\n",
              " 'accident': -2.1,\n",
              " 'accidental': -0.3,\n",
              " 'accidentally': -1.4,\n",
              " 'accidents': -1.3,\n",
              " 'accomplish': 1.8,\n",
              " 'accomplished': 1.9,\n",
              " 'accomplishes': 1.7,\n",
              " 'accusation': -1.0,\n",
              " 'accusations': -1.3,\n",
              " 'accuse': -0.8,\n",
              " 'accused': -1.2,\n",
              " 'accuses': -1.4,\n",
              " 'accusing': -0.7,\n",
              " 'ache': -1.6,\n",
              " 'ached': -1.6,\n",
              " 'aches': -1.0,\n",
              " 'achievable': 1.3,\n",
              " 'aching': -2.2,\n",
              " 'acquit': 0.8,\n",
              " 'acquits': 0.1,\n",
              " 'acquitted': 1.0,\n",
              " 'acquitting': 1.3,\n",
              " 'acrimonious': -1.7,\n",
              " 'active': 1.7,\n",
              " 'actively': 1.3,\n",
              " 'activeness': 0.6,\n",
              " 'activenesses': 0.8,\n",
              " 'actives': 1.1,\n",
              " 'adequate': 0.9,\n",
              " 'admirability': 2.4,\n",
              " 'admirable': 2.6,\n",
              " 'admirableness': 2.2,\n",
              " 'admirably': 2.5,\n",
              " 'admiral': 1.3,\n",
              " 'admirals': 1.5,\n",
              " 'admiralties': 1.6,\n",
              " 'admiralty': 1.2,\n",
              " 'admiration': 2.5,\n",
              " 'admirations': 1.6,\n",
              " 'admire': 2.1,\n",
              " 'admired': 2.3,\n",
              " 'admirer': 1.8,\n",
              " 'admirers': 1.7,\n",
              " 'admires': 1.5,\n",
              " 'admiring': 1.6,\n",
              " 'admiringly': 2.3,\n",
              " 'admit': 0.8,\n",
              " 'admits': 1.2,\n",
              " 'admitted': 0.4,\n",
              " 'admonished': -1.9,\n",
              " 'adopt': 0.7,\n",
              " 'adopts': 0.7,\n",
              " 'adorability': 2.2,\n",
              " 'adorable': 2.2,\n",
              " 'adorableness': 2.5,\n",
              " 'adorably': 2.1,\n",
              " 'adoration': 2.9,\n",
              " 'adorations': 2.2,\n",
              " 'adore': 2.6,\n",
              " 'adored': 1.8,\n",
              " 'adorer': 1.7,\n",
              " 'adorers': 2.1,\n",
              " 'adores': 1.6,\n",
              " 'adoring': 2.6,\n",
              " 'adoringly': 2.4,\n",
              " 'adorn': 0.9,\n",
              " 'adorned': 0.8,\n",
              " 'adorner': 1.3,\n",
              " 'adorners': 0.9,\n",
              " 'adorning': 1.0,\n",
              " 'adornment': 1.3,\n",
              " 'adornments': 0.8,\n",
              " 'adorns': 0.5,\n",
              " 'advanced': 1.0,\n",
              " 'advantage': 1.0,\n",
              " 'advantaged': 1.4,\n",
              " 'advantageous': 1.5,\n",
              " 'advantageously': 1.9,\n",
              " 'advantageousness': 1.6,\n",
              " 'advantages': 1.5,\n",
              " 'advantaging': 1.6,\n",
              " 'adventure': 1.3,\n",
              " 'adventured': 1.3,\n",
              " 'adventurer': 1.2,\n",
              " 'adventurers': 0.9,\n",
              " 'adventures': 1.4,\n",
              " 'adventuresome': 1.7,\n",
              " 'adventuresomeness': 1.3,\n",
              " 'adventuress': 0.8,\n",
              " 'adventuresses': 1.4,\n",
              " 'adventuring': 2.3,\n",
              " 'adventurism': 1.5,\n",
              " 'adventurist': 1.4,\n",
              " 'adventuristic': 1.7,\n",
              " 'adventurists': 1.2,\n",
              " 'adventurous': 1.4,\n",
              " 'adventurously': 1.3,\n",
              " 'adventurousness': 1.8,\n",
              " 'adversarial': -1.5,\n",
              " 'adversaries': -1.0,\n",
              " 'adversary': -0.8,\n",
              " 'adversative': -1.2,\n",
              " 'adversatively': -0.1,\n",
              " 'adversatives': -1.0,\n",
              " 'adverse': -1.5,\n",
              " 'adversely': -0.8,\n",
              " 'adverseness': -0.6,\n",
              " 'adversities': -1.5,\n",
              " 'adversity': -1.8,\n",
              " 'affected': -0.6,\n",
              " 'affection': 2.4,\n",
              " 'affectional': 1.9,\n",
              " 'affectionally': 1.5,\n",
              " 'affectionate': 1.9,\n",
              " 'affectionately': 2.2,\n",
              " 'affectioned': 1.8,\n",
              " 'affectionless': -2.0,\n",
              " 'affections': 1.5,\n",
              " 'afflicted': -1.5,\n",
              " 'affronted': 0.2,\n",
              " 'aggravate': -2.5,\n",
              " 'aggravated': -1.9,\n",
              " 'aggravates': -1.9,\n",
              " 'aggravating': -1.2,\n",
              " 'aggress': -1.3,\n",
              " 'aggressed': -1.4,\n",
              " 'aggresses': -0.5,\n",
              " 'aggressing': -0.6,\n",
              " 'aggression': -1.2,\n",
              " 'aggressions': -1.3,\n",
              " 'aggressive': -0.6,\n",
              " 'aggressively': -1.3,\n",
              " 'aggressiveness': -1.8,\n",
              " 'aggressivities': -1.4,\n",
              " 'aggressivity': -0.6,\n",
              " 'aggressor': -0.8,\n",
              " 'aggressors': -0.9,\n",
              " 'aghast': -1.9,\n",
              " 'agitate': -1.7,\n",
              " 'agitated': -2.0,\n",
              " 'agitatedly': -1.6,\n",
              " 'agitates': -1.4,\n",
              " 'agitating': -1.8,\n",
              " 'agitation': -1.0,\n",
              " 'agitational': -1.2,\n",
              " 'agitations': -1.3,\n",
              " 'agitative': -1.3,\n",
              " 'agitato': -0.1,\n",
              " 'agitator': -1.4,\n",
              " 'agitators': -2.1,\n",
              " 'agog': 1.9,\n",
              " 'agonise': -2.1,\n",
              " 'agonised': -2.3,\n",
              " 'agonises': -2.4,\n",
              " 'agonising': -1.5,\n",
              " 'agonize': -2.3,\n",
              " 'agonized': -2.2,\n",
              " 'agonizes': -2.3,\n",
              " 'agonizing': -2.7,\n",
              " 'agonizingly': -2.3,\n",
              " 'agony': -1.8,\n",
              " 'agree': 1.5,\n",
              " 'agreeability': 1.9,\n",
              " 'agreeable': 1.8,\n",
              " 'agreeableness': 1.8,\n",
              " 'agreeablenesses': 1.3,\n",
              " 'agreeably': 1.6,\n",
              " 'agreed': 1.1,\n",
              " 'agreeing': 1.4,\n",
              " 'agreement': 2.2,\n",
              " 'agreements': 1.1,\n",
              " 'agrees': 0.8,\n",
              " 'alarm': -1.4,\n",
              " 'alarmed': -1.4,\n",
              " 'alarming': -0.5,\n",
              " 'alarmingly': -2.6,\n",
              " 'alarmism': -0.3,\n",
              " 'alarmists': -1.1,\n",
              " 'alarms': -1.1,\n",
              " 'alas': -1.1,\n",
              " 'alert': 1.2,\n",
              " 'alienation': -1.1,\n",
              " 'alive': 1.6,\n",
              " 'allergic': -1.2,\n",
              " 'allow': 0.9,\n",
              " 'alone': -1.0,\n",
              " 'alright': 1.0,\n",
              " 'amaze': 2.5,\n",
              " 'amazed': 2.2,\n",
              " 'amazedly': 2.1,\n",
              " 'amazement': 2.5,\n",
              " 'amazements': 2.2,\n",
              " 'amazes': 2.2,\n",
              " 'amazing': 2.8,\n",
              " 'amazon': 0.7,\n",
              " 'amazonite': 0.2,\n",
              " 'amazons': -0.1,\n",
              " 'amazonstone': 1.0,\n",
              " 'amazonstones': 0.2,\n",
              " 'ambitious': 2.1,\n",
              " 'ambivalent': 0.5,\n",
              " 'amor': 3.0,\n",
              " 'amoral': -1.6,\n",
              " 'amoralism': -0.7,\n",
              " 'amoralisms': -0.7,\n",
              " 'amoralities': -1.2,\n",
              " 'amorality': -1.5,\n",
              " 'amorally': -1.0,\n",
              " 'amoretti': 0.2,\n",
              " 'amoretto': 0.6,\n",
              " 'amorettos': 0.3,\n",
              " 'amorino': 1.2,\n",
              " 'amorist': 1.6,\n",
              " 'amoristic': 1.0,\n",
              " 'amorists': 0.1,\n",
              " 'amoroso': 2.3,\n",
              " 'amorous': 1.8,\n",
              " 'amorously': 2.3,\n",
              " 'amorousness': 2.0,\n",
              " 'amorphous': -0.2,\n",
              " 'amorphously': 0.1,\n",
              " 'amorphousness': 0.3,\n",
              " 'amort': -2.1,\n",
              " 'amortise': 0.5,\n",
              " 'amortised': -0.2,\n",
              " 'amortises': 0.1,\n",
              " 'amortizable': 0.5,\n",
              " 'amortization': 0.6,\n",
              " 'amortizations': 0.2,\n",
              " 'amortize': -0.1,\n",
              " 'amortized': 0.8,\n",
              " 'amortizes': 0.6,\n",
              " 'amortizing': 0.8,\n",
              " 'amusable': 0.7,\n",
              " 'amuse': 1.7,\n",
              " 'amused': 1.8,\n",
              " 'amusedly': 2.2,\n",
              " 'amusement': 1.5,\n",
              " 'amusements': 1.5,\n",
              " 'amuser': 1.1,\n",
              " 'amusers': 1.3,\n",
              " 'amuses': 1.7,\n",
              " 'amusia': 0.3,\n",
              " 'amusias': -0.4,\n",
              " 'amusing': 1.6,\n",
              " 'amusingly': 0.8,\n",
              " 'amusingness': 1.8,\n",
              " 'amusive': 1.7,\n",
              " 'anger': -2.7,\n",
              " 'angered': -2.3,\n",
              " 'angering': -2.2,\n",
              " 'angerly': -1.9,\n",
              " 'angers': -2.3,\n",
              " 'angrier': -2.3,\n",
              " 'angriest': -3.1,\n",
              " 'angrily': -1.8,\n",
              " 'angriness': -1.7,\n",
              " 'angry': -2.3,\n",
              " 'anguish': -2.9,\n",
              " 'anguished': -1.8,\n",
              " 'anguishes': -2.1,\n",
              " 'anguishing': -2.7,\n",
              " 'animosity': -1.9,\n",
              " 'annoy': -1.9,\n",
              " 'annoyance': -1.3,\n",
              " 'annoyances': -1.8,\n",
              " 'annoyed': -1.6,\n",
              " 'annoyer': -2.2,\n",
              " 'annoyers': -1.5,\n",
              " 'annoying': -1.7,\n",
              " 'annoys': -1.8,\n",
              " 'antagonism': -1.9,\n",
              " 'antagonisms': -1.2,\n",
              " 'antagonist': -1.9,\n",
              " 'antagonistic': -1.7,\n",
              " 'antagonistically': -2.2,\n",
              " 'antagonists': -1.7,\n",
              " 'antagonize': -2.0,\n",
              " 'antagonized': -1.4,\n",
              " 'antagonizes': -0.5,\n",
              " 'antagonizing': -2.7,\n",
              " 'anti': -1.3,\n",
              " 'anticipation': 0.4,\n",
              " 'anxieties': -0.6,\n",
              " 'anxiety': -0.7,\n",
              " 'anxious': -1.0,\n",
              " 'anxiously': -0.9,\n",
              " 'anxiousness': -1.0,\n",
              " 'aok': 2.0,\n",
              " 'apathetic': -1.2,\n",
              " 'apathetically': -0.4,\n",
              " 'apathies': -0.6,\n",
              " 'apathy': -1.2,\n",
              " 'apeshit': -0.9,\n",
              " 'apocalyptic': -3.4,\n",
              " 'apologise': 1.6,\n",
              " 'apologised': 0.4,\n",
              " 'apologises': 0.8,\n",
              " 'apologising': 0.2,\n",
              " 'apologize': 0.4,\n",
              " 'apologized': 1.3,\n",
              " 'apologizes': 1.5,\n",
              " 'apologizing': -0.3,\n",
              " 'apology': 0.2,\n",
              " 'appall': -2.4,\n",
              " 'appalled': -2.0,\n",
              " 'appalling': -1.5,\n",
              " 'appallingly': -2.0,\n",
              " 'appalls': -1.9,\n",
              " 'appease': 1.1,\n",
              " 'appeased': 0.9,\n",
              " 'appeases': 0.9,\n",
              " 'appeasing': 1.0,\n",
              " 'applaud': 2.0,\n",
              " 'applauded': 1.5,\n",
              " 'applauding': 2.1,\n",
              " 'applauds': 1.4,\n",
              " 'applause': 1.8,\n",
              " 'appreciate': 1.7,\n",
              " 'appreciated': 2.3,\n",
              " 'appreciates': 2.3,\n",
              " 'appreciating': 1.9,\n",
              " 'appreciation': 2.3,\n",
              " 'appreciations': 1.7,\n",
              " 'appreciative': 2.6,\n",
              " 'appreciatively': 1.8,\n",
              " 'appreciativeness': 1.6,\n",
              " 'appreciator': 2.6,\n",
              " 'appreciators': 1.5,\n",
              " 'appreciatory': 1.7,\n",
              " 'apprehensible': 1.1,\n",
              " 'apprehensibly': -0.2,\n",
              " 'apprehension': -2.1,\n",
              " 'apprehensions': -0.9,\n",
              " 'apprehensively': -0.3,\n",
              " 'apprehensiveness': -0.7,\n",
              " 'approval': 2.1,\n",
              " 'approved': 1.8,\n",
              " 'approves': 1.7,\n",
              " 'ardent': 2.1,\n",
              " 'arguable': -1.0,\n",
              " 'arguably': -1.0,\n",
              " 'argue': -1.4,\n",
              " 'argued': -1.5,\n",
              " 'arguer': -1.6,\n",
              " 'arguers': -1.4,\n",
              " 'argues': -1.6,\n",
              " 'arguing': -2.0,\n",
              " 'argument': -1.5,\n",
              " 'argumentative': -1.5,\n",
              " 'argumentatively': -1.8,\n",
              " 'argumentive': -1.5,\n",
              " 'arguments': -1.7,\n",
              " 'arrest': -1.4,\n",
              " 'arrested': -2.1,\n",
              " 'arrests': -1.9,\n",
              " 'arrogance': -2.4,\n",
              " 'arrogances': -1.9,\n",
              " 'arrogant': -2.2,\n",
              " 'arrogantly': -1.8,\n",
              " 'ashamed': -2.1,\n",
              " 'ashamedly': -1.7,\n",
              " 'ass': -2.5,\n",
              " 'assassination': -2.9,\n",
              " 'assassinations': -2.7,\n",
              " 'assault': -2.8,\n",
              " 'assaulted': -2.4,\n",
              " 'assaulting': -2.3,\n",
              " 'assaultive': -2.8,\n",
              " 'assaults': -2.5,\n",
              " 'asset': 1.5,\n",
              " 'assets': 0.7,\n",
              " 'assfucking': -2.5,\n",
              " 'assholes': -2.8,\n",
              " 'assurance': 1.4,\n",
              " 'assurances': 1.4,\n",
              " 'assure': 1.4,\n",
              " 'assured': 1.5,\n",
              " 'assuredly': 1.6,\n",
              " 'assuredness': 1.4,\n",
              " 'assurer': 0.9,\n",
              " 'assurers': 1.1,\n",
              " 'assures': 1.3,\n",
              " 'assurgent': 1.3,\n",
              " 'assuring': 1.6,\n",
              " 'assuror': 0.5,\n",
              " 'assurors': 0.7,\n",
              " 'astonished': 1.6,\n",
              " 'astound': 1.7,\n",
              " 'astounded': 1.8,\n",
              " 'astounding': 1.8,\n",
              " 'astoundingly': 2.1,\n",
              " 'astounds': 2.1,\n",
              " 'attachment': 1.2,\n",
              " 'attachments': 1.1,\n",
              " 'attack': -2.1,\n",
              " 'attacked': -2.0,\n",
              " 'attacker': -2.7,\n",
              " 'attackers': -2.7,\n",
              " 'attacking': -2.0,\n",
              " 'attacks': -1.9,\n",
              " 'attract': 1.5,\n",
              " 'attractancy': 0.9,\n",
              " 'attractant': 1.3,\n",
              " 'attractants': 1.4,\n",
              " 'attracted': 1.8,\n",
              " 'attracting': 2.1,\n",
              " 'attraction': 2.0,\n",
              " 'attractions': 1.8,\n",
              " 'attractive': 1.9,\n",
              " 'attractively': 2.2,\n",
              " 'attractiveness': 1.8,\n",
              " 'attractivenesses': 2.1,\n",
              " 'attractor': 1.2,\n",
              " 'attractors': 1.2,\n",
              " 'attracts': 1.7,\n",
              " 'audacious': 0.9,\n",
              " 'authority': 0.3,\n",
              " 'aversion': -1.9,\n",
              " 'aversions': -1.1,\n",
              " 'aversive': -1.6,\n",
              " 'aversively': -0.8,\n",
              " 'avert': -0.7,\n",
              " 'averted': -0.3,\n",
              " 'averts': -0.4,\n",
              " 'avid': 1.2,\n",
              " 'avoid': -1.2,\n",
              " 'avoidance': -1.7,\n",
              " 'avoidances': -1.1,\n",
              " 'avoided': -1.4,\n",
              " 'avoider': -1.8,\n",
              " 'avoiders': -1.4,\n",
              " 'avoiding': -1.4,\n",
              " 'avoids': -0.7,\n",
              " 'await': 0.4,\n",
              " 'awaited': -0.1,\n",
              " 'awaits': 0.3,\n",
              " 'award': 2.5,\n",
              " 'awardable': 2.4,\n",
              " 'awarded': 1.7,\n",
              " 'awardee': 1.8,\n",
              " 'awardees': 1.2,\n",
              " 'awarder': 0.9,\n",
              " 'awarders': 1.3,\n",
              " 'awarding': 1.9,\n",
              " 'awards': 2.0,\n",
              " 'awesome': 3.1,\n",
              " 'awful': -2.0,\n",
              " 'awkward': -0.6,\n",
              " 'awkwardly': -1.3,\n",
              " 'awkwardness': -0.7,\n",
              " 'axe': -0.4,\n",
              " 'axed': -1.3,\n",
              " 'backed': 0.1,\n",
              " 'backing': 0.1,\n",
              " 'backs': -0.2,\n",
              " 'bad': -2.5,\n",
              " 'badass': 1.4,\n",
              " 'badly': -2.1,\n",
              " 'bailout': -0.4,\n",
              " 'bamboozle': -1.5,\n",
              " 'bamboozled': -1.5,\n",
              " 'bamboozles': -1.5,\n",
              " 'ban': -2.6,\n",
              " 'banish': -1.9,\n",
              " 'bankrupt': -2.6,\n",
              " 'bankster': -2.1,\n",
              " 'banned': -2.0,\n",
              " 'bargain': 0.8,\n",
              " 'barrier': -0.5,\n",
              " 'bashful': -0.1,\n",
              " 'bashfully': 0.2,\n",
              " 'bashfulness': -0.8,\n",
              " 'bastard': -2.5,\n",
              " 'bastardies': -1.8,\n",
              " 'bastardise': -2.1,\n",
              " 'bastardised': -2.3,\n",
              " 'bastardises': -2.3,\n",
              " 'bastardising': -2.6,\n",
              " 'bastardization': -2.4,\n",
              " 'bastardizations': -2.1,\n",
              " 'bastardize': -2.4,\n",
              " 'bastardized': -2.0,\n",
              " 'bastardizes': -1.8,\n",
              " 'bastardizing': -2.3,\n",
              " 'bastardly': -2.7,\n",
              " 'bastards': -3.0,\n",
              " 'bastardy': -2.7,\n",
              " 'battle': -1.6,\n",
              " 'battled': -1.2,\n",
              " 'battlefield': -1.6,\n",
              " 'battlefields': -0.9,\n",
              " 'battlefront': -1.2,\n",
              " 'battlefronts': -0.8,\n",
              " 'battleground': -1.7,\n",
              " 'battlegrounds': -0.6,\n",
              " 'battlement': -0.4,\n",
              " 'battlements': -0.4,\n",
              " 'battler': -0.8,\n",
              " 'battlers': -0.2,\n",
              " 'battles': -1.6,\n",
              " 'battleship': -0.1,\n",
              " 'battleships': -0.5,\n",
              " 'battlewagon': -0.3,\n",
              " 'battlewagons': -0.5,\n",
              " 'battling': -1.1,\n",
              " 'beaten': -1.8,\n",
              " 'beatific': 1.8,\n",
              " 'beating': -2.0,\n",
              " 'beaut': 1.6,\n",
              " 'beauteous': 2.5,\n",
              " 'beauteously': 2.6,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBScjj7aqDr3",
        "colab_type": "text"
      },
      "source": [
        "Out of 7500 tokens defined in VADER, only 3 contain spaces, and only 2 of those are actually n-grams; the other is an emoticon for “kiss.”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy4T9OolpNLX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "eb9d6905-c42a-4c27-b473-adf135f36964"
      },
      "source": [
        "[(token, score) for token, score in sa.lexicon.items() if \" \" in token]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"( '}{' )\", 1.6),\n",
              " (\"can't stand\", -2.0),\n",
              " ('fed up', -1.8),\n",
              " ('screwed up', -1.5)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IMHxpztqJ4v",
        "colab_type": "text"
      },
      "source": [
        "The VADER algorithm considers the intensity of sentiment polarity in three separate scores (positive, negative, and neutral) and then combines them\n",
        "together into a compound positivity sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT0fCsRCpzGI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b93381b4-bf69-4c76-af46-66051a3cd53e"
      },
      "source": [
        "sa.polarity_scores(text=\"Python is very readable and it's great for NLP.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'compound': 0.6249, 'neg': 0.0, 'neu': 0.661, 'pos': 0.339}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EzenQz5qYZZ",
        "colab_type": "text"
      },
      "source": [
        "Notice that VADER handles negation pretty well—“great” has a slightly more positive sentiment than “not bad.” \n",
        "\n",
        "VADER’s built-in tokenizer ignores any words that aren’t in its lexicon, and it doesn’t consider n-grams at all."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGdcEhFQqUJp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cf086844-31a5-4489-c896-a7f77b6b18a4"
      },
      "source": [
        "sa.polarity_scores(text=\"Python is not a bad choice for most applications.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'compound': 0.431, 'neg': 0.0, 'neu': 0.737, 'pos': 0.263}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d4JMFWVquzZ",
        "colab_type": "text"
      },
      "source": [
        "Let’s see how well this rule-based approach does for the example statements we mentioned earlier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLLBfGupqlPf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d3defdbd-2ae1-4e80-b874-429023497b50"
      },
      "source": [
        "corpus = [\n",
        "   \"Absolutely perfect! Love it! :-) :-) :-)\",\n",
        "   \"Horrible! Completely useless. :(\",\n",
        "   \"It was OK. Some good and some bad things.\"     \n",
        "]\n",
        "\n",
        "for doc in corpus:\n",
        "  scores = sa.polarity_scores(doc)\n",
        "  print(\"{:+}: {}\".format(scores['compound'], doc))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+0.9428: Absolutely perfect! Love it! :-) :-) :-)\n",
            "-0.8768: Horrible! Completely useless. :(\n",
            "-0.1531: It was OK. Some good and some bad things.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grw6Z1Vkrrbi",
        "colab_type": "text"
      },
      "source": [
        "This looks a lot like what you wanted. So the only drawback is that VADER doesn’t look at all the words in a document, only about 7,500. \n",
        "\n",
        "What if you want all the words to help add to the sentiment score? \n",
        "\n",
        "And what if you don’t want to have to code your own understanding of the words in a dictionary of thousands of words or add a bunch of custom words to the dictionary in **SentimentIntensityAnalyzer.lexicon**?\n",
        "\n",
        "**The rule-based approach might be impossible if you don’t understand the language, because you wouldn’t know what scores to put in the dictionary(lexicon)!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNURElwCr_aj",
        "colab_type": "text"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRI7STiRsKFB",
        "colab_type": "text"
      },
      "source": [
        "A Naive Bayes model tries to find keywords in a set of documents that are predictive of your target (output) variable. When your target variable is the sentiment you are trying to predict, the model will find words that predict that sentiment. \n",
        "\n",
        "**The nice thing about a Naive Bayes model is that the internal coefficients will map words or tokens to scores just like VADER does. Only this time you won’t have to be limited to just what an individual human decided those scores should be. The machine will find the “best” scores for any problem.**\n",
        "\n",
        "For any machine learning algorithm, you first need to find a dataset. You need a bunch of text documents that have labels for their positive emotional content (positivity sentiment).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXlvRGBx05dF",
        "colab_type": "text"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEp2HELv9ITE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.width', 75)\n",
        "\n",
        "from nltk.tokenize import casual_tokenize\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from collections import Counter"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNjGrD4b04v2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/totalgood/nlpia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiVtuRkp6soo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir test\n",
        "!cp -r nlpia/src/ test/\n",
        "!rm -rf nlpia\n",
        "!cp -r test/src/nlpia/ ."
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ9KG5Ow8H-0",
        "colab_type": "text"
      },
      "source": [
        "### Loading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3xjsVav1Dan",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "9dbe49bb-8fea-4ec7-cf48-25ee29409cbf"
      },
      "source": [
        "# load the data from nlpia package\n",
        "from nlpia.loaders import get_data"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pugnlp/constants.py:158: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime instead.\n",
            "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlxcCEcF1lIn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "b6149ed8-f4a2-4cb3-eae0-a8e49db04bc7"
      },
      "source": [
        "movies = get_data(\"hutto_movies\")\n",
        "movies.head().round(2)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/nlpia/futil.py:370: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
            "  if (series == np.arange(len(series))).all():\n",
            "/content/nlpia/futil.py:373: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
            "  (series.index == np.arange(len(series))).all() and\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.27</td>\n",
              "      <td>The Rock is destined to be the 21st Century's ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.53</td>\n",
              "      <td>The gorgeously elaborate continuation of ''The...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.60</td>\n",
              "      <td>Effective but too tepid biopic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.47</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.73</td>\n",
              "      <td>Emerges as something rare, an issue movie that...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sentiment                                               text\n",
              "id                                                              \n",
              "1        2.27  The Rock is destined to be the 21st Century's ...\n",
              "2        3.53  The gorgeously elaborate continuation of ''The...\n",
              "3       -0.60                     Effective but too tepid biopic\n",
              "4        1.47  If you sometimes like to go to the movies to h...\n",
              "5        1.73  Emerges as something rare, an issue movie that..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae-BiXvj79qV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "2e5b68ca-36a6-4e37-b28b-cc1fdb69d097"
      },
      "source": [
        "# It looks like movies were rated on a scale from -4 to +4.\n",
        "movies.describe().round(2)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>10605.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-3.88</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-1.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-0.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3.94</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentiment\n",
              "count   10605.00\n",
              "mean        0.00\n",
              "std         1.92\n",
              "min        -3.88\n",
              "25%        -1.77\n",
              "50%        -0.08\n",
              "75%         1.83\n",
              "max         3.94"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyTbuI1t850R",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bo8I9kF89sE",
        "colab_type": "text"
      },
      "source": [
        "Now let’s tokenize all those movie review texts to create a bag of words for each one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgaFh9JP8iBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bags_of_words = []\n",
        "for text in movies.text:\n",
        "  bags_of_words.append(Counter(casual_tokenize(text)))"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEx1UpEd-ac6",
        "colab_type": "text"
      },
      "source": [
        "The from_records() DataFrame constructor takes a sequence of dictionaries. It creates columns for all the keys, and the values are added to the table in the appropriate columns, filling missing values with NaN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57acAXRC-YSO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b928ff50-3900-40db-a9c1-3e0d2eb2f151"
      },
      "source": [
        "df_bows = pd.DataFrame.from_records(bags_of_words)\n",
        "# fill all the NaNs with zeros\n",
        "df_bows = df_bows.fillna(0).astype(int)\n",
        "df_bows.shape"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10605, 20756)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVUONZ9q_CL2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "outputId": "28535b15-f616-400b-f0ac-330bf935a0d3"
      },
      "source": [
        "df_bows.head()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>The</th>\n",
              "      <th>Rock</th>\n",
              "      <th>is</th>\n",
              "      <th>destined</th>\n",
              "      <th>to</th>\n",
              "      <th>be</th>\n",
              "      <th>the</th>\n",
              "      <th>21st</th>\n",
              "      <th>Century's</th>\n",
              "      <th>new</th>\n",
              "      <th>'</th>\n",
              "      <th>Conan</th>\n",
              "      <th>and</th>\n",
              "      <th>that</th>\n",
              "      <th>he's</th>\n",
              "      <th>going</th>\n",
              "      <th>make</th>\n",
              "      <th>a</th>\n",
              "      <th>splash</th>\n",
              "      <th>even</th>\n",
              "      <th>greater</th>\n",
              "      <th>than</th>\n",
              "      <th>Arnold</th>\n",
              "      <th>Schwarzenegger</th>\n",
              "      <th>,</th>\n",
              "      <th>Jean</th>\n",
              "      <th>Claud</th>\n",
              "      <th>Van</th>\n",
              "      <th>Damme</th>\n",
              "      <th>or</th>\n",
              "      <th>Steven</th>\n",
              "      <th>Segal</th>\n",
              "      <th>.</th>\n",
              "      <th>gorgeously</th>\n",
              "      <th>elaborate</th>\n",
              "      <th>continuation</th>\n",
              "      <th>of</th>\n",
              "      <th>Lord</th>\n",
              "      <th>Rings</th>\n",
              "      <th>trilogy</th>\n",
              "      <th>...</th>\n",
              "      <th>Overwrought</th>\n",
              "      <th>snooze</th>\n",
              "      <th>Feeble</th>\n",
              "      <th>salaciously</th>\n",
              "      <th>Disjointed</th>\n",
              "      <th>humbuggery</th>\n",
              "      <th>Eh</th>\n",
              "      <th>unrealistic</th>\n",
              "      <th>nrelentingly</th>\n",
              "      <th>Painfully</th>\n",
              "      <th>Grating</th>\n",
              "      <th>Dramatically</th>\n",
              "      <th>Predictably</th>\n",
              "      <th>Arty</th>\n",
              "      <th>Incoherence</th>\n",
              "      <th>reigns</th>\n",
              "      <th>assed</th>\n",
              "      <th>Abysmally</th>\n",
              "      <th>Bland</th>\n",
              "      <th>ame</th>\n",
              "      <th>drudgery</th>\n",
              "      <th>snubbing</th>\n",
              "      <th>Mildly</th>\n",
              "      <th>Terrible</th>\n",
              "      <th>Degenerates</th>\n",
              "      <th>hogwash</th>\n",
              "      <th>Crummy</th>\n",
              "      <th>Wishy</th>\n",
              "      <th>Inconsequential</th>\n",
              "      <th>Insufferably</th>\n",
              "      <th>Ill</th>\n",
              "      <th>slummer</th>\n",
              "      <th>Rashomon</th>\n",
              "      <th>dipsticks</th>\n",
              "      <th>Bearable</th>\n",
              "      <th>Staggeringly</th>\n",
              "      <th>’</th>\n",
              "      <th>ve</th>\n",
              "      <th>muttering</th>\n",
              "      <th>dissing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 20756 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   The  Rock  is  destined  to  ...  Staggeringly  ’  ve  muttering  dissing\n",
              "0    1     1   1         1   2  ...             0  0   0          0        0\n",
              "1    2     0   1         0   0  ...             0  0   0          0        0\n",
              "2    0     0   0         0   0  ...             0  0   0          0        0\n",
              "3    0     0   1         0   4  ...             0  0   0          0        0\n",
              "4    0     0   0         0   0  ...             0  0   0          0        0\n",
              "\n",
              "[5 rows x 20756 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9doNCik_Som",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "b51d2aea-7f61-4800-f8b8-4c58df8ebb95"
      },
      "source": [
        "df_bows.head()[list(bags_of_words[0].keys())]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>The</th>\n",
              "      <th>Rock</th>\n",
              "      <th>is</th>\n",
              "      <th>destined</th>\n",
              "      <th>to</th>\n",
              "      <th>be</th>\n",
              "      <th>the</th>\n",
              "      <th>21st</th>\n",
              "      <th>Century's</th>\n",
              "      <th>new</th>\n",
              "      <th>'</th>\n",
              "      <th>Conan</th>\n",
              "      <th>and</th>\n",
              "      <th>that</th>\n",
              "      <th>he's</th>\n",
              "      <th>going</th>\n",
              "      <th>make</th>\n",
              "      <th>a</th>\n",
              "      <th>splash</th>\n",
              "      <th>even</th>\n",
              "      <th>greater</th>\n",
              "      <th>than</th>\n",
              "      <th>Arnold</th>\n",
              "      <th>Schwarzenegger</th>\n",
              "      <th>,</th>\n",
              "      <th>Jean</th>\n",
              "      <th>Claud</th>\n",
              "      <th>Van</th>\n",
              "      <th>Damme</th>\n",
              "      <th>or</th>\n",
              "      <th>Steven</th>\n",
              "      <th>Segal</th>\n",
              "      <th>.</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   The  Rock  is  destined  to  be  ...  Van  Damme  or  Steven  Segal  .\n",
              "0    1     1   1         1   2   1  ...    1      1   1       1      1  1\n",
              "1    2     0   1         0   0   0  ...    0      0   0       0      0  4\n",
              "2    0     0   0         0   0   0  ...    0      0   0       0      0  0\n",
              "3    0     0   1         0   4   0  ...    0      0   0       0      0  1\n",
              "4    0     0   0         0   0   0  ...    0      0   0       0      0  1\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0se-RY0PAB5s",
        "colab_type": "text"
      },
      "source": [
        "Now you have all the data that a Naive Bayes model needs to find the keywords that predict sentiment from natural language text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q89x5vQy_5-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb = MultinomialNB()\n",
        "\n",
        "# convert your output variable (sentiment float) to a discrete label (integer, string, or bool).\n",
        "nb = nb.fit(df_bows, movies.sentiment > 0)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB_jnITBAe2S",
        "colab_type": "text"
      },
      "source": [
        "Convert your binary classification variable (0 or 1) to -4 or 4 so you\n",
        "can compare it to the “ground truth” sentiment.\n",
        "\n",
        "Use nb.predict_proba to get a continuous value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itsAG5KiAZVf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5a0fe6e2-1869-465d-9a04-bb54c3526c79"
      },
      "source": [
        "movies['predicted_sentiment'] = nb.predict(df_bows) * 8 - 4\n",
        "\n",
        "# The average absolute value of the prediction error (mean absolute error or MAE) is 2.4.\n",
        "movies['error'] = (movies.predicted_sentiment - movies.sentiment).abs()\n",
        "movies.error.mean()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.3911742904638262"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKEQ06E5Bdk2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "d0413f39-3818-4e7a-b1fe-cc337bb4b247"
      },
      "source": [
        "movies['predicted_ispos'] = (movies.predicted_sentiment > 0).astype(int)\n",
        "movies['sentiment_ispositive'] = (movies.predicted_sentiment > 0).astype(int)\n",
        "movies['''sentiment predicted_sentiment sentiment_ispositive predicted_ispos'''.split()].head(8)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>predicted_sentiment</th>\n",
              "      <th>sentiment_ispositive</th>\n",
              "      <th>predicted_ispos</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.266667</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.533333</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.600000</td>\n",
              "      <td>-4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.466667</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.733333</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2.533333</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2.466667</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.266667</td>\n",
              "      <td>-4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sentiment  predicted_sentiment  sentiment_ispositive  predicted_ispos\n",
              "id                                                                       \n",
              "1    2.266667                    4                     1                1\n",
              "2    3.533333                    4                     1                1\n",
              "3   -0.600000                   -4                     0                0\n",
              "4    1.466667                    4                     1                1\n",
              "5    1.733333                    4                     1                1\n",
              "6    2.533333                    4                     1                1\n",
              "7    2.466667                    4                     1                1\n",
              "8    1.266667                   -4                     0                0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHsfxreFY_jj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "72f64f3b-7be1-4c90-c08b-2a5a9d3b32e5"
      },
      "source": [
        "# You got the “thumbs up” rating correct 93% of the time.\n",
        "(movies.sentiment_ispositive == movies.sentiment_ispositive).sum() / len(movies)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQLtNAESakHP",
        "colab_type": "text"
      },
      "source": [
        "This is a pretty good start at building a sentiment analyzer with only a few lines of code (and a lot of data). **You didn’t have to compile a list of 7500 words and their sentiment like VADER did. You just gave it a bunch of text and labels for that text. That’s the power of machine learning and NLP!**\n",
        "\n",
        "If you want to build a real sentiment analyzer like this, remember to split your training data.\n",
        "\n",
        "You forced your classifier to rate all the text as thumbs up or thumbs down, so a random guess would have had a MAP error of about 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9VfwnxOaAew",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "0c8fbd4f-bda1-45a4-a1a8-6180bd075956"
      },
      "source": [
        "products = get_data(\"hutto_products\")\n",
        "\n",
        "bags_of_words = []\n",
        "for text in products.text:\n",
        "  bags_of_words.append(Counter(casual_tokenize(text)))\n",
        "\n",
        "df_product_bows = pd.DataFrame.from_records(bags_of_words)\n",
        "df_product_bows = df_product_bows.fillna(0).astype(int)\n",
        "df_all_bows = df_bows.append(df_product_bows)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/nlpia/futil.py:370: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
            "  if (series == np.arange(len(series))).all():\n",
            "/content/nlpia/futil.py:373: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead\n",
            "  (series.index == np.arange(len(series))).all() and\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MHVDNdQcAyW",
        "colab_type": "text"
      },
      "source": [
        "Your new bags of words have some tokens that weren’t in the original bags of words DataFrame (23302 columns now instead of 20756 before)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRpNcVtDcATm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "cfdf6105-788d-4e55-e94e-b0e118657016"
      },
      "source": [
        "df_all_bows.columns"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st',\n",
              "       'Century's', 'new',\n",
              "       ...\n",
              "       'sligtly', 'owner', '81', 'defectively', 'warrranty', 'expire',\n",
              "       'expired', 'voids', 'baghdad', 'harddisk'],\n",
              "      dtype='object', length=23302)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64jg_w8rcQML",
        "colab_type": "text"
      },
      "source": [
        "You need to make sure your new product DataFrame of bags of words has the exact same columns (tokens) in the exact same order as the original one used to train your Naive Bayes model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAUkxTOicGxZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6020e1e1-6b2f-41b5-df8a-64466046217f"
      },
      "source": [
        "df_product_bows = df_all_bows.iloc[len(movies):][df_bows.columns]\n",
        "df_product_bows.shape"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3546, 20756)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6OgXFGWcnRz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "27eaf57f-91ad-4e38-d39d-3d60581ac82b"
      },
      "source": [
        "# This is the original movie bags of words\n",
        "df_bows.shape"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10605, 20756)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JMR_seXdi0z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "8c9ed18b-c74e-4411-f571-113d7eb4f109"
      },
      "source": [
        "df_product_bows = df_product_bows.fillna(0).astype(int)\n",
        "products['ispos'] = (products.sentiment > 0).astype(int)\n",
        "products['pred'] = nb.predict(df_product_bows.values).astype(int)\n",
        "products.head()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "      <th>ispos</th>\n",
              "      <th>pred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1_1</td>\n",
              "      <td>-0.90</td>\n",
              "      <td>troubleshooting ad-2500 and ad-2600 no picture...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1_2</td>\n",
              "      <td>-0.15</td>\n",
              "      <td>repost from january 13, 2004 with a better fit...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1_3</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>does your apex dvd player only play dvd audio ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1_4</td>\n",
              "      <td>-0.10</td>\n",
              "      <td>or does it play audio and video but scrolling ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1_5</td>\n",
              "      <td>-0.50</td>\n",
              "      <td>before you try to return the player or waste h...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id  sentiment  ... ispos  pred\n",
              "0  1_1      -0.90  ...     0     0\n",
              "1  1_2      -0.15  ...     0     0\n",
              "2  1_3      -0.20  ...     0     0\n",
              "3  1_4      -0.10  ...     0     0\n",
              "4  1_5      -0.50  ...     0     0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af5CP-wIeeRy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "75dcb3fa-2d6c-457d-f472-fb3b8d331c3d"
      },
      "source": [
        "(products.pred == products.ispos).sum() / len(products)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5572476029328821"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xrjl9dygNn-",
        "colab_type": "text"
      },
      "source": [
        "So your Naive Bayes model does a poor job of predicting whether a product review is positive (thumbs up). One reason for this subpar performance is that your vocabulary from the casual_tokenize product texts has 2546 tokens that weren’t in the movie reviews. That’s about 10% of the tokens in your original movie review tokenization, which means that all those words won’t have any weights or scores in your Naive Bayes model."
      ]
    }
  ]
}