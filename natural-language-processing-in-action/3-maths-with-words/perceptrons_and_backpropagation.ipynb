{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "perceptrons-and-backpropagation.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-in-action/blob/5-baby-steps-with-neuralnetworks/perceptrons_and_backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09loYapD2HCK",
        "colab_type": "text"
      },
      "source": [
        "# Baby steps with neural networks (perceptrons and backpropagation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7KV3MsJ2KrT",
        "colab_type": "text"
      },
      "source": [
        "## Neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aM31TJ02fdu",
        "colab_type": "text"
      },
      "source": [
        "As the availability of processing power and memory has exploded over the course of the decade, an old technology has come into its own again. First proposed in the 1950s by Frank Rosenblatt, the perceptron1 offered a novel algorithm for finding patterns in data.\n",
        "\n",
        "The basic concept lies in a rough mimicry of the operation of a living neuron cell.\n",
        "As electrical signals flow into the cell through the dendrites (see figure) into the\n",
        "nucleus, an electric charge begins to build up. When the cell reaches a certain level of\n",
        "charge, it fires, sending an electrical\n",
        "signal out through the axon.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/perceptron-1.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmPeDAy-44oe",
        "colab_type": "text"
      },
      "source": [
        "The key concept to notice\n",
        "here is the way the cell weights\n",
        "incoming signals when deciding when to fire. The neuron will dynamically change\n",
        "those weights in the decision making process over the course of its life."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxMtK0ft5D_r",
        "colab_type": "text"
      },
      "source": [
        "## Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edjllY4x5FDf",
        "colab_type": "text"
      },
      "source": [
        "Rosenblatt’s original project was to teach a machine to recognize images. The original\n",
        "perceptron was a conglomeration of photo-receptors and potentiometers, not a computer\n",
        "in the current sense. But implementation specifics aside, Rosenblatt’s concept\n",
        "was to take the features of an image and assign a weight, a measure of importance, to\n",
        "each one. The features of the input image were each a small subsection of the image.\n",
        "\n",
        "A grid of photo-receptors would be exposed to the image. Each receptor would see\n",
        "one small piece of the image. The brightness of the image that a particular photoreceptor\n",
        "could see would determine the strength of the signal that it would send to\n",
        "the associated “dendrite.”\n",
        "\n",
        "Each dendrite had an associated weight in the form of a potentiometer. Once\n",
        "enough signal came in, it would pass the signal into the main body of the “nucleus” of\n",
        "the “cell.” Once enough of those signals from all the potentiometers passed a certain\n",
        "threshold, the perceptron would fire down its axon, indicating a positive match on the\n",
        "image it was presented with. If it didn’t fire for a given image, that was a negative classification\n",
        "match. Think “hot dog, not hot dog” or “iris setosa, not iris setosa.”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLvQWAUo5alR",
        "colab_type": "text"
      },
      "source": [
        "## A numerical perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxKYU1-qATqD",
        "colab_type": "text"
      },
      "source": [
        "### Motivation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXZ4HKNQ5fYQ",
        "colab_type": "text"
      },
      "source": [
        "Basically, you’d like to take an example from a dataset, show it to an algorithm, and\n",
        "have the algorithm say yes or no. That’s all you’re doing so far. The first piece you\n",
        "need is a way to determine the features of the sample. Choosing appropriate features\n",
        "turns out to be a surprisingly challenging part of machine learning.\n",
        "\n",
        " In “normal”\n",
        "machine learning problems, like predicting home prices, your features might be\n",
        "square footage, last sold price, and ZIP code. \n",
        "\n",
        "Or perhaps you’d like to predict the species\n",
        "of a certain flower using the Iris dataset.2 In that case your features would be petal\n",
        "length, petal width, sepal length, and sepal width.\n",
        "\n",
        "In Rosenblatt’s experiment, the features were the intensity values of each pixel\n",
        "(subsections of the image), one pixel per photo receptor. You then need a set of\n",
        "weights to assign to each of the features. \n",
        "\n",
        "Don’t worry yet about where these weights\n",
        "come from. Just think of them as a percentage of the signal to let through into the\n",
        "neuron. If you’re familiar with linear regression, then you probably already know\n",
        "where these weights come from.\n",
        "\n",
        "Generally, you’ll see the individual features denoted as xi, where i is a\n",
        "reference integer. And the collection of all features for a given example are\n",
        "denoted as X representing a vector:\n",
        "\n",
        "$$X = [x1, x2, …, xi, …, xn]$$\n",
        "\n",
        "And similarly, you’ll see the associate weights for each feature as wi, where i\n",
        "corresponds to the index of feature x associated with that weight. And the\n",
        "weights are generally represented as a vector W:\n",
        "\n",
        "$$W = [w1, w2, …, wi, …, wn]$$\n",
        "\n",
        "With the features in hand, you just multiply each feature (xi) by the corresponding\n",
        "weight (wi) and then sum up:\n",
        "\n",
        "$$ (x1 * w1) + (x2 * w2) + … + (xi * wi) + … $$\n",
        "\n",
        "The one piece you’re missing here is the neuron’s threshold to fire or not. And it’s\n",
        "just that, a threshold. Once the weighted sum is above a certain threshold, the perceptron\n",
        "outputs 1. Otherwise it outputs 0.\n",
        "\n",
        "You can represent this threshold with a simple step function (labeled “Activation\n",
        "Function” in figure).\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/perceptron-2.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zshc1C7O6m2W",
        "colab_type": "text"
      },
      "source": [
        "### Detour through bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBNU4N-17V5q",
        "colab_type": "text"
      },
      "source": [
        "The bias is an “always on”\n",
        "input to the neuron. The neuron has a weight dedicated to it just as with every other\n",
        "element of the input, and that weight is trained along with the others in the exact\n",
        "same way. This is represented in two ways in the various literature around neural networks.\n",
        "You may see the input represented as the base input vector, say of n-elements,with a 1 appended to the beginning or the end of the vector, giving you an n+1 dimensional\n",
        "vector. The position of the 1 is irrelevant to the network, as long as it’s consistent\n",
        "across all of your samples.\n",
        "\n",
        "The reason for having the bias weight at all is that you need the neuron to be resilient\n",
        "to inputs of all zeros. It may be the case that the network needs to learn to output\n",
        "0 in the face of inputs of 0, but it may not. Without the bias term, the neuron would\n",
        "output 0 * weight = 0 for any weights you started with or tried to learn. With the bias\n",
        "term, you won’t have this problem.\n",
        "\n",
        "Figure is a rather neat visualization of the analogy between some of the signals\n",
        "within a biological neuron in your brain and the signals of an artificial neuron used for\n",
        "deep learning.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/perceptron-3.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPFq-AeM8WE7",
        "colab_type": "text"
      },
      "source": [
        "And in mathematical terms, the output of your perceptron, denoted f(x), looks like:\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/perceptron-4.JPG?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk1e9QrK9Afa",
        "colab_type": "text"
      },
      "source": [
        "Your perceptron hasn’t learned anything just yet. But you have achieved something\n",
        "quite important. You’ve passed data into a model and received an output. That output\n",
        "is likely wrong, given you said nothing about where the weight values come from. But\n",
        "this is where things will get interesting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg-Dm4yd9BNo",
        "colab_type": "text"
      },
      "source": [
        "### A PYTHONIC NEURON"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSjbHn779Nug",
        "colab_type": "text"
      },
      "source": [
        "Calculating the output of the neuron described earlier is straightforward in Python.\n",
        "You can also use the numpy dot function to multiply your two vectors together:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCXsgEXp832Q",
        "colab_type": "code",
        "outputId": "f04209b5-429d-4bcb-9f29-0d5364affa3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# row data\n",
        "example_input = [1, .2, .1, .05, .2]\n",
        "example_weights = [.2, .12, .4, .6, .90]\n",
        "\n",
        "# convert into numpy array\n",
        "input_vector = np.array(example_input)\n",
        "weights = np.array(example_weights)\n",
        "\n",
        "# bias term\n",
        "bias_weight = .2\n",
        "\n",
        "# The multiplication by one (* 1) is just to emphasize that the bias_weight is like all the other weights:\n",
        "# it’s multiplied by an input value, only the bias_weight input feature value is always 1.\n",
        "activation_level = np.dot(input_vector, weights) + (bias_weight * 1)\n",
        "activation_level"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6740000000000002"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS2WINY6-MEc",
        "colab_type": "text"
      },
      "source": [
        "With that, if you use a simple threshold activation function and choose a threshold of\n",
        ".5, your next step is the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX9d-nzx9SwY",
        "colab_type": "code",
        "outputId": "dc04c1d4-a8f2-4d44-f25f-3129f16878d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "threshold = .5\n",
        " \n",
        "if activation_level >= threshold:\n",
        "   perceptron_output = 1\n",
        "else:\n",
        "   perceptron_output = 0\n",
        "\n",
        "# see the result\n",
        "perceptron_output"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvhXY168_qWy",
        "colab_type": "text"
      },
      "source": [
        "Given the example_input, and that particular set of weights, this perceptron will output 1 But if you have several example_input vectors and the associated expected outcomes with each (a labeled dataset), you can decide if the perceptron is correct or\n",
        "not for each guess."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot93ZLbr_tl8",
        "colab_type": "text"
      },
      "source": [
        "### CLASS IS IN SESSION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNS-Plen1mDR",
        "colab_type": "text"
      },
      "source": [
        "The perceptron learns by altering the weights up or down as a function of how\n",
        "wrong the system’s guess was for a given input. But from where does it start? The\n",
        "weights of an untrained neuron start out random! Random values, near zero, are usually\n",
        "chosen from a normal distribution.\n",
        "\n",
        "And from there you can start to learn. Many different samples are shown to the system,\n",
        "and each time the weights are readjusted a small amount based on whether the\n",
        "neuron output was what you wanted or not. With enough examples (and under the\n",
        "right conditions), the error should tend toward zero, and the system learns.\n",
        "\n",
        "The trick is, and this is the key to the whole concept, that each weight is adjusted by\n",
        "how much it contributed to the resulting error. A larger weight (which lets that data\n",
        "point affect the result more) should be blamed more for the rightness/wrongness of\n",
        "the perceptron’s output for that given input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_eF4Q44-6dO",
        "colab_type": "code",
        "outputId": "bd67fbd1-46f5-43f1-ec9b-8f644e488ff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "expected_output = 0\n",
        "new_weights = []\n",
        "\n",
        "#for i, x in enumerate(example_input):\n",
        "#  # For example, in the first index above: new_weight = .2 + (0 - 1) * 1 = -0.8\n",
        "#  new_weights.append(weights[i] + (expected_output - perceptron_output) * x)\n",
        "\n",
        "new_weights = [weights[i] + (expected_output - perceptron_output) * x for i, x in enumerate(example_input)]\n",
        "weights = np.array(new_weights)\n",
        "\n",
        "# Original weights\n",
        "example_weights"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2, 0.12, 0.4, 0.6, 0.9]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cs7Ax1Hr3PmX",
        "colab_type": "code",
        "outputId": "0ae88f97-5103-4496-b28e-afbdf5d767ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# New weights\n",
        "weights"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.8 , -0.08,  0.3 ,  0.55,  0.7 ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wOnI4yW4cDS",
        "colab_type": "text"
      },
      "source": [
        "This process of exposing the network over and over to the same training set can,\n",
        "under the right circumstances, lead to an accurate predictor even on input that the\n",
        "perceptron has never seen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDw01wiB4csB",
        "colab_type": "text"
      },
      "source": [
        "### LOGIC IS A FUN THING TO LEARN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EM8MVTX4xgZ",
        "colab_type": "text"
      },
      "source": [
        "Let’s try to get the computer to understand the concept of logical OR. If either\n",
        "one side or the other of the expression is true (or both sides are), the logical OR statement\n",
        "is true. Simple enough. For this toy problem, you can easily model every possible\n",
        "example by hand (this is rarely the case in reality). Each sample consists of two signals,\n",
        "each of which is either true (1) or false (0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjfD-QPy3YBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_data = [ \n",
        "    [0, 0],    # False, False\n",
        "    [0, 1],    # False, True\n",
        "    [1, 0],    # True, False\n",
        "    [1, 1]     # True, True\n",
        "]\n",
        "\n",
        "expected_results = [\n",
        "    0,    # (False OR False) gives False\n",
        "    1,    # (False OR True ) gives True\n",
        "    1,    # (True OR False) gives True\n",
        "    1,    # (True OR True ) gives True                \n",
        "]\n",
        "\n",
        "activation_threshold = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exdadN-h6WLC",
        "colab_type": "text"
      },
      "source": [
        "You need a few tools to get started: numpy just to get used to doing vector (array) multiplication,\n",
        "and random to initialize the weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV3coIl5544f",
        "colab_type": "code",
        "outputId": "19f286c6-97c7-4794-b653-cb450a75f962",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "weights = np.random.random(2) / 1000   # Small random float 0 < w < .001\n",
        "weights"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00032362, 0.00043451])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5FZJVvZ6ZXz",
        "colab_type": "text"
      },
      "source": [
        "You need a bias as well"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEmXLRgi6PEP",
        "colab_type": "code",
        "outputId": "47e11301-d84f-4c56-d6c0-c08931280933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "bias_weight = np.random.random() / 1000\n",
        "bias_weight"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0007900798942765753"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKpB81SD6uWV",
        "colab_type": "text"
      },
      "source": [
        "Then you can pass it through your pipeline and get a prediction for each of your four\n",
        "samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT7apelV6gao",
        "colab_type": "code",
        "outputId": "5a55757b-5936-4842-a588-56f78f8e7e21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "source": [
        "for idx, sample in enumerate(sample_data):\n",
        "  input_vector = np.array(sample)\n",
        "  activation_level = np.dot(input_vector, weights) + (bias_weight * 1)\n",
        "  if activation_level > activation_threshold:\n",
        "    perceptron_output = 1\n",
        "  else:\n",
        "    perceptron_output = 0\n",
        "  print('Predicted {}'.format(perceptron_output))\n",
        "  print('Expected: {}'.format(expected_results[idx]))\n",
        "  print()    "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted 0\n",
            "Expected: 0\n",
            "\n",
            "Predicted 0\n",
            "Expected: 1\n",
            "\n",
            "Predicted 0\n",
            "Expected: 1\n",
            "\n",
            "Predicted 0\n",
            "Expected: 1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNFZEM3E7mTt",
        "colab_type": "text"
      },
      "source": [
        "Your random weight values didn’t help your little neuron out that much—one right\n",
        "and three wrong. Let’s send it back to school. Instead of just printing 1 or 0, you’ll\n",
        "update the weights at each iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FKBF8Fi7cik",
        "colab_type": "code",
        "outputId": "59e1ed66-6091-47f1-8539-9597642b2873",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "for iteration_num in range(5):\n",
        "  correct_answers = 0\n",
        "\n",
        "  for idx, sample in enumerate(sample_data):\n",
        "    input_vector = np.array(sample)\n",
        "    activation_level = np.dot(input_vector, weights) + (bias_weight * 1)\n",
        "\n",
        "    if activation_level > activation_threshold:\n",
        "      perceptron_output = 1\n",
        "    else:\n",
        "      perceptron_output = 0\n",
        "\n",
        "    if perceptron_output == expected_results[idx]:\n",
        "      correct_answers += 1\n",
        "\n",
        "    # This is where the magic happens.\n",
        "    new_weights = [weights[i] + (expected_results[idx] - perceptron_output) * x for i, x in enumerate(sample)]\n",
        "    # The bias weight is updated as well, just like those associated with the inputs.\n",
        "    bias_weight = bias_weight + ((expected_results[idx] - perceptron_output) * 1)\n",
        "    weights = np.array(new_weights)\n",
        "\n",
        "    print('{} correct answers out of 4, for iteration {}'.format(correct_answers, iteration_num))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 correct answers out of 4, for iteration 0\n",
            "1 correct answers out of 4, for iteration 0\n",
            "2 correct answers out of 4, for iteration 0\n",
            "3 correct answers out of 4, for iteration 0\n",
            "0 correct answers out of 4, for iteration 1\n",
            "1 correct answers out of 4, for iteration 1\n",
            "1 correct answers out of 4, for iteration 1\n",
            "2 correct answers out of 4, for iteration 1\n",
            "0 correct answers out of 4, for iteration 2\n",
            "1 correct answers out of 4, for iteration 2\n",
            "2 correct answers out of 4, for iteration 2\n",
            "3 correct answers out of 4, for iteration 2\n",
            "1 correct answers out of 4, for iteration 3\n",
            "2 correct answers out of 4, for iteration 3\n",
            "3 correct answers out of 4, for iteration 3\n",
            "4 correct answers out of 4, for iteration 3\n",
            "1 correct answers out of 4, for iteration 4\n",
            "2 correct answers out of 4, for iteration 4\n",
            "3 correct answers out of 4, for iteration 4\n",
            "4 correct answers out of 4, for iteration 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5XYEXim-jSp",
        "colab_type": "text"
      },
      "source": [
        "Haha! What a good student your little perceptron is. By updating the weights in the inner loop, the perceptron is learning from its experience of the dataset.\n",
        "\n",
        "This is what is known as convergence. A model is said to converge when its error\n",
        "function settles to a minimum, or at least a consistent value. Sometimes you’re not so lucky. \n",
        "\n",
        "Sometimes a neural network bounces around looking for optimal weights to satisfy\n",
        "the relationships in a batch of data and never converges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRSce3h1_l4A",
        "colab_type": "text"
      },
      "source": [
        "### Perceptron verdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrCK2IrF_pUm",
        "colab_type": "text"
      },
      "source": [
        "The basic perceptron has an inherent flaw. If the data isn’t linearly separable, or the\n",
        "relationship cannot be described by a linear relationship, the model won’t converge\n",
        "and won’t have any useful predictive power. It won’t be able to predict the target variable\n",
        "accurately.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/linearly-separable-data.JPG?raw=1' width='800'/>\n",
        "\n",
        "Linearly separable data points are no problem for a perceptron.\n",
        "Crossed up data will cause a single-neuron perceptron to forever spin its\n",
        "wheels without learning to predict anything better than a random guess, a random\n",
        "flip of a coin. It’s not possible to draw a single line between your two classes (dots and Xs)\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/nonlinearly-separable-data.JPG?raw=1' width='800'/>\n",
        "\n",
        "A perceptron finds a linear equation that describes the relationship between the features\n",
        "of your dataset and the target variable in your dataset. A perceptron is just doing\n",
        "linear regression. A perceptron cannot describe a nonlinear equation or a nonlinear\n",
        "relationship.\n",
        "\n",
        "A lot of relationships between data values aren’t linear, and there’s no good linear\n",
        "regression or linear equation that describes those relationships. And many datasets\n",
        "aren’t linearly separable into classes with lines or planes. Because most data in the\n",
        "world isn’t cleanly separable with lines and planes.\n",
        "\n",
        "But the perceptron idea didn’t die easily. It resurfaced again when the Rumelhardt-\n",
        "McClelland collaboration effort (which Geoffrey Hinton was involved in) showed you\n",
        "could use the idea to solve the XOR problem with multiple perceptrons in concert.\n",
        "\n",
        "The key breakthrough by Rumelhardt-\n",
        "McClelland was the discovery of a way to allocate the error appropriately to each of the\n",
        "perceptrons. The way they did this was to use an old idea called backpropagation. With\n",
        "this idea for backpropagation across layers of neurons, the first modern neural network was born.\n",
        "\n",
        "The basic perceptron has the inherent flaw that if the data isn’t linearly separable,\n",
        "the model won’t converge to a solution with useful predictive power.\n",
        "\n",
        "Even though they could solve complex (nonlinear) problems, neural networks were,\n",
        "for a time, too computationally expensive.\n",
        "\n",
        "They proved impractical for common use, and they found their\n",
        "way back to the dusty shelves of academia and supercomputer experimentation. This\n",
        "began the second “**AI Winter**” that lasted from around 1990 to about 2010. But eventually\n",
        "computing power, backpropagation algorithms, and the proliferation of raw\n",
        "data, like labeled images of cats and dogs, caught up. \n",
        "\n",
        "Computationally expensive algorithms and limited datasets were no longer show-stoppers. Thus the third age of neural networks began.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6pMRde8FgOQ",
        "colab_type": "text"
      },
      "source": [
        "### Keras: Neural networks in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xkaYWZpUVzh",
        "colab_type": "text"
      },
      "source": [
        "Writing a neural network in raw Python is a fun experiment and can be helpful in putting\n",
        "all these pieces together, but Python is at a disadvantage regarding speed, and the\n",
        "shear number of calculations you’re dealing with can make even moderately sized networks intractable. Many Python libraries, though, get you around the speed zone:\n",
        "PyTorch, Theano, TensorFlow, Lasagne, and many more.\n",
        "\n",
        "Keras is a high-level wrapper with an accessible API for Python. The exposed API\n",
        "can be used with three different backends almost interchangeably: Theano, Tensor-\n",
        "Flow from Google, and CNTK from Microsoft. Each has its own low-level implementation\n",
        "of the basic neural network elements and has highly tuned linear algebra\n",
        "libraries to handle the dot products to make the matrix multiplications of neural networks\n",
        "as efficiently as possible.\n",
        "\n",
        "Let’s look at the simple XOR problem and see if you can train a network using\n",
        "Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWu5PqEk9sVX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "92de65e4-843b-44fd-b6e9-3118705b1fc8"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "# Our examples for an exclusive OR.\n",
        "x_train = np.array([\n",
        "    [0, 0],\n",
        "    [0, 1],\n",
        "    [1, 0],\n",
        "    [1, 1]                \n",
        "])\n",
        "y_train = np.array([\n",
        "    [0],\n",
        "    [1],\n",
        "    [1],\n",
        "    [0]                \n",
        "])\n",
        "\n",
        "# The fully connected hidden layer will have 10 neurons.\n",
        "model = Sequential()\n",
        "model.add(Dense(10, input_dim=2))\n",
        "model.add(Activation('tanh'))\n",
        "model.add(Dense(1))\n",
        "# The output layer has one neuron to output a single binary classification value (0 or 1).\n",
        "model.add(Activation('sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 10)                30        \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 11        \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 41\n",
            "Trainable params: 41\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WgiZsxTVn56",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "86997cf0-b686-4ed8-d7a3-195a386333fb"
      },
      "source": [
        "sgd = SGD(lr=0.1)\n",
        "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8mCjaa5W6IY",
        "colab_type": "text"
      },
      "source": [
        "SGD is the stochastic gradient descent optimizer you imported. This is just how the\n",
        "model will try to minimize the error, or loss. lr is the learning rate, the fraction applied\n",
        "to the derivative of the error with respect to each weight. Higher values will speed\n",
        "learn, but may force the model away from the global minimum by shooting past the\n",
        "goal; smaller values will be more precise but increase the training time and leave the\n",
        "model more vulnerable to local minima.\n",
        "\n",
        "The loss function itself is also defined as a\n",
        "parameter; here it’s binary_crossentropy. The metrics parameter is a list of\n",
        "options for the output stream during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pcgzf9g8WwAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "f4be31c3-67f5-45ea-e1bf-cc8ade5448bc"
      },
      "source": [
        "model.predict(x_train)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5       ],\n",
              "       [0.5662843 ],\n",
              "       [0.6204096 ],\n",
              "       [0.66136605]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfJCT69MXJGm",
        "colab_type": "text"
      },
      "source": [
        "The predict method gives the raw output of the last layer, which would be generated\n",
        "by the sigmoid function in this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irqibDE2XFj1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "ba6085f0-ad25-4499-c898-1a0503a748ec"
      },
      "source": [
        "model.predict_classes(x_train)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms45a-6lXXnm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "187a118e-8700-4b0c-abbe-1f2d2cbbfc65"
      },
      "source": [
        "model.predict(x_train)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5       ],\n",
              "       [0.5662843 ],\n",
              "       [0.6204096 ],\n",
              "       [0.66136605]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISiUGBzSXche",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}