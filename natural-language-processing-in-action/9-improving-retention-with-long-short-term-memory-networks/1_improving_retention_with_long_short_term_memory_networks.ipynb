{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-improving-retention-with-long-short-term-memory-networks.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-in-action/blob/9-text-with-long-short-term-memory-networks/1_improving_retention_with_long_short_term_memory_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO1preWdp2QJ",
        "colab_type": "text"
      },
      "source": [
        "# Improving retention with long short-term memory networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfJtCbhbqTT1",
        "colab_type": "text"
      },
      "source": [
        "For all the benefits recurrent neural nets provide for modeling relationships, and therefore possibly causal relationships, in sequence data they suffer from one main deficiency: a token’s effect is almost completely lost by the time two tokens have passed.\n",
        "\n",
        "This is important to the basic structure of the net, but it prevents the common case in human language that the tokens may be deeply interrelated even when they’re far apart in a sentence.\n",
        "\n",
        "Your challenge is to build a network that can pick up on the same core thought in both sentences. What you need is a way to remember the past across the entire input sequence. A long short-term memory (**LSTM**) is just what you need.\n",
        "\n",
        "Modern versions of a long short-term memory network typically use a special neural network unit called a gated recurrent unit (**GRU**). A gated recurrent unit can maintain both long- and short-term memory efficiently, enabling an **LSTM** to process a\n",
        "long sentence or document more accurately.\n",
        "\n",
        "In fact, **LSTMs** work so well they have replaced recurrent neural networks in almost all applications involving time series, discrete sequences, and **NLP**.\n",
        "\n",
        "\n",
        "First, you load the dataset, grab the labels, and shuffle the examples. Then you\n",
        "tokenize it and vectorize it again using the Google Word2vec model. Next, you grab the labels. And finally you split it 80/20 into the training and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8dMuJFaqjr2",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shf2mXZKrZg4",
        "colab_type": "code",
        "outputId": "decab385-53ba-4586-9730-e26c6914d565",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import backend as keras_backend\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, LSTM\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "import re\n",
        "import tqdm\n",
        "\n",
        "import glob\n",
        "from random import shuffle\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "import requests"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woc_3WGcp50m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install pugnlp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evwKZIKdp6h3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pugnlp.futil import path_status, find_files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swxx7Tmxr0np",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQymxGOrr3X5",
        "colab_type": "text"
      },
      "source": [
        "Each data point is prelabeled with a 0 (negative sentiment) or a 1 (positive sentiment).you’re going to swap out their example IMDB movie review dataset\n",
        "for one in raw text, so you can get your hands dirty with the preprocessing of the text as well. And then you’ll see if you can use this trained network to classify text it has never seen before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8RMv-lktomF",
        "colab_type": "text"
      },
      "source": [
        "### Downloading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1jLX7hOrsr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BIG_URLS = {\n",
        "    'w2v': ('https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1', 1647046227),\n",
        "    'slang': ('https://www.dropbox.com/s/43c22018fbfzypd/slang.csv.gz?dl=1', 117633024),\n",
        "    'tweets': ('https://www.dropbox.com/s/5gpb43c494mc8p0/tweets.csv.gz?dl=1', 311725313),\n",
        "    'lsa_tweets': ('https://www.dropbox.com/s/rpjt0d060t4n1mr/lsa_tweets_5589798_2003588x200.tar.gz?dl=1', 3112841563),  # 3112841312\n",
        "    'imdb': ('https://www.dropbox.com/s/yviic64qv84x73j/aclImdb_v1.tar.gz?dl=1', 3112841563),  # 3112841312\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHaBlbdwtXG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These functions are part of the nlpia package which can be pip installed and run from there.\n",
        "def dropbox_basename(url):\n",
        "    filename = os.path.basename(url)\n",
        "    match = re.findall(r'\\?dl=[0-9]$', filename)\n",
        "    if match:\n",
        "        return filename[:-len(match[0])]\n",
        "    return filename\n",
        "\n",
        "def download_file(url, data_path='.', filename=None, size=None, chunk_size=4096, verbose=True):\n",
        "    \"\"\"Uses stream=True and a reasonable chunk size to be able to download large (GB) files over https\"\"\"\n",
        "    if filename is None:\n",
        "        filename = dropbox_basename(url)\n",
        "    file_path = os.path.join(data_path, filename)\n",
        "    if url.endswith('?dl=0'):\n",
        "        url = url[:-1] + '1'  # noninteractive download\n",
        "    if verbose:\n",
        "        tqdm_prog = tqdm\n",
        "        print('requesting URL: {}'.format(url))\n",
        "    else:\n",
        "        tqdm_prog = no_tqdm\n",
        "    r = requests.get(url, stream=True, allow_redirects=True)\n",
        "    size = r.headers.get('Content-Length', None) if size is None else size\n",
        "    print('remote size: {}'.format(size))\n",
        "\n",
        "    stat = path_status(file_path)\n",
        "    print('local size: {}'.format(stat.get('size', None)))\n",
        "    if stat['type'] == 'file' and stat['size'] == size:  # TODO: check md5 or get the right size of remote file\n",
        "        r.close()\n",
        "        return file_path\n",
        "\n",
        "    print('Downloading to {}'.format(file_path))\n",
        "\n",
        "    with open(file_path, 'wb') as f:\n",
        "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
        "            if chunk:  # filter out keep-alive chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "    r.close()\n",
        "    return file_path\n",
        "\n",
        "def untar(fname):\n",
        "    if fname.endswith(\"tar.gz\"):\n",
        "        with tarfile.open(fname) as tf:\n",
        "            tf.extractall()\n",
        "    else:\n",
        "        print(\"Not a tar.gz file: {}\".format(fname))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0oWrTZ1uFHh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "37d5f652-df50-44dc-de78-cec397cb83bb"
      },
      "source": [
        "download_file(BIG_URLS['w2v'][0])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "requesting URL: https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz?dl=1\n",
            "remote size: 1647046227\n",
            "local size: 1647046227\n",
            "Downloading to ./GoogleNews-vectors-negative300.bin.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./GoogleNews-vectors-negative300.bin.gz'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I09TKCmzuF6_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "119cb663-765a-4ab4-f07b-4d0ca4b294c1"
      },
      "source": [
        "untar(download_file(BIG_URLS['imdb'][0]))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "requesting URL: https://www.dropbox.com/s/yviic64qv84x73j/aclImdb_v1.tar.gz?dl=1\n",
            "remote size: 84125825\n",
            "local size: 84125825\n",
            "Downloading to ./aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipv8IOS3v6AK",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing the loaded documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjVYBXqrwT7V",
        "colab_type": "text"
      },
      "source": [
        "The reviews in the train folder are broken up into text files in either the pos or neg folders. You’ll first need to read those in Python with their appropriate label and then shuffle the deck so the samples aren’t all positive and then all negative. Training with the sorted labels will skew training toward whatever comes last, especially when you use certain hyperparameters, such as momentum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtmvAhqfv5Mo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "from random import shuffle\n",
        "\n",
        "def pre_process_data(filepath):\n",
        "  '''\n",
        "  This is dependent on your training data source but we will try to generalize it as best as possible.\n",
        "  '''\n",
        "  positive_path = os.path.join(filepath, 'pos')\n",
        "  negative_path = os.path.join(filepath, 'neg')\n",
        "\n",
        "  pos_label = 1\n",
        "  neg_label = 0\n",
        "\n",
        "  dataset = []\n",
        "\n",
        "  for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
        "    with open(filename, 'r') as f:\n",
        "      dataset.append((pos_label, f.read()))\n",
        "\n",
        "  for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
        "    with open(filename, 'r') as f:\n",
        "      dataset.append((neg_label, f.read()))\n",
        "\n",
        "  shuffle(dataset)\n",
        "\n",
        "  return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W76Jm0mbGm3o",
        "colab_type": "text"
      },
      "source": [
        "### Data tokenization and vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT3S4Gd3FCvO",
        "colab_type": "text"
      },
      "source": [
        "The next step is to tokenize and vectorize the data. You’ll use the Google News pretrained Word2vec vectors, so download those directly from Google.\n",
        "\n",
        "You’ll use gensim to unpack the vectors, You can\n",
        "experiment with the limit argument to the load_word2vec_format method; a\n",
        "higher number will get you more vectors to play with, but memory quickly becomes an issue and return on investment drops quickly in really high values for limit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slErmWLvxsca",
        "colab_type": "code",
        "outputId": "3c5ca493-0fac-44ab-d40e-2ba7633f26e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit=200000)\n",
        "\n",
        "def tokenize_and_vectorize(dataset):\n",
        "  tokenizer = TreebankWordTokenizer()\n",
        "  vectorized_data = []\n",
        "  expected = []\n",
        "\n",
        "  for sample in dataset:\n",
        "    tokens = tokenizer.tokenize(sample[1])\n",
        "    sample_vecs = []\n",
        "    for token in tokens:\n",
        "      try:\n",
        "        sample_vecs.append(word_vectors[token])\n",
        "      except KeyError:\n",
        "        pass    # No matching token in the Google w2v vocab\n",
        "\n",
        "    vectorized_data.append(sample_vecs)\n",
        "\n",
        "  return vectorized_data"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1Z3bpwfIfyq",
        "colab_type": "text"
      },
      "source": [
        "You also need to collect the target values—0 for a negative review, 1 for a positive review—in the same order as the training samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC1qJoaNH5gF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collect_expected(dataset):\n",
        "  '''Peel of the target values from the dataset'''\n",
        "  expected = []\n",
        "  for sample in dataset:\n",
        "    expected.append(sample[0])\n",
        "  \n",
        "  return expected"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq18JWueUznn",
        "colab_type": "text"
      },
      "source": [
        "### Padding and truncating token sequence(sequences of vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuw3-u4QU8po",
        "colab_type": "text"
      },
      "source": [
        "Keras has a preprocessing helper method, pad_sequences, that in theory could be\n",
        "used to pad your input data, but unfortunately it works only with sequences of scalars, and you have sequences of vectors. \n",
        "\n",
        "Let’s write a helper function of your own to pad your input data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEGz1QlmU9L0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_trunc(data, maxlen):\n",
        "  '''For a given dataset pad with zero vectors or truncate to maxlen'''\n",
        "  new_data = []\n",
        "\n",
        "  # Create a vector of 0's the length of our word vectors\n",
        "  zero_vector = []\n",
        "  for _ in range(len(data[0][0])):\n",
        "    zero_vector.append(0.0)\n",
        "  #zero_vector = [0.0 for _ in range(len(data[0][0]))]\n",
        "\n",
        "  for sample in data:\n",
        "    if len(sample) > maxlen:\n",
        "        temp = sample[:maxlen]\n",
        "    elif len(sample) < maxlen:\n",
        "        temp = sample\n",
        "        additional_elems = maxlen - len(sample)\n",
        "        for _ in range(additional_elems):\n",
        "            temp.append(zero_vector)\n",
        "    else:\n",
        "        temp = sample\n",
        "    new_data.append(temp)\n",
        "  \n",
        "  return new_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u65UIysaI_cP",
        "colab_type": "text"
      },
      "source": [
        "### Train/Test splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EgeyjZgJHu4",
        "colab_type": "text"
      },
      "source": [
        "Next you’ll split the prepared data into a training set and a test set. You’re just going to split your imported dataset 80/20, but this ignores the folder of test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH0FKbQWXF3c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dce87f0d-7474-4454-c76c-42917ee34650"
      },
      "source": [
        "# gather the data and prep it.\n",
        "dataset = pre_process_data('./aclImdb_v1/train')\n",
        "print(len(dataset))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuO_XoY4JQd7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "e9f759f0-79f4-4ea7-9fe0-28759dae6db0"
      },
      "source": [
        "vectorized_data = tokenize_and_vectorize(dataset)\n",
        "expected = collect_expected(dataset)\n",
        "\n",
        "# split the data into training and testing sets.\n",
        "split_point = int(len(vectorized_data) * .8)\n",
        "\n",
        "x_train = vectorized_data[:split_point]\n",
        "y_train = expected[:split_point]\n",
        "x_test = vectorized_data[split_point:]\n",
        "y_test = expected[split_point:]"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-9374cdf5c3da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./aclImdb_v1/train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvectorized_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_and_vectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_expected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERwTnAjrKccV",
        "colab_type": "text"
      },
      "source": [
        "### Hyper-parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ_9-wsmKReT",
        "colab_type": "text"
      },
      "source": [
        "The next sets most of the hyperparameters for the net."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2Y0InHSKqGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = 400          # holds the maximum review length\n",
        "batch_size = 32       # How many samples to show the net before backpropagating the error and updating the weights\n",
        "embedding_dims = 300  # Length of the token vectors you’ll create for passing into the convnet\n",
        "epochs = 2            # Number of times we will pass the entire training dataset through the network"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKWpu1TYODk2",
        "colab_type": "text"
      },
      "source": [
        "Then you need to pass your train and test data into the padder/truncator. After that you can convert it to numpy arrays to make Keras happy. This is a tensor with the shape (number of samples, sequence length, word vector length) that you need for your CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGkviQPlFb_j",
        "colab_type": "code",
        "outputId": "0eab9bac-8a2d-4a5e-bf99-1e289517bb0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "source": [
        "# further prep the data by making each point of uniform length.\n",
        "x_train = pad_trunc(x_train, maxlen)\n",
        "x_test = pad_trunc(x_test, maxlen)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-297cfa8f4005>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_trunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_trunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-d188188f1eb1>\u001b[0m in \u001b[0;36mpad_trunc\u001b[0;34m(data, maxlen)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# Create a vector of 0's the length of our word vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mzero_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mzero_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m#zero_vector = [0.0 for _ in range(len(data[0][0]))]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6qc3GwKNxGx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape into a numpy data structure.\n",
        "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mj96pjMOvxX",
        "colab_type": "text"
      },
      "source": [
        "Phew; finally you’re ready to build a neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8CB1D4XOwQm",
        "colab_type": "text"
      },
      "source": [
        "## LSTM Long-Short-Term Memory network architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ildkqgWOyny",
        "colab_type": "text"
      },
      "source": [
        "**LSTMs** introduce the concept of a state for each layer in the recurrent network. The state acts as its memory. You can think of it as adding attributes to a class in objectoriented programming. The memory state’s attributes are updated with each training example.\n",
        "\n",
        "In LSTMs, the rules that govern the information stored in the state (memory) are trained neural nets themselves—therein lies the magic. They can be trained to learn what to remember, while at the same time the rest of the recurrent net learns to predict\n",
        "the target label! With the introduction of a memory and state, you can begin to learn dependencies that stretch not just one or two tokens away, but across the entirety of each data sample. With those long-term dependencies in hand, you can start to see beyond the words themselves and into something deeper about language.\n",
        "\n",
        "With LSTMs, patterns that humans take for granted and process on a subconscious level begin to be available to your model. And with those patterns, you can not only more accurately predict sample classifications, but you can start to generate novel text using those patterns. The state of the art in this field is still far from perfect, but the results you’ll see, even in your toy examples, are striking.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/lstm-network-1.png?raw=1' width='800'/>\n",
        "\n",
        "The memory state is affected by the input and also affects the layer output just as in a normal recurrent net. But that memory state persists across all the time steps of the time series (your sentence or document). So each input can have an effect on the memory state as well as an effect on the hidden layer output. The magic of the memory state is that it learns what to remember at the same time that it learns to reproduce the output, using standard backpropagation!\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/lstm-network-2.png?raw=1' width='800'/>\n",
        "\n",
        "It looks similar to a normal recurrent neural net. However, in addition to the activation output feeding into the next time-step version of the layer, you add a memory state that also passes through time steps of the network. At each time-step iteration, the hidden recurrent unit has access to the memory unit. The addition of this memory unit, and the mechanisms that interact with it, make this quite a bit different from a traditional neural network layer. However, you may like to know that it’s possible to design a set of traditional recurrent neural network layers (a computational graph) that accomplishes all the computations that exist within an LSTM layer. An LSTM layer is just a highly specialized recurrent neural network.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/lstm-network-3.png?raw=1' width='800'/>\n",
        "\n",
        "So let’s take a closer look at one of these cells. Instead of being a series of weights on the input and an activation function on those weights, each cell is now somewhat more complicated. As before, the input to the layer (or cell) is a combination of the input sample and output from the previous time step. As information flows into the cell instead of a vector of weights, it’s now greeted by three gates: \n",
        "* a forget gate, \n",
        "* an input/candidate gate, and \n",
        "* an output gate\n",
        "\n",
        "Each of these gates is a feed forward network layer composed of a series of weights that the network will learn, plus an activation function. Technically one of the gates is composed of two feed forward paths, so there will be four sets of weights to learn in this layer. The weights and activations will aim to allow information to flow through the cell in different amounts, all the way through to the cell’s state (or memory)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JRIDUL3uZbW",
        "colab_type": "text"
      },
      "source": [
        "### LSTM layer in Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgLDVCooUYXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_neurons = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S609TpBkWOEI",
        "colab_type": "text"
      },
      "source": [
        "One import and one line of Keras code changed. But a great deal more is going on under the surface. From the summary, you can see you have many more parameters to train than you did in the SimpleRNN from last chapter for the same number of neurons\n",
        "(50). Recall the simple RNN had the following weights:\n",
        "\n",
        "* 300 (one for each element of the input vector)\n",
        "* 1 (one for the bias term)\n",
        "* 50 (one for each neuron’s output from the previous time step)\n",
        "\n",
        "For a total of 351 per neuron.\n",
        "\n",
        "351 * 50 = 17,550 for the layer\n",
        "\n",
        "The cells have three gates (a total of four neurons):\n",
        "\n",
        "17,550 * 4 = 70,200\n",
        "\n",
        "But what is the memory? The memory is going to be represented by a vector that is the same number of elements as neurons in the cell. Your example has a relatively simple 50 neurons, so the memory unit will be a vector of floats that is 50 elements long.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/lstm-network-4.png?raw=1' width='800'/>\n",
        "\n",
        "You take the first token from the first sample and pass its 300-element vector representation into the first LSTM cell. On the way into the cell, the vector representation of the data is concatenated with the vector output from the previous time step (which is a 0 vector in the first time step). In this example, you’ll have a vector that is 300 + 50 elements long. Sometimes you’ll see a 1 appended to the vector—this corresponds to the bias term. Because the bias term always multiplies its associated weight by a value of one before passing to the activation function, that input is occasionally omitted from the input vector representation, to keep the diagrams more digestible.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/lstm-network-5.png?raw=1' width='800'/>\n",
        "\n",
        "At the first fork in the road, you hand off a copy of the combined input vector to the ominous sounding forget gate. The forget gate’s goal is to learn, based on a given input, how much of the cell’s memory you want to erase.\n",
        "\n",
        "The idea behind wanting to forget is as important as wanting to remember. As a human reader, when you pick up certain bits of information from text—say whether the noun is singular or plural—you want to retain that information so that later in the\n",
        "sentence you can recognize the right verb conjugation or adjective form to match with it. \n",
        "\n",
        "In romance languages, you’d have to recognize a noun’s gender, too, and use that later in a sentence as well. But an input sequence can easily switch from one noun to another, because an input sequence can be composed of multiple phrases, sentences, or even documents. As new thoughts are expressed in later statements, the fact that the noun is plural may not be at all relevant to later unrelated text.\n",
        "\n",
        "---\n",
        "\n",
        "*A thinker sees his own actions as experiments and questions—as attempts to find out something. Success and failure are for him answers above all.*\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this quote, the verb “see” is conjugated to fit with the noun “thinker.” The next active verb you come across is “to be” in the second sentence. At that point “be” is conjugated into “are” to agree with “Success and failure.” If you were to conjugate it to match the first noun you came across, “thinker,” you would use the wrong verb form, “is” instead. \n",
        "\n",
        "So an LSTM must model not only long-term dependencies within a\n",
        "sequence, but just as crucially, also forget long-term dependencies as new ones arise. This is what forgetting gates are for, making room for the relevant memories in your memory cells.\n",
        "\n",
        "The network isn’t working with these kinds of explicit representations. Your network is trying to find a set of weights to multiply by the inputs from the sequence of tokens so that the memory cell and the output are both updated in a way that minimizes the error. It’s amazing that they work at all. And they work very well indeed. But enough marveling: back to forgetting.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/lstm-network-6.png?raw=1' width='800'/>\n",
        "\n",
        "The forget gate itself is just a feed forward network. It consists of n neurons each with m + n + 1 weights. So your example forget gate has 50 neurons each with 351 (300 + 50 + 1) weights. The activation function for a forget gate is the sigmoid function, because you want the output for each neuron in the gate to be between 0 and 1.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/lstm-network-7.png?raw=1' width='800'/>\n",
        "\n",
        "The output vector of the forget gate is then a mask of sorts, albeit a porous one, that erases elements of the memory vector. As the forget gate outputs values closer to 1, more of the memory’s knowledge in the associated element is retained for that time step; the closer it is to 0 the more of that memory value is erased.\n",
        "\n",
        "Actively forgetting things, check. You better learn how to remember something new, or this is going to go south pretty quickly. Just like in the forget gate, you use a small network to learn how much to augment the memory based on two things: the\n",
        "input so far and the output from the last time step. This is what happens in the next gate you branch into: **the candidate gate**.\n",
        "\n",
        "The candidate gate has two separate neurons inside it that do two things:\n",
        "* Decide which input vector elements are worth remembering (similar to the mask in the forget gate)\n",
        "* Route the remembered input elements to the right memory “slot”\n",
        "\n",
        "The first part of a candidate gate is a neuron with a sigmoid activation function whose goal is to learn which input values of the memory vector to update. This neuron closely resembles the mask in the forget gate.\n",
        "\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/lstm-network-8.png?raw=1' width='800'/>\n",
        "\n",
        "The second part of this gate determines what values you’re going to update the memory with. This second part has a tanh activation function that forces the output value to range between -1 and 1. The output of these two vectors are multiplied\n",
        "together elementwise. The resulting vector from this multiplication is then added, again elementwise, to the memory register, thus remembering the new details.\n",
        "\n",
        "This gate is learning simultaneously which values to extract and the magnitude of those particular values. The mask and magnitude become what’s added to the memory state. As in the forget gate, the candidate gate is learning to mask off the inappropriate information before adding it to the cell’s memory.\n",
        "\n",
        "So old, hopefully irrelevant things are forgotten, and new things are remembered. Then you arrive at the last gate of the cell: **the output gate**.\n",
        "\n",
        "Up until this point in the journey through the cell, you’ve only written to the cell’s memory. Now it’s finally time to get some use out of this structure. The output gate takes the input (remember this is still the concatenation of the input at time step t and the output of the cell at time step t-1) and passes it into the output gate.\n",
        "\n",
        "The concatenated input is passed into the weights of the n neurons, and then you apply a sigmoid activation function to output an n-dimensional vector of floats, just like the output of a SimpleRNN. But instead of handing that information out through the cell wall, you pause.\n",
        "\n",
        "The memory structure you’ve built up is now primed, and it gets to weigh in on what you should output. This judgment is achieved by using the memory to create one last mask. This mask is a kind of gate as well, but you refrain from using that term because this mask doesn’t have any learned parameters, which differentiates it from the three previous gates described.\n",
        "\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/lstm-network-9.png?raw=1' width='800'/>\n",
        "\n",
        "The mask created from the memory is the memory state with a tanh function applied elementwise, which gives an n-dimensional vector of floats between -1 and 1.\n",
        "\n",
        "That mask vector is then multiplied elementwise with the raw vector computed in the output gate’s first step. The resulting n-dimensional vector is finally passed out of the cell as the cell’s official output at time step t.\n",
        "\n",
        "Thereby the memory of the cell gets the last word on what’s important to output at time step t, given what was input at time step t and output at t-1, and all the details it has gleaned up to this point in the input sequence.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56b1b3xkbk5z",
        "colab_type": "text"
      },
      "source": [
        "### Backpropagation through time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgRheA4yblaY",
        "colab_type": "text"
      },
      "source": [
        "Backpropagation—as with any other neural net. For a moment, let’s step back and look at the problem you’re trying to solve with this new complexity. A vanilla RNN is susceptible to a vanishing gradient because the derivative at any given time step is a factor of the weights themselves, so as you step back in time coalescing the various deltas, after a few iterations, the weights may shrink the gradient away to 0. \n",
        "\n",
        "The update to the weights at the end of the backpropagation\n",
        "(which would equate to the beginning of the sequence) are either\n",
        "minuscule or effectively 0. A similar problem occurs when the weights are somewhat large: the gradient explodes and grows disproportionately to the network.\n",
        "\n",
        "An LSTM avoids this problem via the memory state itself. The neurons in each of the gates are updated via derivatives of the functions they fed, namely those that update the memory state on the forward pass. \n",
        "\n",
        "So at any given time step, as the normal chain rule is applied backwards to the forward propagation, the updates to the neurons\n",
        "are dependent on only the memory state at that time step and the previous one.\n",
        "\n",
        "This way, the error of the entire function is kept “nearer” to the neurons for each time step. This is known as the error carousel.\n",
        "\n",
        "So you can just swap out the Keras SimpleRNN layer for the Keras\n",
        "LSTM layer, and all the other pieces of your classifier will stay the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-Gqkg8I0B_b",
        "colab_type": "text"
      },
      "source": [
        "### Traing and saving model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdBO-Z8T0DI3",
        "colab_type": "text"
      },
      "source": [
        "OK, now it’s time to actually train that recurrent network that we so carefully assembled\n",
        "in the previous section. As with your other Keras models, you need to give the\n",
        ".fit() method your data and tell it how long you want to run training (epochs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkF0zyt4s6IC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the model\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5aPTrww8HZ3",
        "colab_type": "text"
      },
      "source": [
        "You would like to save the model state after training.\n",
        "Because you aren’t going to hold the model in memory for now, you can grab its\n",
        "structure in a JSON file and save the trained weights in another file for later reinstantiation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viQIQimB8enz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_structure = model.to_json()   # Note that this doesn’t save the weights of the network, only the structure.\n",
        "\n",
        "# Save your trained model before you lose it!\n",
        "with open('lstm_model1.json', 'w') as json_file:\n",
        "  json_file.write(model_structure)\n",
        "model.save_weights('lstm_weights1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhheVs8B-k6A",
        "colab_type": "text"
      },
      "source": [
        "That is an enormous leap in the validation accuracy compared to the simple RNN. You can start to see how large a gain you can achieve by providing the model with a memory when the relationship of tokens is so important. The beauty of the algorithm is that it learns the relationships of the tokens it sees. The network is now able to model those relationships, specifically in the context of the cost function you provide.\n",
        "\n",
        "Now your trained model will be persisted on disk; should it converge, you won’t have to train it again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5ccRG3FOg_j",
        "colab_type": "text"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ko8dLd8N0Mr",
        "colab_type": "text"
      },
      "source": [
        "Let’s make up a sentence with an obvious negative sentiment and see what the network has to say about it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfls5muqpnEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading model\n",
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "with open('lstm_model1.json', 'r') as json_file:\n",
        "  json_string = json_file.read()\n",
        "model = model_from_json(json_string)\n",
        "\n",
        "model.load_weights('lstm_weights1.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDoFDTjjN1st",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_1 = \"\"\"\n",
        "I'm hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  \n",
        "The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ2hHRP4OKhG",
        "colab_type": "text"
      },
      "source": [
        "With the model pretrained, testing a new sample is quick. The are still thousands and\n",
        "thousands of calculations to do, but for each sample you only need one forward pass\n",
        "and no backpropagation to get a result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chBtRjF7OAlw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You pass a dummy value in the first element of the tuple just because\n",
        "# your helper expects it from the way you processed the initial data.\n",
        "# That value won’t ever see the network, so it can be anything.\n",
        "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
        "\n",
        "# Tokenize returns a list of the data (length 1 here)\n",
        "test_vec_list = pad_trunc(vec_list, maxlen)\n",
        "\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "print(\"Sample's sentiment, 1 - pos, 2 - neg : {}\".format(model.predict_classes(test_vec)))\n",
        "print(\"Raw output of sigmoid function: {}\".format(model.predict(test_vec)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHWgKfSjaHvT",
        "colab_type": "text"
      },
      "source": [
        "Going through this process of examining the probabilities and input data associated with incorrect predictions helps build your machine learning intuition so you can build better NLP pipelines in the future. This is backpropagation through the\n",
        "human brain for the problem of model tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k99l4XH7sO4W",
        "colab_type": "text"
      },
      "source": [
        "## Dirty data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGVEzI2ALJEJ",
        "colab_type": "text"
      },
      "source": [
        "This more powerful model still has a great number of hyperparameters to toy with. But now is a good time to pause and look back to the beginning, to your data. You’ve been using the same data, processed in exactly the same way since you started with convolutional neural nets, specifically so you could see the variations in the types of models and their performance on a given dataset.\n",
        "\n",
        "Padding or truncating each sample to 400 tokens was important for convolutional nets so that the filters could “scan” a vector with a consistent length. And convolutional nets output a consistent vector as well. It’s important for the output to be a consistent dimensionality, because the output goes into a fully connected feed forward layer at the end of the chain, which needs a fixed length vector as input.\n",
        "\n",
        "Similarly, your implementations of recurrent neural nets, both simple and LSTM, are striving toward a fixed length thought vector you can pass into a feed forward layer for classification. A fixed length vector representation of an object, such as a thought vector, is also called an embedding. So that the thought vector is of consistent size, you have to unroll the net to a consistent number of time steps (tokens). \n",
        "\n",
        "Let’s look at the choice of 400 as the number of time steps to unroll the net."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ20E_8Wc7cB",
        "colab_type": "text"
      },
      "source": [
        "### Optimize the thought vector size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zou3bzCyc9PT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDR7XE2rsRuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_neurons = 100\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(SimpleRNN(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)))\n",
        "model1.add(Dropout(0.2))\n",
        "\n",
        "model1.add(Flatten())\n",
        "model1.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model1.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model1.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un8caIkctTFp",
        "colab_type": "text"
      },
      "source": [
        "Train your larger network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZg2-d0XtT28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkIvMh6bt6mT",
        "colab_type": "text"
      },
      "source": [
        "The validation accuracy of 78.24% is only 0.04% better after we doubled the complexity of our model in one of the layers. This negligible improvement should lead you to think the model (for this network layer) is too complex for the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NPNfAz0tfto",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_structure = model1.to_json()   # Note that this doesn’t save the weights of the network, only the structure.\n",
        "\n",
        "# Save your trained model before you lose it!\n",
        "with open('simple_rnn_model2.json', 'w') as json_file:\n",
        "  json_file.write(model_structure)\n",
        "model1.save_weights('simple_rnn_weights2.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT--kwllu5-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading model\n",
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "with open('simple_rnn_model2.json', 'r') as json_file:\n",
        "  json_string = json_file.read()\n",
        "model = model_from_json(json_string)\n",
        "\n",
        "model.load_weights('simple_rnn_weights2.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpEQc5F7vAht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_1 = \"\"\"\n",
        "I'm hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  \n",
        "The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGyq5igPvFYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
        "\n",
        "# Tokenize returns a list of the data (length 1 here)\n",
        "test_vec_list = pad_trunc(vec_list, maxlen)\n",
        "\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "model.predict(test_vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ-Ki055vNha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.predict_classes(test_vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLdYsuVPvnmf",
        "colab_type": "text"
      },
      "source": [
        "If you feel the model is overfitting the training data but you can’t find a way to make\n",
        "your model simpler, you can always try increasing the Dropout(percentage). This is\n",
        "a sledgehammer (actually a shotgun) that can mitigate the risk of overfitting while\n",
        "allowing your model to have as much complexity as it needs to match the data. If you\n",
        "set the dropout percentage much above 50%, the model starts to have a difficult time\n",
        "learning. Your learning will slow and validation error will bounce around a lot. But\n",
        "20% to 50% is a pretty safe range for a lot of NLP problems for recurrent networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RN0BzYyxl48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Bidirectional\n",
        "\n",
        "num_neurons = 10\n",
        "maxlen = 100\n",
        "embedding_dims = 300\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Bidirectional(SimpleRNN(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims))))\n",
        "model2.add(Dropout(0.2))\n",
        "\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model2.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "model2.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
        "\n",
        "model2.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96hOA7_gx-uj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_structure = model2.to_json()   # Note that this doesn’t save the weights of the network, only the structure.\n",
        "\n",
        "# Save your trained model before you lose it!\n",
        "with open('simple_rnn_model3.json', 'w') as json_file:\n",
        "  json_file.write(model_structure)\n",
        "model2.save_weights('simple_rnn_weights3.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj7CdvI2zKhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading model\n",
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "with open('simple_rnn_model3.json', 'r') as json_file:\n",
        "  json_string = json_file.read()\n",
        "model = model_from_json(json_string)\n",
        "\n",
        "model.load_weights('simple_rnn_weights3.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "448cB57qzQNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_1 = \"\"\"\n",
        "I'm hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  \n",
        "The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWmODzZhzQ4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
        "\n",
        "# Tokenize returns a list of the data (length 1 here)\n",
        "test_vec_list = pad_trunc(vec_list, maxlen)\n",
        "\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "model.predict(test_vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPcWL6wuzTgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.predict_classes(test_vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv0HfYeR1LvQ",
        "colab_type": "text"
      },
      "source": [
        "With these tools you’re well on your way to not just predicting and classifying text, but\n",
        "actually modeling language itself and how it’s used. And with that deeper algorithmic\n",
        "understanding, instead of just parroting text your model has seen before, you can\n",
        "generate completely new statements!"
      ]
    }
  ]
}