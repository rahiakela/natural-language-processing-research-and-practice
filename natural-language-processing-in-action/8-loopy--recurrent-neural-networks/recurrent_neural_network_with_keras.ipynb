{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "recurrent_neural_network_with_keras.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-in-action/blob/master/8-loopy--recurrent-neural-networks/recurrent_neural_network_with_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO1preWdp2QJ"
      },
      "source": [
        "## Recurrent Neural Network with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfJtCbhbqTT1"
      },
      "source": [
        "First, you load the dataset, grab the labels, and shuffle the examples. Then you\n",
        "tokenize it and vectorize it again using the Google Word2vec model. Next, you grab the labels. And finally you split it 80/20 into the training and test sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8dMuJFaqjr2"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shf2mXZKrZg4"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import backend as keras_backend\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, SimpleRNN\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "import os\n",
        "import tarfile\n",
        "import re\n",
        "import tqdm\n",
        "\n",
        "import glob\n",
        "from random import shuffle\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "import requests"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcenJQHa5UN0"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# dowload w2v pretrained model\n",
        "wget https://www.dropbox.com/s/965dir4dje0hfi4/GoogleNews-vectors-negative300.bin.gz\n",
        "\n",
        "# unzip the dowloaded file\n",
        "gzip -d GoogleNews-vectors-negative300.bin.gz\n",
        "\n",
        "# dowload IMDB dataset\n",
        "https://www.dropbox.com/s/yviic64qv84x73j/aclImdb_v1.tar.gz\n",
        "\n",
        "# unzip IMDB dataset\n",
        "gzip -d aclImdb_v1.tar.gz\n",
        "tar -xvf aclImdb_v1.tar\n",
        "\n",
        "# remove tar file\n",
        "rm -rf aclImdb_v1.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swxx7Tmxr0np"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQymxGOrr3X5"
      },
      "source": [
        "Each data point is prelabeled with a 0 (negative sentiment) or a 1 (positive sentiment).you’re going to swap out their example IMDB movie review dataset\n",
        "for one in raw text, so you can get your hands dirty with the preprocessing of the text as well. And then you’ll see if you can use this trained network to classify text it has never seen before."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ipv8IOS3v6AK"
      },
      "source": [
        "### Preprocessing the loaded documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjVYBXqrwT7V"
      },
      "source": [
        "The reviews in the train folder are broken up into text files in either the pos or neg folders. You’ll first need to read those in Python with their appropriate label and then shuffle the deck so the samples aren’t all positive and then all negative. Training with the sorted labels will skew training toward whatever comes last, especially when you use certain hyperparameters, such as momentum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtmvAhqfv5Mo"
      },
      "source": [
        "import glob\n",
        "from random import shuffle\n",
        "\n",
        "def pre_process_data(filepath):\n",
        "  '''\n",
        "  This is dependent on your training data source but we will try to generalize it as best as possible.\n",
        "  '''\n",
        "  positive_path = os.path.join(filepath, 'pos')\n",
        "  negative_path = os.path.join(filepath, 'neg')\n",
        "\n",
        "  pos_label = 1\n",
        "  neg_label = 0\n",
        "\n",
        "  dataset = []\n",
        "\n",
        "  for filename in glob.glob(os.path.join(positive_path, '*.txt')):\n",
        "    with open(filename, 'r') as f:\n",
        "      dataset.append((pos_label, f.read()))\n",
        "\n",
        "  for filename in glob.glob(os.path.join(negative_path, '*.txt')):\n",
        "    with open(filename, 'r') as f:\n",
        "      dataset.append((neg_label, f.read()))\n",
        "\n",
        "  shuffle(dataset)\n",
        "\n",
        "  return dataset\n",
        "\n",
        "dataset = pre_process_data('./aclImdb/train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AB9vWVN9Oz-",
        "outputId": "c3f64b46-1b0e-4311-cd83-485732e47c75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataset[:5]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  'When you look back at another bad Nightmare sequel like Freddy\\'s Revenge, you have to at least give it some credit for trying something new. And although The Dream Child is more enjoyable it offers absolutely nothing new to the series. Yes, there\\'s the creative deaths as usual, like a kid becoming part of a comic book and facing \"Super Freddy\" but even scenes like that aren\\'t used to their full potential and the parts without Freddy are just boring.<br /><br />This marked the official death of scariness to the series. Freddy seems to be the comedic relief now...but to what?<br /><br />My Rating: 4/10'),\n",
              " (1,\n",
              "  'Simply put, this is the best movie to come out of Michigan since... well, ever! Evil Dead eat your heart out, Hatred of A Minute was some of the oddest, and best cinema to be seen by this reviewer in a long time. I recommend this movie to anyone who is in need of a head trip, or a good case of the willies!'),\n",
              " (1,\n",
              "  \"In 1943, a group of RAF Officers, including Eric Wiiliams, decide to escape from a POW camp using a Gymnastic Vaulting Horse in the courtyard. In 1950, it was decided to film his account, and it kick-started a peculiar British Film Genre- the Military Prison Camp story that reached its apogee in Danger Within (1959).<br /><br />The Wooden Horse is one of the quietest films I have ever watched. There are no great dramatic moments, but a steady storyline eventually builds to a climax that has more tension because the story doesn't give way for unlikely drama, jump cuts or jacked up (somethings about to happen!) music. It is utterly of its time and works beautifully.<br /><br />Leo Glenn, Anthony Steel and David Tomlinson lead a curiously low key cast of extras and (I suspect) non-actors. Without exception, all are constantly mono-tonal and quiet. They keep emotion out of their roles. As so many were, until recently, ex-service, I suspect they recreated their war time roles as 'Officers and Gentlemen'.<br /><br />This unemotional approach does not detract from any dramatic tension. On the contrary, unlike most Wartime Escape Films, the story doesn't end at the barbed wire: and that fact alone keeps me glued to the end.\"),\n",
              " (0,\n",
              "  \"Alex D. Linz replaces Macaulay Culkin as the central figure in the third movie in the Home Alone empire. Four industrial spies acquire a missile guidance system computer chip and smuggle it through an airport inside a remote controlled toy car. Because of baggage confusion, grouchy Mrs. Hess (Marian Seldes) gets the car. She gives it to her neighbor, Alex (Linz), just before the spies turn up. The spies rent a house in order to burglarize each house in the neighborhood until they locate the car. Home alone with the chicken pox, Alex calls 911 each time he spots a theft in progress, but the spies always manage to elude the police while Alex is accused of making prank calls. The spies finally turn their attentions toward Alex, unaware that he has rigged devices to cleverly booby-trap his entire house. Home Alone 3 wasn't horrible, but probably shouldn't have been made, you can't just replace Macauley Culkin, Joe Pesci, or Daniel Stern. Home Alone 3 had some funny parts, but I don't like when characters are changed in a movie series, view at own risk.\"),\n",
              " (0,\n",
              "  \"My kids picked this out at the video store...it's great to hear Liza as Dorothy cause she sounds just like her mom. But there are too many bad songs, and the animation is pretty crude compared to other cartoons of that time.\")]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W76Jm0mbGm3o"
      },
      "source": [
        "### Data tokenization and vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT3S4Gd3FCvO"
      },
      "source": [
        "The next step is to tokenize and vectorize the data. You’ll use the Google News pretrained Word2vec vectors, so download those directly from Google.\n",
        "\n",
        "You’ll use gensim to unpack the vectors, You can\n",
        "experiment with the limit argument to the load_word2vec_format method; a\n",
        "higher number will get you more vectors to play with, but memory quickly becomes an issue and return on investment drops quickly in really high values for limit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slErmWLvxsca"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, limit=200000)\n",
        "\n",
        "def tokenize_and_vectorize(dataset):\n",
        "  tokenizer = TreebankWordTokenizer()\n",
        "  vectorized_data = []\n",
        "  expected = []\n",
        "\n",
        "  for sample in dataset:\n",
        "    tokens = tokenizer.tokenize(sample[1])       # tokenize 2nd element of sample tuple\n",
        "    sample_vecs = []\n",
        "    for token in tokens:\n",
        "      try:\n",
        "        sample_vecs.append(word_vectors[token])  # map token to w2v vocab \n",
        "      except KeyError:\n",
        "        pass    # No matching token in the Google w2v vocab\n",
        "\n",
        "    vectorized_data.append(sample_vecs)\n",
        "\n",
        "  return vectorized_data"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGF3PpBhpe11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5220debb-cb58-47d7-9103-3e17d17ec639"
      },
      "source": [
        "word_vectors['dog']"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 5.12695312e-02, -2.23388672e-02, -1.72851562e-01,  1.61132812e-01,\n",
              "       -8.44726562e-02,  5.73730469e-02,  5.85937500e-02, -8.25195312e-02,\n",
              "       -1.53808594e-02, -6.34765625e-02,  1.79687500e-01, -4.23828125e-01,\n",
              "       -2.25830078e-02, -1.66015625e-01, -2.51464844e-02,  1.07421875e-01,\n",
              "       -1.99218750e-01,  1.59179688e-01, -1.87500000e-01, -1.20117188e-01,\n",
              "        1.55273438e-01, -9.91210938e-02,  1.42578125e-01, -1.64062500e-01,\n",
              "       -8.93554688e-02,  2.00195312e-01, -1.49414062e-01,  3.20312500e-01,\n",
              "        3.28125000e-01,  2.44140625e-02, -9.71679688e-02, -8.20312500e-02,\n",
              "       -3.63769531e-02, -8.59375000e-02, -9.86328125e-02,  7.78198242e-03,\n",
              "       -1.34277344e-02,  5.27343750e-02,  1.48437500e-01,  3.33984375e-01,\n",
              "        1.66015625e-02, -2.12890625e-01, -1.50756836e-02,  5.24902344e-02,\n",
              "       -1.07421875e-01, -8.88671875e-02,  2.49023438e-01, -7.03125000e-02,\n",
              "       -1.59912109e-02,  7.56835938e-02, -7.03125000e-02,  1.19140625e-01,\n",
              "        2.29492188e-01,  1.41601562e-02,  1.15234375e-01,  7.50732422e-03,\n",
              "        2.75390625e-01, -2.44140625e-01,  2.96875000e-01,  3.49121094e-02,\n",
              "        2.42187500e-01,  1.35742188e-01,  1.42578125e-01,  1.75781250e-02,\n",
              "        2.92968750e-02, -1.21582031e-01,  2.28271484e-02, -4.76074219e-02,\n",
              "       -1.55273438e-01,  3.14331055e-03,  3.45703125e-01,  1.22558594e-01,\n",
              "       -1.95312500e-01,  8.10546875e-02, -6.83593750e-02, -1.47094727e-02,\n",
              "        2.14843750e-01, -1.21093750e-01,  1.57226562e-01, -2.07031250e-01,\n",
              "        1.36718750e-01, -1.29882812e-01,  5.29785156e-02, -2.71484375e-01,\n",
              "       -2.98828125e-01, -1.84570312e-01, -2.29492188e-01,  1.19140625e-01,\n",
              "        1.53198242e-02, -2.61718750e-01, -1.23046875e-01, -1.86767578e-02,\n",
              "       -6.49414062e-02, -8.15429688e-02,  7.86132812e-02, -3.53515625e-01,\n",
              "        5.24902344e-02, -2.45361328e-02, -5.43212891e-03, -2.08984375e-01,\n",
              "       -2.10937500e-01, -1.79687500e-01,  2.42187500e-01,  2.57812500e-01,\n",
              "        1.37695312e-01, -2.10937500e-01, -2.17285156e-02, -1.38671875e-01,\n",
              "        1.84326172e-02, -1.23901367e-02, -1.59179688e-01,  1.61132812e-01,\n",
              "        2.08007812e-01,  1.03027344e-01,  9.81445312e-02, -6.83593750e-02,\n",
              "       -8.72802734e-03, -2.89062500e-01, -2.14843750e-01, -1.14257812e-01,\n",
              "       -2.21679688e-01,  4.12597656e-02, -3.12500000e-01, -5.59082031e-02,\n",
              "       -9.76562500e-02,  5.81054688e-02, -4.05273438e-02, -1.73828125e-01,\n",
              "        1.64062500e-01, -2.53906250e-01, -1.54296875e-01, -2.31933594e-02,\n",
              "       -2.38281250e-01,  2.07519531e-02, -2.73437500e-01,  3.90625000e-03,\n",
              "        1.13769531e-01, -1.73828125e-01,  2.57812500e-01,  2.35351562e-01,\n",
              "        5.22460938e-02,  6.83593750e-02, -1.75781250e-01,  1.60156250e-01,\n",
              "       -5.98907471e-04,  5.98144531e-02, -2.11914062e-01, -5.54199219e-02,\n",
              "       -7.51953125e-02, -3.06640625e-01,  4.27734375e-01,  5.32226562e-02,\n",
              "       -2.08984375e-01, -5.71289062e-02, -2.09960938e-01,  3.29589844e-02,\n",
              "        1.05468750e-01, -1.50390625e-01, -9.37500000e-02,  1.16699219e-01,\n",
              "        6.44531250e-02,  2.80761719e-02,  2.41210938e-01, -1.25976562e-01,\n",
              "       -1.00585938e-01, -1.22680664e-02, -3.26156616e-04,  1.58691406e-02,\n",
              "        1.27929688e-01, -3.32031250e-02,  4.07714844e-02, -1.31835938e-01,\n",
              "        9.81445312e-02,  1.74804688e-01, -2.36328125e-01,  5.17578125e-02,\n",
              "        1.83593750e-01,  2.42919922e-02, -4.31640625e-01,  2.46093750e-01,\n",
              "       -3.03955078e-02, -2.47802734e-02, -1.17187500e-01,  1.61132812e-01,\n",
              "       -5.71289062e-02,  1.16577148e-02,  2.81250000e-01,  4.27734375e-01,\n",
              "        4.56542969e-02,  1.01074219e-01, -3.95507812e-02,  1.77001953e-02,\n",
              "       -8.98437500e-02,  1.35742188e-01,  2.08007812e-01,  1.88476562e-01,\n",
              "       -1.52343750e-01, -2.37304688e-01, -1.90429688e-01,  7.12890625e-02,\n",
              "       -2.46093750e-01, -2.61718750e-01, -2.34375000e-01, -1.45507812e-01,\n",
              "       -1.17187500e-02, -1.50390625e-01, -1.13281250e-01,  1.82617188e-01,\n",
              "        2.63671875e-01, -1.37695312e-01, -4.58984375e-01, -4.68750000e-02,\n",
              "       -1.26953125e-01, -4.22363281e-02, -1.66992188e-01,  1.26953125e-01,\n",
              "        2.59765625e-01, -2.44140625e-01, -2.19726562e-01, -8.69140625e-02,\n",
              "        1.59179688e-01, -3.78417969e-02,  8.97216797e-03, -2.77343750e-01,\n",
              "       -1.04980469e-01, -1.75781250e-01,  2.28515625e-01, -2.70996094e-02,\n",
              "        2.85156250e-01, -2.73437500e-01,  1.61132812e-02,  5.90820312e-02,\n",
              "       -2.39257812e-01,  1.77734375e-01, -1.34765625e-01,  1.38671875e-01,\n",
              "        3.53515625e-01,  1.22070312e-01,  1.43554688e-01,  9.22851562e-02,\n",
              "        2.29492188e-01, -3.00781250e-01, -4.88281250e-02, -1.79687500e-01,\n",
              "        2.96875000e-01,  1.75781250e-01,  4.80957031e-02, -3.38745117e-03,\n",
              "        7.91015625e-02, -2.38281250e-01, -2.31445312e-01,  1.66015625e-01,\n",
              "       -2.13867188e-01, -7.03125000e-02, -7.56835938e-02,  1.96289062e-01,\n",
              "       -1.29882812e-01, -1.05957031e-01, -3.53515625e-01, -1.16699219e-01,\n",
              "       -5.10253906e-02,  3.39355469e-02, -1.43554688e-01, -3.90625000e-03,\n",
              "        1.73828125e-01, -9.96093750e-02, -1.66015625e-01, -8.54492188e-02,\n",
              "       -3.82812500e-01,  5.90820312e-02, -6.22558594e-02,  8.83789062e-02,\n",
              "       -8.88671875e-02,  3.28125000e-01,  6.83593750e-02, -1.91406250e-01,\n",
              "       -8.35418701e-04,  1.04003906e-01,  1.52343750e-01, -1.53350830e-03,\n",
              "        4.16015625e-01, -3.32031250e-02,  1.49414062e-01,  2.42187500e-01,\n",
              "       -1.76757812e-01, -4.93164062e-02, -1.24511719e-01,  1.25976562e-01,\n",
              "        1.74804688e-01,  2.81250000e-01, -1.80664062e-01,  1.03027344e-01,\n",
              "       -2.75390625e-01,  2.61718750e-01,  2.46093750e-01, -4.71191406e-02,\n",
              "        6.25000000e-02,  4.16015625e-01, -3.55468750e-01,  2.22656250e-01],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6cz5WwRBOTt",
        "outputId": "6e205451-b1d8-453d-ea9f-0bb956c41599",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word_vectors['cat']"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.0123291 ,  0.20410156, -0.28515625,  0.21679688,  0.11816406,\n",
              "        0.08300781,  0.04980469, -0.00952148,  0.22070312, -0.12597656,\n",
              "        0.08056641, -0.5859375 , -0.00445557, -0.296875  , -0.01312256,\n",
              "       -0.08349609,  0.05053711,  0.15136719, -0.44921875, -0.0135498 ,\n",
              "        0.21484375, -0.14746094,  0.22460938, -0.125     , -0.09716797,\n",
              "        0.24902344, -0.2890625 ,  0.36523438,  0.41210938, -0.0859375 ,\n",
              "       -0.07861328, -0.19726562, -0.09082031, -0.14160156, -0.10253906,\n",
              "        0.13085938, -0.00346375,  0.07226562,  0.04418945,  0.34570312,\n",
              "        0.07470703, -0.11230469,  0.06738281,  0.11230469,  0.01977539,\n",
              "       -0.12353516,  0.20996094, -0.07226562, -0.02783203,  0.05541992,\n",
              "       -0.33398438,  0.08544922,  0.34375   ,  0.13964844,  0.04931641,\n",
              "       -0.13476562,  0.16308594, -0.37304688,  0.39648438,  0.10693359,\n",
              "        0.22167969,  0.21289062, -0.08984375,  0.20703125,  0.08935547,\n",
              "       -0.08251953,  0.05957031,  0.10205078, -0.19238281, -0.09082031,\n",
              "        0.4921875 ,  0.03955078, -0.07080078, -0.0019989 , -0.23046875,\n",
              "        0.25585938,  0.08984375, -0.10644531,  0.00105286, -0.05883789,\n",
              "        0.05102539, -0.0291748 ,  0.19335938, -0.14160156, -0.33398438,\n",
              "        0.08154297, -0.27539062,  0.10058594, -0.10449219, -0.12353516,\n",
              "       -0.140625  ,  0.03491211, -0.11767578, -0.1796875 , -0.21484375,\n",
              "       -0.23828125,  0.08447266, -0.07519531, -0.25976562, -0.21289062,\n",
              "       -0.22363281, -0.09716797,  0.11572266,  0.15429688,  0.07373047,\n",
              "       -0.27539062,  0.14257812, -0.0201416 ,  0.10009766, -0.19042969,\n",
              "       -0.09375   ,  0.14160156,  0.17089844,  0.3125    , -0.16699219,\n",
              "       -0.08691406, -0.05004883, -0.24902344, -0.20800781, -0.09423828,\n",
              "       -0.12255859, -0.09472656, -0.390625  , -0.06640625, -0.31640625,\n",
              "        0.10986328, -0.00156403,  0.04345703,  0.15625   , -0.18945312,\n",
              "       -0.03491211,  0.03393555, -0.14453125,  0.01611328, -0.14160156,\n",
              "       -0.02392578,  0.01501465,  0.07568359,  0.10742188,  0.12695312,\n",
              "        0.10693359, -0.01184082, -0.24023438,  0.0291748 ,  0.16210938,\n",
              "        0.19921875, -0.28125   ,  0.16699219, -0.11621094, -0.25585938,\n",
              "        0.38671875, -0.06640625, -0.4609375 , -0.06176758, -0.14453125,\n",
              "       -0.11621094,  0.05688477,  0.03588867, -0.10693359,  0.18847656,\n",
              "       -0.16699219, -0.01794434,  0.10986328, -0.12353516, -0.16308594,\n",
              "       -0.14453125,  0.12890625,  0.11523438,  0.13671875,  0.05688477,\n",
              "       -0.08105469, -0.06152344, -0.06689453,  0.27929688, -0.19628906,\n",
              "        0.07226562,  0.12304688, -0.20996094, -0.22070312,  0.21386719,\n",
              "       -0.1484375 , -0.05932617,  0.05224609,  0.06445312, -0.02636719,\n",
              "        0.13183594,  0.19433594,  0.27148438,  0.18652344,  0.140625  ,\n",
              "        0.06542969, -0.14453125,  0.05029297,  0.08837891,  0.12255859,\n",
              "        0.26757812,  0.0534668 , -0.32226562, -0.20703125,  0.18164062,\n",
              "        0.04418945, -0.22167969, -0.13769531, -0.04174805, -0.00286865,\n",
              "        0.04077148,  0.07275391, -0.08300781,  0.08398438, -0.3359375 ,\n",
              "       -0.40039062,  0.01757812, -0.18652344, -0.0480957 , -0.19140625,\n",
              "        0.10107422,  0.09277344, -0.30664062, -0.19921875, -0.0168457 ,\n",
              "        0.12207031,  0.14648438, -0.12890625, -0.23535156, -0.05371094,\n",
              "       -0.06640625,  0.06884766, -0.03637695,  0.2109375 , -0.06005859,\n",
              "        0.19335938,  0.05151367, -0.05322266,  0.02893066, -0.27539062,\n",
              "        0.08447266,  0.328125  ,  0.01818848,  0.01495361,  0.04711914,\n",
              "        0.37695312, -0.21875   , -0.03393555,  0.01116943,  0.36914062,\n",
              "        0.02160645,  0.03466797,  0.07275391,  0.16015625, -0.16503906,\n",
              "       -0.296875  ,  0.15039062, -0.29101562,  0.13964844,  0.00448608,\n",
              "        0.171875  , -0.21972656,  0.09326172, -0.19042969,  0.01599121,\n",
              "       -0.09228516,  0.15722656, -0.14160156, -0.0534668 ,  0.03613281,\n",
              "        0.23632812, -0.15136719, -0.00689697, -0.27148438, -0.07128906,\n",
              "       -0.16503906,  0.18457031, -0.08398438,  0.18554688,  0.11669922,\n",
              "        0.02758789, -0.04760742,  0.17871094,  0.06542969, -0.03540039,\n",
              "        0.22949219,  0.02697754, -0.09765625,  0.26953125,  0.08349609,\n",
              "       -0.13085938, -0.10107422, -0.00738525,  0.07128906,  0.14941406,\n",
              "       -0.20605469,  0.18066406, -0.15820312,  0.05932617,  0.28710938,\n",
              "       -0.04663086,  0.15136719,  0.4921875 , -0.27539062,  0.05615234],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN06Mbs1-v5b",
        "outputId": "41e12af2-0bb2-403d-831a-1417fdebba61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(word_vectors['dog'].shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(300,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTIAPeHgA_pc"
      },
      "source": [
        "# now delete pretrained model after loading it into RAM\n",
        "!rm -rf GoogleNews-vectors-negative300.bin"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1Z3bpwfIfyq"
      },
      "source": [
        "You also need to collect the target values—0 for a negative review, 1 for a positive review—in the same order as the training samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC1qJoaNH5gF"
      },
      "source": [
        "def collect_expected(dataset):\n",
        "  '''Peel of the target values from the dataset'''\n",
        "  expected = []\n",
        "  for sample in dataset:\n",
        "    expected.append(sample[0])\n",
        "  \n",
        "  return expected"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAwhZvflImSK"
      },
      "source": [
        "And then you simply pass your data into those functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu5lfSiGIMwd"
      },
      "source": [
        "vectorized_data = tokenize_and_vectorize(dataset)\n",
        "expected = collect_expected(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xjU-wfB_UnR",
        "outputId": "c7a214c3-1aa9-4286-9f1e-b214617b75a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(len(expected))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u65UIysaI_cP"
      },
      "source": [
        "### Train/Test splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EgeyjZgJHu4"
      },
      "source": [
        "Next you’ll split the prepared data into a training set and a test set. You’re just going to split your imported dataset 80/20, but this ignores the folder of test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuO_XoY4JQd7"
      },
      "source": [
        "split_point = int(len(vectorized_data) * .8)\n",
        "\n",
        "x_train = vectorized_data[:split_point]\n",
        "y_train = expected[:split_point]\n",
        "x_test = vectorized_data[split_point:]\n",
        "y_test = expected[split_point:]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERwTnAjrKccV"
      },
      "source": [
        "### Hyper-parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ_9-wsmKReT"
      },
      "source": [
        "The next sets most of the hyperparameters for the net."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2Y0InHSKqGV"
      },
      "source": [
        "maxlen = 400          # holds the maximum review length\n",
        "batch_size = 32       # How many samples to show the net before backpropagating the error and updating the weights\n",
        "embedding_dims = 300  # Length of the token vectors you’ll create for passing into the convnet\n",
        "epochs = 2            # Number of times we will pass the entire training dataset through the network"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qAveehRMnBK"
      },
      "source": [
        "### Padding and truncating token sequence(sequences of vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Di-Sa4YOMshT"
      },
      "source": [
        "Keras has a preprocessing helper method, pad_sequences, that in theory could be\n",
        "used to pad your input data, but unfortunately it works only with sequences of scalars, and you have sequences of vectors. \n",
        "\n",
        "Let’s write a helper function of your own to pad your input data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v51zAgevM0Kq"
      },
      "source": [
        "def pad_trunc(data, maxlen):\n",
        "  '''For a given dataset pad with zero vectors or truncate to maxlen'''\n",
        "  new_data = []\n",
        "\n",
        "  # Create a vector of 0's the length of our word vectors\n",
        "  zero_vector = []\n",
        "  for _ in range(len(data[0][0])):\n",
        "    zero_vector.append(0.0)\n",
        "  #zero_vector = [0.0 for _ in range(len(data[0][0]))]\n",
        "\n",
        "  for sample in data:\n",
        "    if len(sample) > maxlen:\n",
        "        temp = sample[:maxlen]\n",
        "    elif len(sample) < maxlen:\n",
        "        temp = sample\n",
        "        additional_elems = maxlen - len(sample)\n",
        "        for _ in range(additional_elems):\n",
        "            temp.append(zero_vector)\n",
        "    else:\n",
        "        temp = sample\n",
        "    new_data.append(temp)\n",
        "  \n",
        "  return new_data"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKWpu1TYODk2"
      },
      "source": [
        "Then you need to pass your train and test data into the padder/truncator. After that you can convert it to numpy arrays to make Keras happy. This is a tensor with the shape (number of samples, sequence length, word vector length) that you need for your CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGkviQPlFb_j"
      },
      "source": [
        "x_train = pad_trunc(x_train, maxlen)\n",
        "x_test = pad_trunc(x_test, maxlen)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6qc3GwKNxGx"
      },
      "source": [
        "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mj96pjMOvxX"
      },
      "source": [
        "Phew; finally you’re ready to build a neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8CB1D4XOwQm"
      },
      "source": [
        "## Recurrent neural network architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ildkqgWOyny"
      },
      "source": [
        "Sequential is one of the base classes for neural networks in Keras. From here you can start to layer on the magic.\n",
        "\n",
        "```python\n",
        "model = Sequential()\n",
        "```\n",
        "And then, as before, the Keras magic handles the complexity of assembling a neural net: you just need to add the recurrent layer you want to your network.\n",
        "\n",
        "```python\n",
        "num_neurons = 50\n",
        "odel.add(SimpleRNN(num_neurons, return_sequences=True,input_shape=(maxlen,embedding_dims)))\n",
        "```\n",
        "\n",
        "Now the infrastructure is set up to take each input and pass it into a simple recurrent neural net and for each token, gather\n",
        "the output into a vector. Because your sequences are 400 tokens long and you’re using 50 hidden neurons, your output from this layer will be a vector 400 elements long. Each of those elements is a vector 50 elements long, with one output for each of the neurons.\n",
        "\n",
        "Notice here the keyword argument return_sequences. It’s going to tell the network to return the network value at each time step, hence the 400 vectors, each 50 long. If return_sequences was set to False (the Keras default behavior), only a single 50-dimensional vector would be returned.\n",
        "\n",
        "\n",
        "When using a recurrent neural net, truncating and padding isn’t usually necessary. You can provide training data of varying lengths and unroll the net until you hit the end of the input. Keras will handle this automatically. The catch is that your output of the recurrent layer will vary from time step to time step with the input. A four-token input will output a sequence four elements long. A 100-token sequence will produce a sequence of 100 elements. If you need to pass this into another layer, one that expects a uniform input, it won’t work. But there are cases where that’s acceptable, and even preferred. But back to your classifier.\n",
        "\n",
        "\n",
        "```python\n",
        "model.add(Dropout(.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "```\n",
        "\n",
        "You requested that the simple RNN return full sequences, but to prevent overfitting you add a Dropout layer to zero out 20% of those inputs, randomly chosen on each input example. And then finally you add a classifier. In this case, you have one class: “Yes - Positive Sentiment - 1” or “No - Negative Sentiment - 0,” so you chose a layer with one neuron (Dense(1)) and a sigmoid activation function. \n",
        "\n",
        "But a Dense layer expects a “flat” vector of n elements (each element a float) as input. And the data coming out of the SimpleRNN is a tensor 400 elements long, and each of those are 50 elements long. But a feedforward network doesn’t care about order of elements as long as you’re consistent with the order. You use the convenience layer, Flatten(), that Keras provides to flatten the input from a 400 x 50 tensor to a vector 20,000 elements long. And that’s what you pass into the final layer that’ll make the classification.\n",
        "\n",
        "In reality, the Flatten layer is a mapping. That means the error is backpropagated from the last layer back to the appropriate output in the RNN layer and each of those backpropagated errors are then backpropagated through time from the appropriate point in the output.\n",
        "\n",
        "Passing the “thought vector” produced by the recurrent neural network layer into a feedforward network no longer keeps the order of the input you tried so hard to incorporate. But the important takeaway is to notice that the “learning” related to\n",
        "sequence of tokens happens in the RNN layer itself; the aggregation of errors via backpropagation through time is encoding that relationship in the network and expressing\n",
        "it in the “thought vector” itself. Your decision based on the thought vector, via the classifier, is providing feedback to the “quality” of that thought vector with respect to your specific classification problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JRIDUL3uZbW"
      },
      "source": [
        "### Putting things together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgLDVCooUYXG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "81a9b082-bc96-42cc-e648-78065331607f"
      },
      "source": [
        "num_neurons = 50\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn (SimpleRNN)       (None, 400, 50)           17550     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 400, 50)           0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 20000)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 20001     \n",
            "=================================================================\n",
            "Total params: 37,551\n",
            "Trainable params: 37,551\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S609TpBkWOEI"
      },
      "source": [
        "In the SimpleRNN layer, you requested 50 neurons. Each of those neurons will\n",
        "receive input (and apply a weight to) each input sample. In an RNN, the input at each\n",
        "time step is one token. Your tokens are represented by word vectors in this case, each\n",
        "300 elements long (300-dimensional). Each neuron will need 300 weights:\n",
        "\n",
        "50 * 300 = 15,000\n",
        "\n",
        "Each neuron also has the bias term, which always has an input value of 1 (that’s what\n",
        "makes it a bias) but has a trainable weight:\n",
        "\n",
        "15,000 + 50 (bias weights) = 15,050\n",
        "\n",
        "15,050 weights in the first time step of the first layer. Now each of those 50 neurons\n",
        "will feed its output into the network’s next time step. Each neuron accepts the full\n",
        "input vector as well as the full output vector. In the first time step, the feedback from\n",
        "the output doesn’t exist yet. It’s initiated as a vector of zeros, its length the same as the\n",
        "length of the output.\n",
        "\n",
        "Each neuron in the hidden layer now has weights for each token embedding dimension:\n",
        "that’s 300 weights. It also has 1 bias for each neuron. And you have the 50 weights\n",
        "for the output results in the previous time step (or zeros for the first t=0 time step).\n",
        "These 50 weights are the key feedback step in a recurrent neural network. That gives us\n",
        "\n",
        "300 + 1 + 50 = 351\n",
        "\n",
        "351 times 50 neurons gives:\n",
        "\n",
        "351 * 50 = 17,550\n",
        "\n",
        "17,550 parameters to train. You’re unrolling this net 400 time steps (probably too much given the problems associated with vanishing gradients, but even so, this network turns out to still be effective). But those 17,550 parameters are the same in each of the unrollings, and they remain the same until all the backpropagations have been calculated.\n",
        "The updates to the weights occur at once at the end of the sequence forward propagation and subsequent backpropagation out to still be effective). But those 17,550 parameters are the same in each of the unrollings, and they remain the same until all the backpropagations have been calculated.\n",
        "The updates to the weights occur at once at the end of the sequence forward propagation and subsequent backpropagation Although you’re adding complexity to\n",
        "the backpropagation algorithm, you’re saved by the fact you’re not training a net with\n",
        "a little over 7 million parameters (17,550 * 400), which is what it would look like if the\n",
        "unrollings each had their own weight sets.\n",
        "\n",
        "The final layer in the summary is reporting 20,001 parameters to train, which is relatively straightforward. After the Flatten() layer, the input is a 20,000-dimensional vector plus the one bias input. Because you only have one neuron in the output layer, the total number of parameters is \n",
        "\n",
        "(20,000 input elements + 1 bias unit) * 1 neuron = 20,001 parameters\n",
        "\n",
        "Those numbers can be a little misleading in computational time because there are so many extra steps to backpropagation through time (compared to convolutional neural networks or standard feedforward networks). Computation time shouldn’t be a\n",
        "deal killer. Recurrent nets’ special talent at memory is the start of a bigger world in NLP or any other sequence data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-Gqkg8I0B_b"
      },
      "source": [
        "### Traing and saving model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdBO-Z8T0DI3"
      },
      "source": [
        "OK, now it’s time to actually train that recurrent network that we so carefully assembled\n",
        "in the previous section. As with your other Keras models, you need to give the\n",
        ".fit() method your data and tell it how long you want to run training (epochs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkF0zyt4s6IC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "a87ce9ff-8a10-4014-87c6-61e0b1345b4a"
      },
      "source": [
        "# train the model\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/2\n",
            "20000/20000 [==============================] - 176s 9ms/sample - loss: 0.5819 - accuracy: 0.7114 - val_loss: 0.5115 - val_accuracy: 0.7654\n",
            "Epoch 2/2\n",
            "20000/20000 [==============================] - 172s 9ms/sample - loss: 0.4252 - accuracy: 0.8071 - val_loss: 0.4587 - val_accuracy: 0.8006\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8df42ba3c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5aPTrww8HZ3"
      },
      "source": [
        "You would like to save the model state after training.\n",
        "Because you aren’t going to hold the model in memory for now, you can grab its\n",
        "structure in a JSON file and save the trained weights in another file for later reinstantiation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viQIQimB8enz"
      },
      "source": [
        "model_structure = model.to_json()   # Note that this doesn’t save the weights of the network, only the structure.\n",
        "\n",
        "# Save your trained model before you lose it!\n",
        "with open('simple_rnn_model1.json', 'w') as json_file:\n",
        "  json_file.write(model_structure)\n",
        "model.save_weights('simple_rnn_weights1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhheVs8B-k6A"
      },
      "source": [
        "Now your trained model will be persisted on disk; should it converge, you won’t have to train it again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5ccRG3FOg_j"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ko8dLd8N0Mr"
      },
      "source": [
        "Let’s make up a sentence with an obvious negative sentiment and see what the network has to say about it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfls5muqpnEl"
      },
      "source": [
        "# loading model\n",
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "with open('simple_rnn_model1.json', 'r') as json_file:\n",
        "  json_string = json_file.read()\n",
        "model = model_from_json(json_string)\n",
        "\n",
        "model.load_weights('simple_rnn_weights1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDoFDTjjN1st"
      },
      "source": [
        "sample_1 = \"\"\"\n",
        "I'm hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  \n",
        "The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ2hHRP4OKhG"
      },
      "source": [
        "With the model pretrained, testing a new sample is quick. The are still thousands and\n",
        "thousands of calculations to do, but for each sample you only need one forward pass\n",
        "and no backpropagation to get a result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chBtRjF7OAlw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c9fc9a56-7049-441b-e0c4-cff4a9ee3a49"
      },
      "source": [
        "# You pass a dummy value in the first element of the tuple just because\n",
        "# your helper expects it from the way you processed the initial data.\n",
        "# That value won’t ever see the network, so it can be anything.\n",
        "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
        "\n",
        "# Tokenize returns a list of the data (length 1 here)\n",
        "test_vec_list = pad_trunc(vec_list, maxlen)\n",
        "\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "model.predict(test_vec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.23188677]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTHN4qssPkN7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1a43bc6f-36ee-416a-ff55-82caaf37eea9"
      },
      "source": [
        "model.predict_classes(test_vec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k99l4XH7sO4W"
      },
      "source": [
        "### Build a larger network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDR7XE2rsRuB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "23534b34-8471-4a31-9bc9-81fd0315ffe7"
      },
      "source": [
        "num_neurons = 100\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(SimpleRNN(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims)))\n",
        "model1.add(Dropout(0.2))\n",
        "\n",
        "model1.add(Flatten())\n",
        "model1.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model1.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_2 (SimpleRNN)     (None, 400, 100)          40100     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 400, 100)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 40000)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 40001     \n",
            "=================================================================\n",
            "Total params: 80,101\n",
            "Trainable params: 80,101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un8caIkctTFp"
      },
      "source": [
        "Train your larger network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZg2-d0XtT28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "da96d2a1-b5f3-4f46-dc68-6b175248aa0e"
      },
      "source": [
        "model1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/2\n",
            "20000/20000 [==============================] - 165s 8ms/sample - loss: 0.6884 - accuracy: 0.6708 - val_loss: 0.5532 - val_accuracy: 0.7420\n",
            "Epoch 2/2\n",
            "20000/20000 [==============================] - 165s 8ms/sample - loss: 0.4485 - accuracy: 0.8050 - val_loss: 0.5035 - val_accuracy: 0.7776\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8ddc3bb3c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkIvMh6bt6mT"
      },
      "source": [
        "The validation accuracy of 78.24% is only 0.04% better after we doubled the complexity of our model in one of the layers. This negligible improvement should lead you to think the model (for this network layer) is too complex for the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NPNfAz0tfto"
      },
      "source": [
        "model_structure = model1.to_json()   # Note that this doesn’t save the weights of the network, only the structure.\n",
        "\n",
        "# Save your trained model before you lose it!\n",
        "with open('simple_rnn_model2.json', 'w') as json_file:\n",
        "  json_file.write(model_structure)\n",
        "model1.save_weights('simple_rnn_weights2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT--kwllu5-l"
      },
      "source": [
        "# loading model\n",
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "with open('simple_rnn_model2.json', 'r') as json_file:\n",
        "  json_string = json_file.read()\n",
        "model = model_from_json(json_string)\n",
        "\n",
        "model.load_weights('simple_rnn_weights2.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpEQc5F7vAht"
      },
      "source": [
        "sample_1 = \"\"\"\n",
        "I'm hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  \n",
        "The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGyq5igPvFYP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3c655adb-779c-424b-b0c4-3fd4374267e6"
      },
      "source": [
        "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
        "\n",
        "# Tokenize returns a list of the data (length 1 here)\n",
        "test_vec_list = pad_trunc(vec_list, maxlen)\n",
        "\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "model.predict(test_vec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.46618623]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ-Ki055vNha",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1d514f89-a8dc-406c-bb8b-843086283f2c"
      },
      "source": [
        "model.predict_classes(test_vec)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLdYsuVPvnmf"
      },
      "source": [
        "If you feel the model is overfitting the training data but you can’t find a way to make\n",
        "your model simpler, you can always try increasing the Dropout(percentage). This is\n",
        "a sledgehammer (actually a shotgun) that can mitigate the risk of overfitting while\n",
        "allowing your model to have as much complexity as it needs to match the data. If you\n",
        "set the dropout percentage much above 50%, the model starts to have a difficult time\n",
        "learning. Your learning will slow and validation error will bounce around a lot. But\n",
        "20% to 50% is a pretty safe range for a lot of NLP problems for recurrent networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d4_Zx0hQP8Y"
      },
      "source": [
        "## Statefulness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtKEj7dPQVSd"
      },
      "source": [
        "Sometimes you want to remember information from one input sample to the next, not just one-time step (token) to the next within a single sample.\n",
        "\n",
        "Keras provides a keyword argument in the base RNN layer called stateful. It defaults to False. If you flip this to True\n",
        "when adding the SimpleRNN layer to your model, the last sample’s last output passes\n",
        "into itself at the next time step along with the first token input, just as it would in the\n",
        "middle of the sample.\n",
        "\n",
        "Setting stateful to True can be a good idea when you want to model a large document\n",
        "that has been split into paragraphs or sentences for processing. And you might even use it to model the meaning of an entire corpus of related documents. But you\n",
        "wouldn’t want to train a stateful RNN on unrelated documents or passages without\n",
        "resetting the state of the model between samples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV-z-e_WwyUb"
      },
      "source": [
        "## Two-way street"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1tZQpqpwzd9"
      },
      "source": [
        "So far we’ve discussed relationships between words and what has come before. But\n",
        "can’t a case be made for flipping those word dependencies?\n",
        "\n",
        "*They wanted to pet the dog whose fur was brown.*\n",
        "\n",
        "As you get to the token “fur,” you have encountered “dog” already and know something\n",
        "about it. But the sentence also contains the information that the dog has fur,\n",
        "and that the dog’s fur is brown. And that information is relevant to the previous action\n",
        "of petting and the fact that “they” wanted to do the petting. Perhaps “they” only like to\n",
        "pet soft, furry brown things and don’t like petting prickly green things like cacti.\n",
        "\n",
        "Humans read the sentence in one direction but are capable of flitting back to earlier\n",
        "parts of the text in their brain as new information is revealed. Humans can deal\n",
        "with information that isn’t presented in the best possible order. It would be nice if you\n",
        "could allow your model to flit back across the input as well. That is where bidirectional\n",
        "recurrent neural nets come in.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/bidirectional-recurrent-neural-net.png?raw=1' width='800'/>\n",
        "\n",
        "The basic idea is you arrange two RNNs right next to each other, passing the input into one as normal and the same input backward into the other net.\n",
        "\n",
        "The output of those two are then concatenated at each time step to the related (same input token) time step in the other network. You take the output of the final time step\n",
        "in the input and concatenate it with the output generated by the same input token at the first time step of the backward net.\n",
        "\n",
        "**Note**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Keras also has a go_backwards keyword argument. If this is set to True, Keras automatically flips the input sequences and inputs them into the network in reverse order. This is the second half of a bidirectional layer.\n",
        "\n",
        "If you’re not using a bidirectional wrapper, this keyword can be useful, because a recurrent neural network (due to the vanishing gradients problem) is more receptive to data at the end of the sample than at the beginning. \n",
        "\n",
        "If you have padded your samples with <PAD> tokens at the end, all the good, juicy stuff is buried deep in the input loop. go_backwards can be a quick way around this problem.\n",
        "\n",
        "---\n",
        "\n",
        "Keras added a layer wrapper that will automatically flip\n",
        "around the necessary inputs and outputs to automatically assemble a bi-directional\n",
        "RNN for us.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RN0BzYyxl48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "outputId": "87c78a74-7827-43c6-b315-f888f5a9c626"
      },
      "source": [
        "from tensorflow.keras.layers import Bidirectional\n",
        "\n",
        "num_neurons = 10\n",
        "maxlen = 100\n",
        "embedding_dims = 300\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(Bidirectional(SimpleRNN(num_neurons, return_sequences=True, input_shape=(maxlen, embedding_dims))))\n",
        "model2.add(Dropout(0.2))\n",
        "\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model2.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "model2.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
        "\n",
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/2\n",
            "20000/20000 [==============================] - 314s 16ms/sample - loss: 0.5419 - accuracy: 0.7267 - val_loss: 0.4660 - val_accuracy: 0.7864\n",
            "Epoch 2/2\n",
            "20000/20000 [==============================] - 308s 15ms/sample - loss: 0.4351 - accuracy: 0.8043 - val_loss: 0.4362 - val_accuracy: 0.8122\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bidirectional_3 (Bidirection multiple                  6220      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "flatten_6 (Flatten)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              multiple                  8001      \n",
            "=================================================================\n",
            "Total params: 14,221\n",
            "Trainable params: 14,221\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96hOA7_gx-uj"
      },
      "source": [
        "model_structure = model2.to_json()   # Note that this doesn’t save the weights of the network, only the structure.\n",
        "\n",
        "# Save your trained model before you lose it!\n",
        "with open('simple_rnn_model3.json', 'w') as json_file:\n",
        "  json_file.write(model_structure)\n",
        "model2.save_weights('simple_rnn_weights3.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj7CdvI2zKhl"
      },
      "source": [
        "# loading model\n",
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "with open('simple_rnn_model3.json', 'r') as json_file:\n",
        "  json_string = json_file.read()\n",
        "model = model_from_json(json_string)\n",
        "\n",
        "model.load_weights('simple_rnn_weights3.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "448cB57qzQNF"
      },
      "source": [
        "sample_1 = \"\"\"\n",
        "I'm hate that the dismal weather that had me down for so long, when will it break! Ugh, when does happiness return?  \n",
        "The sun is blinding and the puffy clouds are too thin.  I can't wait for the weekend.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWmODzZhzQ4X"
      },
      "source": [
        "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
        "\n",
        "# Tokenize returns a list of the data (length 1 here)\n",
        "test_vec_list = pad_trunc(vec_list, maxlen)\n",
        "\n",
        "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
        "model.predict(test_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPcWL6wuzTgu"
      },
      "source": [
        "model.predict_classes(test_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iv0HfYeR1LvQ"
      },
      "source": [
        "With these tools you’re well on your way to not just predicting and classifying text, but\n",
        "actually modeling language itself and how it’s used. And with that deeper algorithmic\n",
        "understanding, instead of just parroting text your model has seen before, you can\n",
        "generate completely new statements!"
      ]
    }
  ]
}