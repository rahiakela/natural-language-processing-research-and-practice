{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-understanding-sentiment-using-bert-based-transfer-learning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPJMr/kdGdNoGigNWPA7Pcp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/advanced-natural-language-processing-with-tensorflow-2/blob/main/4-transfer-learning/2_understanding_sentiment_using_bert_based_transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZQmMt3biyFJ"
      },
      "source": [
        "##Understanding Sentiment using GloVe based transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2oEUmwSjwFy"
      },
      "source": [
        "We have used BiLSTM model to predict the sentiment of IMDb movie reviews. That model learned embeddings of the words from scratch. This model had an accuracy of `83.55%` on the test set, while the SOTA result was closer to `97.4%`. If pre-trained embeddings are used, we expect an increase in model accuracy. \n",
        "\n",
        "After all the setup is completed, we will need to use TensorFlow to use these pre-trained embeddings. There will be two different models that will be tried – \n",
        "- the first will be based on feature extraction\n",
        "- the second one on fine-tuning\n",
        "\n",
        "Let's try this out and see the impact of transfer learning on this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAlD10-fkC9C"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z-6lgCI10I0"
      },
      "source": [
        "!pip -q install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "u34tQYtIkEHv",
        "outputId": "3b74ea3b-b27d-4db2-e315-f0c4c87078d6"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import TFBertForSequenceClassification\n",
        "from transformers import TFBertModel\n",
        "\n",
        "# create training and validation splits\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn9fXsXr82zx"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hjVwYZ7kTr1",
        "outputId": "3e864ffc-dd0c-49c0-da0d-04a9f20bc9e4"
      },
      "source": [
        "######## GPU CONFIGS FOR RTX 2070 ###############\n",
        "## Please ignore if not training on GPU       ##\n",
        "## this is important for running CuDNN on GPU ##\n",
        "\n",
        "tf.keras.backend.clear_session() #- for easy reset of notebook state\n",
        "\n",
        "# chck if GPU can be seen by TF\n",
        "tf.config.list_physical_devices('GPU')\n",
        "# only if you want to see how commands are executed\n",
        "#tf.debugging.set_log_device_placement(True)\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Restrict TensorFlow to only use the first GPU\n",
        "  try:\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
        "  except RuntimeError as e:\n",
        "    # Visible devices must be set before GPUs have been initialized\n",
        "    print(e)\n",
        "###############################################"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 Physical GPUs, 1 Logical GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LojXqIvwhSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "526f3b34-1b1e-47f1-b096-03e87213fe6e"
      },
      "source": [
        "# Download the GloVe embeddings\n",
        "!wget -q http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpQRIAuqklY1"
      },
      "source": [
        "##Loading IMDb training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_KDedC2kmPN"
      },
      "source": [
        "TensorFlow Datasets or the tfds package will be used to load the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_KrBaGTkqro"
      },
      "source": [
        "imdb_train, ds_info = tfds.load(name=\"imdb_reviews\", split=\"train\", with_info=True, as_supervised=True)\n",
        "imdb_test = tfds.load(name=\"imdb_reviews\", split=\"test\", as_supervised=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUvRVMTVm8qc",
        "outputId": "54d50d1f-70d6-4c4b-87e4-76ce2765b74f"
      },
      "source": [
        "# Check label and example from the dataset\n",
        "for example, label in imdb_train.take(1):\n",
        "  print(example, \"\\n\", label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string) \n",
            " tf.Tensor(0, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5gpnqf_1_v8"
      },
      "source": [
        "##Tokenization and normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4M0orQZ2C_z"
      },
      "source": [
        "Hugging Face have provided pre-trained models as well as abstractions that make working with advanced models like BERT a breeze. The general flow for getting BERT to work will be:\n",
        "\n",
        "1. Load a pre-trained model\n",
        "2. Instantiate a tokenizer and tokenize the data\n",
        "3. Set up a model and compile it\n",
        "4. Fit the model on the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QERtH0IW2-zg"
      },
      "source": [
        "### 1-Load pre-trained model tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27nJwz673MfS"
      },
      "source": [
        "The tokenizer is the first step – it needs to be imported before it can be used:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dd8oiPNl3M7l"
      },
      "source": [
        "# downloads the configuration and the vocabulary file from the cloud and instantiates a tokenizer\n",
        "bert_name = \"bert-base-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_name,\n",
        "                                          add_special_tokens=True,\n",
        "                                          do_lower_case=False,\n",
        "                                          max_length=150,\n",
        "                                          pad_to_max_length=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHR0Nhuo62fi"
      },
      "source": [
        "There are three sequences that need to be provided to the BERT model:\n",
        "\n",
        "- **input_ids**: This corresponds to the tokens in the inputs converted into IDs.\n",
        "- **token_type_ids**: If the input contains two sequences then these IDs tell the model indicates which input_ids correspond to which sequence.\n",
        "- **attention_mask**: Given that the sequences are padded, this mask tells the\n",
        "model where the actual tokens end so that the attention calculation does not\n",
        "use the padding tokens.\n",
        "\n",
        "If the input sequence was \"Don't be lured\", then the figure shows how it is\n",
        "tokenized with the WordPiece tokenizer as well as the addition of special tokens.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/advanced-nlp-with-tensorflow-2/bert-sequences.png?raw=1' width='800'/>\n",
        "\n",
        "Only one sequence is provided, hence the token type IDs or segment IDs all have the same value. The attention mask is set to 1, where the corresponding entry in the tokens is an actual token.\n",
        "\n",
        "Let's generate these encodings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Lri3oTX3hFN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff679fab-72b8-4e7b-bfca-b95267afb622"
      },
      "source": [
        "tokenizer.encode_plus(\" Don't be lured\",\n",
        "                      add_special_tokens=True,\n",
        "                      max_length=9,\n",
        "                      truncation=True,\n",
        "                      pad_to_max_length=True,\n",
        "                      return_attention_mask=True,\n",
        "                      return_token_type_ids=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 1790, 112, 189, 1129, 19615, 1181, 102, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MplwczZ9Dw2"
      },
      "source": [
        "If two strings are passed to the tokenizer, then they are treated as a pair."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zHyx8LW9DNk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcde47e0-d19f-41d5-9759-fb783577cc55"
      },
      "source": [
        "tokenizer.encode_plus(\" Don't be\", \" lured\",\n",
        "                      add_special_tokens=True,\n",
        "                      max_length=9,\n",
        "                      truncation=True,\n",
        "                      pad_to_max_length=True,\n",
        "                      return_attention_mask=True,\n",
        "                      return_token_type_ids=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 1790, 112, 189, 1129, 102, 19615, 1181, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z44Xmbr-9eNl"
      },
      "source": [
        "The input IDs have two separators to distinguish between the two sequences. The\n",
        "token type IDs help distinguish which tokens correspond to which sequence. \n",
        "\n",
        "Note that the token type ID for the padding token is set to 0. In the network, it is never used as all the values are multiplied by the attention mask.\n",
        "\n",
        "To perform encoding of the inputs for all the IMDb reviews, a helper function is\n",
        "defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVr8de6b8j0v"
      },
      "source": [
        "def bert_encoder(review):\n",
        "  txt = review.numpy().decode(\"utf-8\")\n",
        "  encoded = tokenizer.encode_plus(txt, \n",
        "                                  add_special_tokens=True,\n",
        "                                  max_length=150,\n",
        "                                  truncation=True,\n",
        "                                  pad_to_max_length=True,\n",
        "                                  return_attention_mask=True,\n",
        "                                  return_token_type_ids=True)\n",
        "  return encoded[\"input_ids\"], encoded[\"token_type_ids\"], encoded[\"attention_mask\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21LcJxwA_cwh"
      },
      "source": [
        "Now, this needs to be applied to every review in the training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLgYl7ua_aiI"
      },
      "source": [
        "bert_train = [bert_encoder(review) for review, label in imdb_train]\n",
        "bert_label = [label for review, label in imdb_train]\n",
        "\n",
        "bert_train = np.array(bert_train)\n",
        "# Labels of the reviews are also converted into categorical values.\n",
        "bert_label = tf.keras.utils.to_categorical(bert_label, num_classes=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eak53QOFAABs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "938a5cad-f6fc-4a87-b62c-1ed8f6dfcf3d"
      },
      "source": [
        "print(bert_train.shape, bert_label.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 3, 150) (25000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44cTSYkEBN3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9671393c-de57-412e-de53-3091f0f79122"
      },
      "source": [
        "# create training and validation splits\n",
        "x_train, x_val, y_train, y_val = train_test_split(bert_train, bert_label, test_size=0.2, random_state=42)\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_val.shape, y_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 3, 150) (20000, 2)\n",
            "(5000, 3, 150) (5000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqqqAz4y0Y9_"
      },
      "source": [
        "A little more data processing is required to wrangle the inputs into three input\n",
        "dictionaries in `tf.DataSet` for easy use in training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itgg1e-zB47f"
      },
      "source": [
        "train_reviews, train_segments, train_masks = np.split(x_train, 3, axis=1)\n",
        "val_reviews, val_segments, val_masks = np.split(x_val, 3, axis=1)\n",
        "\n",
        "train_reviews = train_reviews.squeeze()\n",
        "train_segments = train_segments.squeeze()\n",
        "train_masks = train_masks.squeeze()\n",
        "\n",
        "val_reviews = val_reviews.squeeze()\n",
        "val_segments = val_segments.squeeze()\n",
        "val_masks = val_masks.squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bikCAqw-1syG"
      },
      "source": [
        "These training and validation sequences are converted into a dataset like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlhBG3ZU1ta_"
      },
      "source": [
        "def example_to_features(input_ids, attention_masks, token_type_ids, y):\n",
        "  return {\n",
        "      \"input_ids\": input_ids,\n",
        "      \"attention_mask\": attention_masks,\n",
        "      \"token_type_ids\": token_type_ids\n",
        "  }, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2V9hY6V2QTk"
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((train_reviews, train_masks, train_segments, y_train)) \\\n",
        "                          .map(example_to_features) \\\n",
        "                          .shuffle(100) \\\n",
        "                          .batch(16) \n",
        "\n",
        "valid_ds = tf.data.Dataset.from_tensor_slices((val_reviews, val_masks, val_segments, y_val)) \\\n",
        "                          .map(example_to_features) \\\n",
        "                          .shuffle(100) \\\n",
        "                          .batch(16) \\"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBHIseSQ5AFQ"
      },
      "source": [
        "A batch size of 16 has been used here. The memory of the GPU is the limiting factor here. Google Colab can support a batch length of 32. An 8 GB RAM GPU can support a batch size of 16.\n",
        "\n",
        "Now, we are ready to train a model using BERT for classification.\n",
        "We will see two approaches. \n",
        "\n",
        "- The first approach will use a pre-built classification\n",
        "model on top of BERT.\n",
        "- The second approach will use the base BERT model and adds custom layers on top to accomplish the same task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEIWvKRg5dfS"
      },
      "source": [
        "##Pre-built BERT classification model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNMel40N5eJx"
      },
      "source": [
        "Hugging Face libraries make it really easy to use a pre-built BERT model for\n",
        "classification by providing a class to do so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6ZjfDJS3Mg0"
      },
      "source": [
        "bert_model = TFBertForSequenceClassification.from_pretrained(bert_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz8f7iyj6AWV"
      },
      "source": [
        "To use this model, we only need to provide an optimizer and a loss\n",
        "function and compile the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etG-bxLp5xeh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e97ca52-60c3-4191-9a0e-2b169d3adf08"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "bert_model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
        "bert_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_sequence_classification\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert (TFBertMainLayer)       multiple                  108310272 \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         multiple                  0         \n",
            "_________________________________________________________________\n",
            "classifier (Dense)           multiple                  1538      \n",
            "=================================================================\n",
            "Total params: 108,311,810\n",
            "Trainable params: 108,311,810\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGTjdVWb7KuD"
      },
      "source": [
        "So, the model has the entire BERT model, a dropout layer, and a classifier layer on top. This is as simple as it gets.\n",
        "\n",
        ">The BERT paper suggests some settings for fine-tuning. They\n",
        "suggest a batch size of `16` or `32`, run for `2` to `4` epochs. Further,\n",
        "they suggest using one of the following learning rates for Adam:\n",
        "`5e-5, 3e-5, or 2e-5`.\n",
        "\n",
        "We batched the data into sets of `16`. Here, the Adam optimizer is configured to use a learning rate of `2e-5`.\n",
        "\n",
        "Let's train this model for 3 epochs. Note that training is going to be quite slow:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-R5EnF-6gv2"
      },
      "source": [
        "print(\"Fine-tuning BERT on IMDB\")\n",
        "bert_history = bert_model.fit(train_ds, epochs=3, validation_data=valid_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsDotTve9NBI"
      },
      "source": [
        "The validation accuracy is quite impressive for the little work we have done here if it holds on the test set. That needs to be checked next. Using the convenience methods, the test data will be tokenized and encoded in the right format:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGx8upS68UK5"
      },
      "source": [
        "# prep data for testing\n",
        "bert_test = [bert_encoder(review) for review, label in imdb_test]\n",
        "bert_test_label = [label for review, label in imdb_test]\n",
        "\n",
        "bert_test2 = np.array(bert_test)\n",
        "# Labels of the reviews are also converted into categorical values.\n",
        "bert_test_label2 = tf.keras.utils.to_categorical(bert_test_label, num_classes=2)\n",
        "\n",
        "test_reviews, test_segments, test_masks = np.split(bert_test2, 3, axis=1)\n",
        "\n",
        "test_reviews = test_reviews.squeeze()\n",
        "test_segments = test_segments.squeeze()\n",
        "test_masks = test_masks.squeeze()\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_reviews, test_masks, test_segments, bert_test_label2)) \\\n",
        "                         .map(example_to_features) \\\n",
        "                         .shuffle(100) \\\n",
        "                         .batch(16) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQcpGRzfAEwx"
      },
      "source": [
        "Evaluating the performance of this model on the test dataset, we get the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WscMKmfXAHVz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a80110cd-739b-4e78-f49c-d7bcb11ea611"
      },
      "source": [
        "bert_model.evaluate(test_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1563/1563 [==============================] - 276s 176ms/step - loss: 0.4078 - accuracy: 0.8810\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4078245759010315, 0.8809599876403809]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl4rIsarASFk"
      },
      "source": [
        "The model accuracy is almost 88%! This is higher than the best GloVe model shown\n",
        "previously, and it took much less code to implement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLc5PMGlAZzN"
      },
      "source": [
        "##Custom model with BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJM_3SAhAa4H"
      },
      "source": [
        "The BERT model outputs contextual embeddings for all of the input tokens. The\n",
        "embedding corresponding to the `[CLS]` token is generally used for classification tasks, and it represents the entire document. \n",
        "\n",
        "The pre-built model from Hugging Face returns the embeddings for the entire sequence as well as this pooled output, which represents the entire document as the output of the model. This pooled output vector can be used in future layers to help with the classification task. This is the approach we will take in building a customer model.\n",
        "\n",
        "The starting point for this exploration is the base TFBertModel. It can be imported and instantiated like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E058ysQe1pUy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "694c6688-1eb0-41fc-bc19-d10f0d2ba9f9"
      },
      "source": [
        "bert_name = \"bert-base-cased\"\n",
        "bert = TFBertModel.from_pretrained(bert_name)\n",
        "bert.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert (TFBertMainLayer)       multiple                  108310272 \n",
            "=================================================================\n",
            "Total params: 108,310,272\n",
            "Trainable params: 108,310,272\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzk6D6Cu3gtk"
      },
      "source": [
        "Since we are using the same pre-trained model, the cased BERT-Base model, we\n",
        "can reuse the tokenized and prepared data from the section above.\n",
        "\n",
        "Now, the custom model needs to be defined. The first layer of this model is the BERT layer. This layer will take three inputs, namely the input tokens, attention masks, and token type IDs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S538MBkT3qeu"
      },
      "source": [
        "max_seq_len = 150\n",
        "\n",
        "input_ids = tf.keras.layers.Input((max_seq_len,), dtype=tf.int64, name=\"input_ids\")\n",
        "attention_mask = tf.keras.layers.Input((max_seq_len,), dtype=tf.int64, name=\"attention_mask\")\n",
        "token_type_ids = tf.keras.layers.Input((max_seq_len,), dtype=tf.int64, name=\"token_type_ids\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIKkFPZa4nsj"
      },
      "source": [
        "These names need to match the dictionary defined in the training and testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQV13C3o4ooI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "808ea494-b6dd-4656-db29-240982d03625"
      },
      "source": [
        "train_ds.element_spec"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'attention_mask': TensorSpec(shape=(None, 150), dtype=tf.int64, name=None),\n",
              "  'input_ids': TensorSpec(shape=(None, 150), dtype=tf.int64, name=None),\n",
              "  'token_type_ids': TensorSpec(shape=(None, 150), dtype=tf.int64, name=None)},\n",
              " TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1owtVTyL8SxX"
      },
      "source": [
        "**The BERT model expects these inputs in a dictionary. It can also accept the inputs as named arguments, but this approach is clearer and makes it easy to trace the inputs.**\n",
        "\n",
        "Once the inputs are mapped, the output of the BERT model can be computed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IvzXvkg8cKY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97221d02-5866-4408-cb1f-060d47b38ae5"
      },
      "source": [
        "input_dict = {\n",
        "    \"input_ids\": input_ids,\n",
        "    \"attention_mask\": attention_mask,\n",
        "    \"token_type_ids\": token_type_ids\n",
        "}\n",
        "\n",
        "outputs = bert(input_dict)\n",
        "# let's see the output structure\n",
        "outputs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TFBaseModelOutputWithPooling([('last_hidden_state',\n",
              "                               <KerasTensor: shape=(None, 150, 768) dtype=float32 (created by layer 'tf_bert_model_1')>),\n",
              "                              ('pooler_output',\n",
              "                               <KerasTensor: shape=(None, 768) dtype=float32 (created by layer 'tf_bert_model_1')>)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CuNiir09NT7"
      },
      "source": [
        "The first output has embeddings for each of the input tokens including the special tokens `[CLS]` and `[SEP]`. \n",
        "\n",
        "The second output corresponds to the output of the `[CLS]` token. \n",
        "\n",
        "This output will be used further in the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dnll8HjC9T15"
      },
      "source": [
        "x = tf.keras.layers.Dropout(0.2)(outputs[1])\n",
        "x = tf.keras.layers.Dense(200, activation=\"relu\")(x)\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\n",
        "x = tf.keras.layers.Dense(2, activation=\"sigmoid\")(x)\n",
        "\n",
        "custom_model = tf.keras.models.Model(inputs=input_dict, outputs=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04pFSElv_21J"
      },
      "source": [
        "We add a dense layer and a couple of dropout layers before an output layer. Now, the custom model is ready for training. \n",
        "\n",
        "The model needs to be compiled with an optimizer, loss function, and metrics to watch for:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f94QsK3J_7qX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87a0e1c0-f341-4c8f-9857-c3bc2c49a162"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "custom_model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
        "custom_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "attention_mask (InputLayer)     [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_ids (InputLayer)          [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "token_type_ids (InputLayer)     [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model_1 (TFBertModel)   TFBaseModelOutputWit 108310272   attention_mask[0][0]             \n",
            "                                                                 input_ids[0][0]                  \n",
            "                                                                 token_type_ids[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_112 (Dropout)           (None, 768)          0           tf_bert_model_1[0][1]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 200)          153800      dropout_112[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_113 (Dropout)           (None, 200)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            402         dropout_113[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 108,464,474\n",
            "Trainable params: 108,464,474\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2zNfIm6Afac"
      },
      "source": [
        "This custom model has 154,202 additional trainable parameters in addition to the\n",
        "BERT parameters. The model is ready to be trained. \n",
        "\n",
        "We will use the same settings from the previous BERT section and train the model for 3 epochs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw8o98IjA0G3"
      },
      "source": [
        "print(\"Custom Model: Fine-tuning BERT on IMDB\")\n",
        "custom_history = custom_model.fit(train_ds, epochs=3, validation_data=valid_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IKNBxPTBJZC"
      },
      "source": [
        "Evaluating on the test set gives an accuracy of `88.18%`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ6fujuQBK8V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "219c0235-70b2-4114-b697-672e18fb1996"
      },
      "source": [
        "custom_model.evaluate(test_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1563/1563 [==============================] - 277s 177ms/step - loss: 0.3815 - accuracy: 0.8818\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.38145649433135986, 0.8817600011825562]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_nd7eWRBOtJ"
      },
      "source": [
        "If a lot of fine-tuning is done, then there is a risk of BERT\n",
        "forgetting its pretrained parameters. This can be a limitation while building custom models on top as a few epochs may not be sufficient to train the layers that have been added.\n",
        "\n",
        "In this case, the BERT model layer can be frozen, and training can be\n",
        "continued further. Freezing the BERT layer is fairly easy, though it needs the\n",
        "re-compilation of the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpeLmADBB2Fp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d807101-45d8-4baf-ef54-ae403677f629"
      },
      "source": [
        "bert.trainable = False                  # don't train BERT any more\n",
        "optimizer = tf.keras.optimizers.Adam()  # standard learning rate\n",
        "\n",
        "custom_model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
        "custom_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "attention_mask (InputLayer)     [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_ids (InputLayer)          [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "token_type_ids (InputLayer)     [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model_1 (TFBertModel)   TFBaseModelOutputWit 108310272   attention_mask[0][0]             \n",
            "                                                                 input_ids[0][0]                  \n",
            "                                                                 token_type_ids[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dropout_112 (Dropout)           (None, 768)          0           tf_bert_model_1[0][1]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 200)          153800      dropout_112[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_113 (Dropout)           (None, 200)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 2)            402         dropout_113[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 108,464,474\n",
            "Trainable params: 154,202\n",
            "Non-trainable params: 108,310,272\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DfgMLIdCUF8"
      },
      "source": [
        "We can see that all the BERT parameters are now set to non-trainable. Since the model was being recompiled, we also took the opportunity to change the learning rate.\n",
        "\n",
        "Now, training can be continued for a number of epochs like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIgCyRYlCeGz"
      },
      "source": [
        "print(\"Custom Model: Keep training custom model on IMDB\")\n",
        "custom_history = custom_model.fit(train_ds, epochs=10, validation_data=valid_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPKMuyfcD8m7"
      },
      "source": [
        "Checking the model on the test set yields 88.16% accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFmPY4HnD92Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dbc698c-8ab3-462a-a057-e597f0f8aa16"
      },
      "source": [
        "custom_model.evaluate(test_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1563/1563 [==============================] - 278s 178ms/step - loss: 0.4942 - accuracy: 0.8817\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4942091703414917, 0.8816800117492676]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-GVTW7BEJIu"
      },
      "source": [
        "If you are contemplating whether the accuracy of this custom model is lower than the pre-trained model, then it is a fair question to ponder over.\n",
        "\n",
        "**A bigger network is not always better, and overtraining can lead to a reduction in model performance due to overfitting.**\n",
        "\n",
        "Something to try in the custom model is to use the output encodings\n",
        "of all the input tokens and pass them through an LSTM layer or concatenate them\n",
        "together to pass through dense layers and then make the prediction."
      ]
    }
  ]
}