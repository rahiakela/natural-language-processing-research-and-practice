{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "2-text-vectorization.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "RS6j_Uu1aKr7",
        "Q5NI7lL_aPrR",
        "50W5rWfeOoR3"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/advanced-natural-language-processing-with-tensorflow-2/blob/main/1-essentials-of-nlp/2_text_vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdA7sBdGl2S2"
      },
      "source": [
        "## Text Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xk0ZWFhsJg7"
      },
      "source": [
        "To understand how to process text, it is important to understand the general\n",
        "workflow for NLP.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/advanced-nlp-with-tensorflow-2/text-processing-workflow.png?raw=1' width='800'/>\n",
        "\n",
        "The first two steps of the process in the preceding diagram involve collecting labeled data. A supervised model or even a semi-supervised model needs data to operate.\n",
        "\n",
        "The next step is usually normalizing and featurizing the data. Models have a hard time processing text data as is. There is a lot of hidden structure in a given text that needs to be processed and exposed. These two steps focus on that. \n",
        "\n",
        "There are a couple of challenges in using the text content of messages. **The first is that text can be of arbitrary lengths.**\n",
        "\n",
        "Comparing this to image data, we know that each image has a fixed width and height. Even if the corpus of images has a mixture of sizes, images\n",
        "can be resized to a common size with minimal loss of information by using a variety of compression mechanisms.\n",
        "\n",
        "In NLP, this is a bigger problem compared to computer vision. A common approach to handle this is to truncate the text.\n",
        "\n",
        "**The second issue is that of the representation of words with a numerical quantity or feature.**\n",
        "\n",
        "In computer vision, the smallest unit is a pixel. Each pixel has a set of\n",
        "numerical values indicating color or intensity. \n",
        "\n",
        "In a text, the smallest unit could be a word. Aggregating the Unicode values of the characters does not convey or embody the meaning of the word.\n",
        "\n",
        "**A core problem then is to construct a numerical representation of words. Vectorization is the process of converting a word to a vector of numbers that embodies the information contained in the word. Depending on the vectorization technique, this vector may have additional properties that may allow comparison with other words.**\n",
        "\n",
        "These are the followings text vectorization approach:\n",
        "\n",
        "- **Count-based vectorization**: The simplest approach for vectorizing is to use counts of words.\n",
        "- **TF-IDF based text vectorization**: This is more sophisticated, with its origins in information retrieval.\n",
        "- **Word2Vec based text vectorization**: it generate embeddings or word vectors.\n",
        "- **BERT based text vectorization**: The newest method in this area.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZPInUZktiyI"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy9eSdb1cn4Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1c2b5dec-bcbb-49f4-ffe9-460334ae66fa"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "\n",
        "import pandas as pd \n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import gensim.downloader as api\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOWZnCUXrhlM"
      },
      "source": [
        "# Basic 1-layer neural network model for evaluation\n",
        "def make_model(input_dims=3, num_units=12):\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  # Adds a densely-connected layer with 12 units to the model:\n",
        "  model.add(tf.keras.layers.Dense(num_units, \n",
        "                                  input_dim=input_dims, \n",
        "                                  activation='relu'))\n",
        "\n",
        "  # Add a sigmoid layer with a binary output unit:\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZ3vRSqLvTu6"
      },
      "source": [
        "## Data collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr1vzM27oyLD"
      },
      "source": [
        "**The first step of any Machine Learning (ML) project is to obtain a dataset.**\n",
        "\n",
        "We will be using the SMS Spam Collection dataset made available by University of California, Irvine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JSjikP_c0Ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0df1761-b442-4b09-b31f-5de0afa8ce00"
      },
      "source": [
        "# Download the zip file\n",
        "path_to_zip = tf.keras.utils.get_file(\"smsspamcollection.zip\",\n",
        "                  origin=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\",\n",
        "                  extract=True)\n",
        "\n",
        "# Unzip the file into a folder\n",
        "!unzip $path_to_zip -d data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
            "204800/203415 [==============================] - 0s 1us/step\n",
            "Archive:  /root/.keras/datasets/smsspamcollection.zip\n",
            "  inflating: data/SMSSpamCollection  \n",
            "  inflating: data/readme             \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytFnxCGFaJ20"
      },
      "source": [
        "# optional step - helps if colab gets disconnected\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbXeir-dm0nr"
      },
      "source": [
        "Reading the data file is trivial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Kb8AF3wHaKE3",
        "outputId": "21b0a587-8f4d-447a-dd9c-2a2f58029bf1"
      },
      "source": [
        "# Let's see if we read the data correctly\n",
        "# lines = io.open('/content/drive/My Drive/colab-data/SMSSpamCollection').read().strip().split('\\n')\n",
        "lines = io.open('/content/data/SMSSpamCollection').read().strip().split('\\n')\n",
        "lines[0]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "dYG5LFyDm-C-",
        "outputId": "8ead47c7-9a69-4a22-99fc-ba1125e946dd"
      },
      "source": [
        "lines[2]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"spam\\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCFBMGdWvnNn"
      },
      "source": [
        "### Pre-process Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zle10-0pnUjv"
      },
      "source": [
        "The next step is to split each line into two columns – one with the text of the message and the other as the label. While we are separating these labels, we will also convert the labels to numeric values. Since we are interested in predicting spam messages, we can assign a value of 1 to the spam\n",
        "messages. A value of 0 will be assigned to legitimate messages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhpfi9lWC5We",
        "outputId": "03a57529-724c-4d69-87ac-fafbc6b1f6a6"
      },
      "source": [
        "spam_dataset = []\n",
        "spam_count = 0\n",
        "ham_count = 0\n",
        "for line in lines:\n",
        "  label, text = line.split('\\t')\n",
        "  if label.lower().strip() == 'spam':\n",
        "    spam_dataset.append((1, text.strip()))\n",
        "    spam_count += 1\n",
        "  else:\n",
        "    spam_dataset.append(((0, text.strip())))\n",
        "    ham_count += 1\n",
        "\n",
        "spam_dataset[:5]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'),\n",
              " (0, 'Ok lar... Joking wif u oni...'),\n",
              " (1,\n",
              "  \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\"),\n",
              " (0, 'U dun say so early hor... U c already then say...'),\n",
              " (0, \"Nah I don't think he goes to usf, he lives around here though\")]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV1uWIEUoLWQ",
        "outputId": "99b397f8-eba2-4363-fa11-b4285a581e38"
      },
      "source": [
        "print(\"Spam: \", spam_count, \", Ham: \", ham_count)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spam:  747 , Ham:  4827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JReDkDrhoSWG"
      },
      "source": [
        "Now the dataset is ready for further processing in the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlaM80gFooWR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "9fcbff98-2473-4cca-eb0c-04d79ed1a1ef"
      },
      "source": [
        "# To do so, first, we will convert the data into a pandas DataFrame\n",
        "df = pd.DataFrame(spam_dataset, columns=['Spam', 'Message'])\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "      <th>Message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Spam                                            Message\n",
              "0     0  Go until jurong point, crazy.. Available only ...\n",
              "1     0                      Ok lar... Joking wif u oni...\n",
              "2     1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3     0  U dun say so early hor... U c already then say...\n",
              "4     0  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWcKp2AVoyNY"
      },
      "source": [
        "Now let's split the dataset into training and test sets, with 80% of the records in the training set and the rest in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUBRwsmUoy_m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "69d4ead9-a19b-487f-baa7-ccccec48d7ba"
      },
      "source": [
        "train=df.sample(frac=0.8,random_state=42) #random state is a seed value\n",
        "test=df.drop(train.index)\n",
        "\n",
        "train.describe()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4459.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.132765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.339359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Spam\n",
              "count  4459.000000\n",
              "mean      0.132765\n",
              "std       0.339359\n",
              "min       0.000000\n",
              "25%       0.000000\n",
              "50%       0.000000\n",
              "75%       0.000000\n",
              "max       1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpHpJ94DqkC-"
      },
      "source": [
        "y_train = train[['Spam']]\n",
        "y_test = test[['Spam']]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50W5rWfeOoR3"
      },
      "source": [
        "## Count-based vectorization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGPl4OiyoRzY"
      },
      "source": [
        "The idea behind count-based vectorization is really simple. Each unique word\n",
        "appearing in the corpus is assigned a column in the vocabulary. Each document,\n",
        "which would correspond to individual messages in the spam example, is assigned\n",
        "a row. The counts of the words appearing in that document are entered in\n",
        "the relevant cell corresponding to the document and the word. \n",
        "\n",
        "**With $n$ unique documents containing $m$ unique words, this results in a matrix of $n$ rows by $m$ columns.**\n",
        "\n",
        "Consider a corpus like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cco-INbyhouu"
      },
      "source": [
        "corpus = [\n",
        "  \"I like fruits. Fruits like bananas\",\n",
        "  \"I love bananas but eat an apple\",\n",
        "  \"An apple a day keeps the doctor away\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FoicaJMrMwV"
      },
      "source": [
        "There are three documents in this corpus of text. The scikit-learn (sklearn)\n",
        "library provides methods for undertaking count-based vectorization.\n",
        "\n",
        "The `CountVectorizer` class provides a built-in tokenizer that separates the tokens of two or more characters in length. This class takes a variety of options including a custom tokenizer, a stop word list, the option to convert characters to lowercase prior to tokenization, and a binary mode that converts every positive count to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clVOFDb1ERTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82a97f0c-3c01-4519-d61d-31106a7a4c85"
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "vectorizer.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['an',\n",
              " 'apple',\n",
              " 'away',\n",
              " 'bananas',\n",
              " 'but',\n",
              " 'day',\n",
              " 'doctor',\n",
              " 'eat',\n",
              " 'fruits',\n",
              " 'keeps',\n",
              " 'like',\n",
              " 'love',\n",
              " 'the']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B3znbFis2qw"
      },
      "source": [
        "The full matrix can be seen as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM7jlBZdFmiD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09e86138-af30-4ce0-9f83-f1b9d4b9c22a"
      },
      "source": [
        "X.toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 2, 0, 0],\n",
              "       [1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0],\n",
              "       [1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHrQlO9itGcl"
      },
      "source": [
        "This process has now converted a sentence such as \"I like fruits. Fruits like bananas\" into a vector `(0, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 0)`.\n",
        "\n",
        "**This is an example of context free vectorization. Context-free refers to the fact that the order of the words in the document did not make any difference in the generation of the vector. This is merely counting the instances of the words in a document.**\n",
        "\n",
        "Consequently, words with multiple meanings may be grouped into one, for example, bank. This may refer to a place near the river or a place to keep money. \n",
        "\n",
        "**However, it does provide a method to compare documents and derive similarity. The cosine similarity or distance can be computed between two documents, to see which documents are similar to which other documents**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pbT9sH-GwvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64df9835-17cd-4ce9-f3f9-128f4e60deee"
      },
      "source": [
        "cosine_similarity(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.13608276, 0.        ],\n",
              "       [0.13608276, 1.        , 0.3086067 ],\n",
              "       [0.        , 0.3086067 , 1.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q61lNJt0t_6v"
      },
      "source": [
        "This shows that the first sentence and the second sentence have a `0.136` similarity score (on a scale of 0 to 1). The first and third sentence have nothing in common. The second and third sentence have a similarity score of `0.308` – the highest in this set.\n",
        "\n",
        "**Another use case of this technique is to check the similarity of the documents\n",
        "with given keywords.**\n",
        "\n",
        "Let's say that the query is apple and bananas. This first step is\n",
        "to compute the vector of this query, and then compute the cosine similarity scores against the documents in the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe6qPCQ4Gxb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d2e94db-8c8a-4067-ed21-0a117127c5fa"
      },
      "source": [
        "query = vectorizer.transform([\"apple and bananas\"])\n",
        "\n",
        "cosine_similarity(X, query)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.23570226],\n",
              "       [0.57735027],\n",
              "       [0.26726124]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2A93J9gusmf"
      },
      "source": [
        "This shows that this query matches the second sentence in the corpus the best. The third sentence would rank second, and the first sentence would rank lowest.\n",
        "\n",
        "In a few lines, a basic search engine has been implemented, along with logic to serve queries!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w2zVnOnER_3"
      },
      "source": [
        "## TF-IDF Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec-hagd6kMX7"
      },
      "source": [
        "In creating a vector representation of the document, only the presence of words was included – it does not factor in the importance of a word. If the corpus of documents being processed is about a set of recipes with fruits, then one may expect words like apples, raspberries, and washing to appear frequently. \n",
        "\n",
        "**Term Frequency (TF) represents how often a word or token occurs in a given document.**\n",
        "\n",
        "In a set of documents about fruits and cooking, a word like apple may not be terribly specific to help identify a recipe. However, a word like tuile\n",
        "may be uncommon in that context. Therefore, it may help to narrow the search for recipes much faster than a word like raspberry. On a side note, feel free to search the web for raspberry tuile recipes. \n",
        "\n",
        "**If a word is rare, we want to give it a higher weight, as it may contain more information than a common word. A term can be upweighted\n",
        "by the inverse of the number of documents it appears in. Consequently, words that occur in a lot of documents will get a smaller score compared to terms that appear in fewer documents. This is called the Inverse Document Frequency (IDF).**\n",
        "\n",
        "Mathematically, the score of each term in a document can be computed as follows:\n",
        "\n",
        "$$ TF - IDF(t, d) = TF(t, d) * IDF(t) $$\n",
        "\n",
        "Here, t represents the word or term, and d represents a specific document.\n",
        "\n",
        "**It is common to normalize the TF of a term in a document by the total number of tokens in that document.**\n",
        "\n",
        "The IDF is defined as follows:\n",
        "\n",
        "$$ IDF(t) = log\\frac{N}{1 + n_t} $$\n",
        "\n",
        "Here, $N$ represents the total number of documents in the corpus, and $n_t$ represents the number of documents where the term is present. The addition of 1 in the denominator avoids the divide-by-zero error.\n",
        "\n",
        "Let's convert the counts from the previous section into their TF-IDF equivalents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKyNZgATHmhM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "outputId": "c2c106ad-c474-4c9b-e156-976e62714875"
      },
      "source": [
        "transformer = TfidfTransformer(smooth_idf=False)\n",
        "tfidf = transformer.fit_transform(X.toarray())\n",
        "\n",
        "pd.DataFrame(tfidf.toarray(), columns=vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>an</th>\n",
              "      <th>apple</th>\n",
              "      <th>away</th>\n",
              "      <th>bananas</th>\n",
              "      <th>but</th>\n",
              "      <th>day</th>\n",
              "      <th>doctor</th>\n",
              "      <th>eat</th>\n",
              "      <th>fruits</th>\n",
              "      <th>keeps</th>\n",
              "      <th>like</th>\n",
              "      <th>love</th>\n",
              "      <th>the</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.230408</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.688081</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.688081</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.321267</td>\n",
              "      <td>0.321267</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.321267</td>\n",
              "      <td>0.479709</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.479709</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.479709</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.275785</td>\n",
              "      <td>0.275785</td>\n",
              "      <td>0.411797</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.411797</td>\n",
              "      <td>0.411797</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.411797</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.411797</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         an     apple      away  ...      like      love       the\n",
              "0  0.000000  0.000000  0.000000  ...  0.688081  0.000000  0.000000\n",
              "1  0.321267  0.321267  0.000000  ...  0.000000  0.479709  0.000000\n",
              "2  0.275785  0.275785  0.411797  ...  0.000000  0.000000  0.411797\n",
              "\n",
              "[3 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXjwNTxtoBz8"
      },
      "source": [
        "This should give some intuition on how TF-IDF is computed. Even with three toy\n",
        "sentences and a very limited vocabulary, many of the columns in each row are 0.\n",
        "\n",
        "**This vectorization produces sparse representations.**\n",
        "\n",
        "\n",
        "Now, this can be applied to the problem of detecting spam messages. Thus far, the features for each message have been computed based on some aggregate statistics and added to the pandas DataFrame. Now, the content of the message will be tokenized and converted into a set of columns. The TF-IDF score for each word or token will be computed for each message in the array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rifoynAZvO9H"
      },
      "source": [
        "tfidf = TfidfVectorizer(binary=True)\n",
        "X = tfidf.fit_transform(train['Message']).astype('float32')\n",
        "X_test = tfidf.transform(test['Message']).astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDnG2OPHwD7U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f0335b-14d5-4934-9230-3ab91159fbb3"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4459, 7741)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXoRuIRqpIaR"
      },
      "source": [
        "The second parameter shows that 7,741 tokens were uniquely identified. These are the columns of features that will be used in the model later.\n",
        "\n",
        "Note that the vectorizer was created with the binary flag. This implies that even if a token appears multiple times in a message, it is counted as one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "862RegYipSH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a235ab-22fa-4cd4-ecb4-01c99c698b2b"
      },
      "source": [
        "X.toarray()[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ3sMm4ApwlO"
      },
      "source": [
        "### Modeling using TF-IDF features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiO1BqAmpyO9"
      },
      "source": [
        "The next trains the TF-IDF model on the training dataset. Then, it converts the words in the test set according to the TF-IDF scores learned from the training set. \n",
        "\n",
        "Let's train a model on just these TF-IDF features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGujuysLwKJe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7387e67-933d-4037-b553-a50f94c87bd6"
      },
      "source": [
        "_, cols = X.shape\n",
        "model2 = make_model(cols)  # to match tf-idf dimensions\n",
        "\n",
        "lb = LabelEncoder()\n",
        "y = lb.fit_transform(y_train)\n",
        "dummy_y_train = np_utils.to_categorical(y)\n",
        "model2.fit(X.toarray(), y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "446/446 [==============================] - 4s 2ms/step - loss: 0.4988 - accuracy: 0.8623\n",
            "Epoch 2/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.1223 - accuracy: 0.9670\n",
            "Epoch 3/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.0505 - accuracy: 0.9894\n",
            "Epoch 4/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.0285 - accuracy: 0.9946\n",
            "Epoch 5/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9968\n",
            "Epoch 6/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.0125 - accuracy: 0.9979\n",
            "Epoch 7/10\n",
            "446/446 [==============================] - 1s 3ms/step - loss: 0.0085 - accuracy: 0.9987\n",
            "Epoch 8/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9996\n",
            "Epoch 9/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.0044 - accuracy: 0.9992\n",
            "Epoch 10/10\n",
            "446/446 [==============================] - 1s 2ms/step - loss: 0.0029 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f90c05dc890>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfYJFcbIq3EY"
      },
      "source": [
        "Whoa – we are able to classify every one correctly! In all honesty, the model is probably overfitting, so some regularization should be applied. The test set gives this result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXYe6f8my21Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c37d477e-5f72-4bc5-8550-8a9f9e4ea2eb"
      },
      "source": [
        "model2.evaluate(X_test.toarray(), y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35/35 [==============================] - 0s 2ms/step - loss: 0.0548 - accuracy: 0.9848\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.054796118289232254, 0.9847533702850342]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYWzn0MqrD_S"
      },
      "source": [
        "An accuracy rate of 98.39% is by far the best we have gotten in any model so far. Checking the confusion matrix, it is evident that this model is indeed doing very well:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtRMpISqrI63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5e12acb-953f-43b2-e466-ccb0d549fdc9"
      },
      "source": [
        "y_test_pred = model2.predict_classes(X_test.toarray())\n",
        "tf.math.confusion_matrix(tf.constant(y_test.Spam), y_test_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
              "array([[958,   2],\n",
              "       [ 15, 140]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Va2geD7tETP"
      },
      "source": [
        "Only 2 regular messages were classified as spam, while only 15 spam messages\n",
        "were classified as being not spam. This is indeed a very good model.\n",
        "\n",
        "This model, without using a lot of pretraining and knowledge of\n",
        "language, vocabulary, and grammar, was able to do a very reasonable job with the task at hand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptf40YDjh71c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "e1bf96d7-a1ec-494a-934c-843ac9c3e225"
      },
      "source": [
        "train.loc[train.Spam == 1].describe() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>592.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Spam\n",
              "count  592.0\n",
              "mean     1.0\n",
              "std      0.0\n",
              "min      1.0\n",
              "25%      1.0\n",
              "50%      1.0\n",
              "75%      1.0\n",
              "max      1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiMYppLxtXSb"
      },
      "source": [
        "However, this model ignores the relationships between words completely. It treats the words in a document as unordered items in a set. There are better models that vectorize the tokens in a way that preserves some of the relationships between the tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMMB-9H3IEwm"
      },
      "source": [
        "# Word Vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vemTcz3xtL42"
      },
      "source": [
        "In NLP, a lot of research has been focused on learning the words or representations in an unsupervised way. This is called representation learning. The output of this approach is a representation of a word in some vector space, and the word can be considered embedded in that space. Consequently, these word vectors are also called embeddings.\n",
        "\n",
        "**The core hypothesis behind word vector algorithms is that words that occur near each other are related to each other.** \n",
        "\n",
        "To see the intuition behind this, consider two words, bake and oven. Given a sentence fragment of five words, where one of these words is present, what would be the probability of the other being present as well?\n",
        "You would be right in guessing that the probability is likely quite high. Suppose now that words are being mapped into some two-dimensional space. In that space, these two words should be closer to each other, and probably further away from words like astronomy and tractor.\n",
        "\n",
        "The task of learning these embeddings for the words can be then thought of as\n",
        "adjusting words in a giant multidimensional space where similar words are closer to each other and dissimilar words are further apart from each other.\n",
        "\n",
        "A revolutionary approach to do this is called Word2Vec.This approach\n",
        "produces dense vectors of the order of 50-300 dimensions generally (though larger are known), where most of the values are non-zero.\n",
        "\n",
        "The original paper had two algorithms proposed in it: **continuous bag-of-words and continuous skipgram.** On semantic tasks and overall, the performance of skip-gram was state of the art at the time of its publication. Consequently, the continuous skip-gram model with negative sampling has become synonymous with Word2Vec."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVQn6SDWznW3"
      },
      "source": [
        "### Pretrained models using Word2Vec embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YC7JteizojR"
      },
      "source": [
        "Since we are only interested in experimenting with a pretrained model, we can\n",
        "use the Gensim library and its pretrained embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hNrv_O2nPgD",
        "outputId": "da0f98f4-c9b1-487e-9752-5977b57f2b16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "api.info()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'corpora': {'20-newsgroups': {'checksum': 'c92fd4f6640a86d5ba89eaad818a9891',\n",
              "   'description': 'The notorious collection of approximately 20,000 newsgroup posts, partitioned (nearly) evenly across 20 different newsgroups.',\n",
              "   'fields': {'data': '',\n",
              "    'id': 'original id inferred from folder name',\n",
              "    'set': \"marker of original split (possible values 'train' and 'test')\",\n",
              "    'topic': 'name of topic (20 variant of possible values)'},\n",
              "   'file_name': '20-newsgroups.gz',\n",
              "   'file_size': 14483581,\n",
              "   'license': 'not found',\n",
              "   'num_records': 18846,\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://qwone.com/~jason/20Newsgroups/'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/20-newsgroups/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  '__testing_matrix-synopsis': {'checksum': '1767ac93a089b43899d54944b07d9dc5',\n",
              "   'description': '[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.',\n",
              "   'file_name': '__testing_matrix-synopsis.gz',\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis']},\n",
              "  '__testing_multipart-matrix-synopsis': {'checksum-0': 'c8b0c7d8cf562b1b632c262a173ac338',\n",
              "   'checksum-1': '5ff7fc6818e9a5d9bc1cf12c35ed8b96',\n",
              "   'checksum-2': '966db9d274d125beaac7987202076cba',\n",
              "   'description': '[THIS IS ONLY FOR TESTING] Synopsis of the movie matrix.',\n",
              "   'file_name': '__testing_multipart-matrix-synopsis.gz',\n",
              "   'parts': 3,\n",
              "   'read_more': ['http://www.imdb.com/title/tt0133093/plotsummary?ref_=ttpl_pl_syn#synopsis']},\n",
              "  'fake-news': {'checksum': '5e64e942df13219465927f92dcefd5fe',\n",
              "   'description': \"News dataset, contains text and metadata from 244 websites and represents 12,999 posts in total from a specific window of 30 days. The data was pulled using the webhose.io API, and because it's coming from their crawler, not all websites identified by their BS Detector are present in this dataset. Data sources that were missing a label were simply assigned a label of 'bs'. There are (ostensibly) no genuine, reliable, or trustworthy news sources represented in this dataset (so far), so don't trust anything you read.\",\n",
              "   'fields': {'author': 'author of story',\n",
              "    'comments': 'number of Facebook comments',\n",
              "    'country': 'data from webhose.io',\n",
              "    'crawled': 'date the story was archived',\n",
              "    'domain_rank': 'data from webhose.io',\n",
              "    'language': 'data from webhose.io',\n",
              "    'likes': 'number of Facebook likes',\n",
              "    'main_img_url': 'image from story',\n",
              "    'ord_in_thread': '',\n",
              "    'participants_count': 'number of participants',\n",
              "    'published': 'date published',\n",
              "    'replies_count': 'number of replies',\n",
              "    'shares': 'number of Facebook shares',\n",
              "    'site_url': 'site URL from BS detector',\n",
              "    'spam_score': 'data from webhose.io',\n",
              "    'text': 'text of story',\n",
              "    'thread_title': '',\n",
              "    'title': 'title of story',\n",
              "    'type': 'type of website (label from BS detector)',\n",
              "    'uuid': 'unique identifier'},\n",
              "   'file_name': 'fake-news.gz',\n",
              "   'file_size': 20102776,\n",
              "   'license': 'https://creativecommons.org/publicdomain/zero/1.0/',\n",
              "   'num_records': 12999,\n",
              "   'parts': 1,\n",
              "   'read_more': ['https://www.kaggle.com/mrisdal/fake-news'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fake-news/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  'patent-2017': {'checksum-0': '818501f0b9af62d3b88294d86d509f8f',\n",
              "   'checksum-1': '66c05635c1d3c7a19b4a335829d09ffa',\n",
              "   'description': \"Patent Grant Full Text. Contains the full text including tables, sequence data and 'in-line' mathematical expressions of each patent grant issued in 2017.\",\n",
              "   'file_name': 'patent-2017.gz',\n",
              "   'file_size': 3087262469,\n",
              "   'license': 'not found',\n",
              "   'num_records': 353197,\n",
              "   'parts': 2,\n",
              "   'read_more': ['http://patents.reedtech.com/pgrbft.php'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/patent-2017/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  'quora-duplicate-questions': {'checksum': 'd7cfa7fbc6e2ec71ab74c495586c6365',\n",
              "   'description': 'Over 400,000 lines of potential question duplicate pairs. Each line contains IDs for each question in the pair, the full text for each question, and a binary value that indicates whether the line contains a duplicate pair or not.',\n",
              "   'fields': {'id': 'the id of a training set question pair',\n",
              "    'is_duplicate': 'the target variable, set to 1 if question1 and question2 have essentially the same meaning, and 0 otherwise',\n",
              "    'qid1': 'unique ids of each question',\n",
              "    'qid2': 'unique ids of each question',\n",
              "    'question1': 'the full text of each question',\n",
              "    'question2': 'the full text of each question'},\n",
              "   'file_name': 'quora-duplicate-questions.gz',\n",
              "   'file_size': 21684784,\n",
              "   'license': 'probably https://www.quora.com/about/tos',\n",
              "   'num_records': 404290,\n",
              "   'parts': 1,\n",
              "   'read_more': ['https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/quora-duplicate-questions/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  'semeval-2016-2017-task3-subtaskA-unannotated': {'checksum': '2de0e2f2c4f91c66ae4fcf58d50ba816',\n",
              "   'description': 'SemEval 2016 / 2017 Task 3 Subtask A unannotated dataset contains 189,941 questions and 1,894,456 comments in English collected from the Community Question Answering (CQA) web forum of Qatar Living. These can be used as a corpus for language modelling.',\n",
              "   'fields': {'RelComments': [{'RELC_DATE': 'date of posting',\n",
              "      'RELC_ID': 'comment identifier',\n",
              "      'RELC_USERID': 'identifier of the user posting the comment',\n",
              "      'RELC_USERNAME': 'name of the user posting the comment',\n",
              "      'RelCText': 'text of answer'}],\n",
              "    'RelQuestion': {'RELQ_CATEGORY': 'question category, according to the Qatar Living taxonomy',\n",
              "     'RELQ_DATE': 'date of posting',\n",
              "     'RELQ_ID': 'question indentifier',\n",
              "     'RELQ_USERID': 'identifier of the user asking the question',\n",
              "     'RELQ_USERNAME': 'name of the user asking the question',\n",
              "     'RelQBody': 'body of question',\n",
              "     'RelQSubject': 'subject of question'},\n",
              "    'THREAD_SEQUENCE': ''},\n",
              "   'file_name': 'semeval-2016-2017-task3-subtaskA-unannotated.gz',\n",
              "   'file_size': 234373151,\n",
              "   'license': 'These datasets are free for general research use.',\n",
              "   'num_records': 189941,\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://alt.qcri.org/semeval2016/task3/',\n",
              "    'http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf',\n",
              "    'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
              "    'https://github.com/Witiko/semeval-2016_2017-task3-subtaskA-unannotated-english'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskA-unannotated-eng/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  'semeval-2016-2017-task3-subtaskBC': {'checksum': '701ea67acd82e75f95e1d8e62fb0ad29',\n",
              "   'description': 'SemEval 2016 / 2017 Task 3 Subtask B and C datasets contain train+development (317 original questions, 3,169 related questions, and 31,690 comments), and test datasets in English. The description of the tasks and the collected data is given in sections 3 and 4.1 of the task paper http://alt.qcri.org/semeval2016/task3/data/uploads/semeval2016-task3-report.pdf linked in section “Papers” of https://github.com/RaRe-Technologies/gensim-data/issues/18.',\n",
              "   'fields': {'2016-dev': ['...'],\n",
              "    '2016-test': ['...'],\n",
              "    '2016-train': ['...'],\n",
              "    '2017-test': ['...']},\n",
              "   'file_name': 'semeval-2016-2017-task3-subtaskBC.gz',\n",
              "   'file_size': 6344358,\n",
              "   'license': 'All files released for the task are free for general research use',\n",
              "   'num_records': -1,\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://alt.qcri.org/semeval2017/task3/',\n",
              "    'http://alt.qcri.org/semeval2017/task3/data/uploads/semeval2017-task3.pdf',\n",
              "    'https://github.com/RaRe-Technologies/gensim-data/issues/18',\n",
              "    'https://github.com/Witiko/semeval-2016_2017-task3-subtaskB-english'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/semeval-2016-2017-task3-subtaskB-eng/__init__.py',\n",
              "   'record_format': 'dict'},\n",
              "  'text8': {'checksum': '68799af40b6bda07dfa47a32612e5364',\n",
              "   'description': 'First 100,000,000 bytes of plain text from Wikipedia. Used for testing purposes; see wiki-english-* for proper full Wikipedia datasets.',\n",
              "   'file_name': 'text8.gz',\n",
              "   'file_size': 33182058,\n",
              "   'license': 'not found',\n",
              "   'num_records': 1701,\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://mattmahoney.net/dc/textdata.html'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/text8/__init__.py',\n",
              "   'record_format': 'list of str (tokens)'},\n",
              "  'wiki-english-20171001': {'checksum-0': 'a7d7d7fd41ea7e2d7fa32ec1bb640d71',\n",
              "   'checksum-1': 'b2683e3356ffbca3b6c2dca6e9801f9f',\n",
              "   'checksum-2': 'c5cde2a9ae77b3c4ebce804f6df542c2',\n",
              "   'checksum-3': '00b71144ed5e3aeeb885de84f7452b81',\n",
              "   'description': 'Extracted Wikipedia dump from October 2017. Produced by `python -m gensim.scripts.segment_wiki -f enwiki-20171001-pages-articles.xml.bz2 -o wiki-en.gz`',\n",
              "   'fields': {'section_texts': 'list of body of sections',\n",
              "    'section_titles': 'list of titles of sections',\n",
              "    'title': 'Title of wiki article'},\n",
              "   'file_name': 'wiki-english-20171001.gz',\n",
              "   'file_size': 6516051717,\n",
              "   'license': 'https://dumps.wikimedia.org/legal.html',\n",
              "   'num_records': 4924894,\n",
              "   'parts': 4,\n",
              "   'read_more': ['https://dumps.wikimedia.org/enwiki/20171001/'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/wiki-english-20171001/__init__.py',\n",
              "   'record_format': 'dict'}},\n",
              " 'models': {'__testing_word2vec-matrix-synopsis': {'checksum': '534dcb8b56a360977a269b7bfc62d124',\n",
              "   'description': '[THIS IS ONLY FOR TESTING] Word vecrors of the movie matrix.',\n",
              "   'file_name': '__testing_word2vec-matrix-synopsis.gz',\n",
              "   'parameters': {'dimensions': 50},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v using a preprocessed corpus. Converted to w2v format with `python3.5 -m gensim.models.word2vec -train <input_filename> -iter 50 -output <output_filename>`.',\n",
              "   'read_more': []},\n",
              "  'conceptnet-numberbatch-17-06-300': {'base_dataset': 'ConceptNet, word2vec, GloVe, and OpenSubtitles 2016',\n",
              "   'checksum': 'fd642d457adcd0ea94da0cd21b150847',\n",
              "   'description': 'ConceptNet Numberbatch consists of state-of-the-art semantic vectors (also known as word embeddings) that can be used directly as a representation of word meanings or as a starting point for further machine learning. ConceptNet Numberbatch is part of the ConceptNet open data project. ConceptNet provides lots of ways to compute with word meanings, one of which is word embeddings. ConceptNet Numberbatch is a snapshot of just the word embeddings. It is built using an ensemble that combines data from ConceptNet, word2vec, GloVe, and OpenSubtitles 2016, using a variation on retrofitting.',\n",
              "   'file_name': 'conceptnet-numberbatch-17-06-300.gz',\n",
              "   'file_size': 1225497562,\n",
              "   'license': 'https://github.com/commonsense/conceptnet-numberbatch/blob/master/LICENSE.txt',\n",
              "   'num_records': 1917247,\n",
              "   'parameters': {'dimension': 300},\n",
              "   'parts': 1,\n",
              "   'read_more': ['http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972',\n",
              "    'https://github.com/commonsense/conceptnet-numberbatch',\n",
              "    'http://conceptnet.io/'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/conceptnet-numberbatch-17-06-300/__init__.py'},\n",
              "  'fasttext-wiki-news-subwords-300': {'base_dataset': 'Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens)',\n",
              "   'checksum': 'de2bb3a20c46ce65c9c131e1ad9a77af',\n",
              "   'description': '1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).',\n",
              "   'file_name': 'fasttext-wiki-news-subwords-300.gz',\n",
              "   'file_size': 1005007116,\n",
              "   'license': 'https://creativecommons.org/licenses/by-sa/3.0/',\n",
              "   'num_records': 999999,\n",
              "   'parameters': {'dimension': 300},\n",
              "   'parts': 1,\n",
              "   'read_more': ['https://fasttext.cc/docs/en/english-vectors.html',\n",
              "    'https://arxiv.org/abs/1712.09405',\n",
              "    'https://arxiv.org/abs/1607.01759'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/fasttext-wiki-news-subwords-300/__init__.py'},\n",
              "  'glove-twitter-100': {'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'checksum': 'b04f7bed38756d64cf55b58ce7e97b15',\n",
              "   'description': 'Pre-trained vectors based on  2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
              "   'file_name': 'glove-twitter-100.gz',\n",
              "   'file_size': 405932991,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 1193514,\n",
              "   'parameters': {'dimension': 100},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-100.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-100/__init__.py'},\n",
              "  'glove-twitter-200': {'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'checksum': 'e52e8392d1860b95d5308a525817d8f9',\n",
              "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-twitter-200.gz',\n",
              "   'file_size': 795373100,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 1193514,\n",
              "   'parameters': {'dimension': 200},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-200.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-200/__init__.py'},\n",
              "  'glove-twitter-25': {'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'checksum': '50db0211d7e7a2dcd362c6b774762793',\n",
              "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-twitter-25.gz',\n",
              "   'file_size': 109885004,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 1193514,\n",
              "   'parameters': {'dimension': 25},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-25/__init__.py'},\n",
              "  'glove-twitter-50': {'base_dataset': 'Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased)',\n",
              "   'checksum': 'c168f18641f8c8a00fe30984c4799b2b',\n",
              "   'description': 'Pre-trained vectors based on 2B tweets, 27B tokens, 1.2M vocab, uncased (https://nlp.stanford.edu/projects/glove/)',\n",
              "   'file_name': 'glove-twitter-50.gz',\n",
              "   'file_size': 209216938,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 1193514,\n",
              "   'parameters': {'dimension': 50},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-50.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-twitter-50/__init__.py'},\n",
              "  'glove-wiki-gigaword-100': {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'checksum': '40ec481866001177b8cd4cb0df92924f',\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-wiki-gigaword-100.gz',\n",
              "   'file_size': 134300434,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 400000,\n",
              "   'parameters': {'dimension': 100},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-100.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-100/__init__.py'},\n",
              "  'glove-wiki-gigaword-200': {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'checksum': '59652db361b7a87ee73834a6c391dfc1',\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-wiki-gigaword-200.gz',\n",
              "   'file_size': 264336934,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 400000,\n",
              "   'parameters': {'dimension': 200},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-200.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-200/__init__.py'},\n",
              "  'glove-wiki-gigaword-300': {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'checksum': '29e9329ac2241937d55b852e8284e89b',\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-wiki-gigaword-300.gz',\n",
              "   'file_size': 394362229,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 400000,\n",
              "   'parameters': {'dimension': 300},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-300.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-300/__init__.py'},\n",
              "  'glove-wiki-gigaword-50': {'base_dataset': 'Wikipedia 2014 + Gigaword 5 (6B tokens, uncased)',\n",
              "   'checksum': 'c289bc5d7f2f02c6dc9f2f9b67641813',\n",
              "   'description': 'Pre-trained vectors based on Wikipedia 2014 + Gigaword, 5.6B tokens, 400K vocab, uncased (https://nlp.stanford.edu/projects/glove/).',\n",
              "   'file_name': 'glove-wiki-gigaword-50.gz',\n",
              "   'file_size': 69182535,\n",
              "   'license': 'http://opendatacommons.org/licenses/pddl/',\n",
              "   'num_records': 400000,\n",
              "   'parameters': {'dimension': 50},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt`.',\n",
              "   'read_more': ['https://nlp.stanford.edu/projects/glove/',\n",
              "    'https://nlp.stanford.edu/pubs/glove.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/glove-wiki-gigaword-50/__init__.py'},\n",
              "  'word2vec-google-news-300': {'base_dataset': 'Google News (about 100 billion words)',\n",
              "   'checksum': 'a5e5354d40acb95f9ec66d5977d140ef',\n",
              "   'description': \"Pre-trained vectors trained on a part of the Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. The phrases were obtained using a simple data-driven approach described in 'Distributed Representations of Words and Phrases and their Compositionality' (https://code.google.com/archive/p/word2vec/).\",\n",
              "   'file_name': 'word2vec-google-news-300.gz',\n",
              "   'file_size': 1743563840,\n",
              "   'license': 'not found',\n",
              "   'num_records': 3000000,\n",
              "   'parameters': {'dimension': 300},\n",
              "   'parts': 1,\n",
              "   'read_more': ['https://code.google.com/archive/p/word2vec/',\n",
              "    'https://arxiv.org/abs/1301.3781',\n",
              "    'https://arxiv.org/abs/1310.4546',\n",
              "    'https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/?from=http%3A%2F%2Fresearch.microsoft.com%2Fpubs%2F189726%2Frvecs.pdf'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-google-news-300/__init__.py'},\n",
              "  'word2vec-ruscorpora-300': {'base_dataset': 'Russian National Corpus (about 250M words)',\n",
              "   'checksum': '9bdebdc8ae6d17d20839dd9b5af10bc4',\n",
              "   'description': 'Word2vec Continuous Skipgram vectors trained on full Russian National Corpus (about 250M words). The model contains 185K words.',\n",
              "   'file_name': 'word2vec-ruscorpora-300.gz',\n",
              "   'file_size': 208427381,\n",
              "   'license': 'https://creativecommons.org/licenses/by/4.0/deed.en',\n",
              "   'num_records': 184973,\n",
              "   'parameters': {'dimension': 300, 'window_size': 10},\n",
              "   'parts': 1,\n",
              "   'preprocessing': 'The corpus was lemmatized and tagged with Universal PoS',\n",
              "   'read_more': ['https://www.academia.edu/24306935/WebVectors_a_Toolkit_for_Building_Web_Interfaces_for_Vector_Semantic_Models',\n",
              "    'http://rusvectores.org/en/',\n",
              "    'https://github.com/RaRe-Technologies/gensim-data/issues/3'],\n",
              "   'reader_code': 'https://github.com/RaRe-Technologies/gensim-data/releases/download/word2vec-ruscorpora-300/__init__.py'}}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPiod_jN0DKm"
      },
      "source": [
        "Note that these particular embeddings are approximately 1.6 GB in size, so may take a very long time to load."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eytG4hu4nPSB",
        "outputId": "b4b1ddb3-b762-42f0-da4b-38518916ec95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_w2v = api.load(\"word2vec-google-news-300\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[=================================================-] 99.0% 1645.6/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyKV7anJ0RmY"
      },
      "source": [
        "Now, we are ready to inspect the similar words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0lxhBMeIYrl",
        "outputId": "c563dac0-29dd-40ba-bd64-2887a8a46beb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_w2v.most_similar(\"cookies\",topn=10)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('cookie', 0.745154082775116),\n",
              " ('oatmeal_raisin_cookies', 0.6887780427932739),\n",
              " ('oatmeal_cookies', 0.662139892578125),\n",
              " ('cookie_dough_ice_cream', 0.6520504951477051),\n",
              " ('brownies', 0.6479344964027405),\n",
              " ('homemade_cookies', 0.6476464867591858),\n",
              " ('gingerbread_cookies', 0.6461867690086365),\n",
              " ('Cookies', 0.6341644525527954),\n",
              " ('cookies_cupcakes', 0.6275068521499634),\n",
              " ('cupcakes', 0.6258294582366943)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laxFKApf0bvf"
      },
      "source": [
        "This is pretty good. Let's see how this model does at a word analogy task:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkfcI1niIYQ4",
        "outputId": "c5f48c5c-9324-40d8-cdad-2fa2b4e1a90f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "model_w2v.doesnt_match([\"USA\",\"Canada\",\"India\",\"Tokyo\"])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tokyo'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr9ZRiId0mXp"
      },
      "source": [
        "The model is able to guess that compared to the other words, which are all countries, Tokyo is the odd one out, as it is a city.\n",
        "\n",
        "Now, let's try a very famous example of mathematics on these word vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ0lCjnaImWZ",
        "outputId": "d6305b59-eca7-4f67-9d9f-3705621da3fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "king = model_w2v['king']\n",
        "man = model_w2v['man']\n",
        "woman = model_w2v['woman']\n",
        "\n",
        "queen = king - man + woman  \n",
        "model_w2v.similar_by_vector(queen)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('king', 0.8449392318725586),\n",
              " ('queen', 0.7300517559051514),\n",
              " ('monarch', 0.6454660892486572),\n",
              " ('princess', 0.6156251430511475),\n",
              " ('crown_prince', 0.5818676948547363),\n",
              " ('prince', 0.5777117609977722),\n",
              " ('kings', 0.5613663792610168),\n",
              " ('sultan', 0.5376776456832886),\n",
              " ('Queen_Consort', 0.5344247817993164),\n",
              " ('queens', 0.5289887189865112)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUuHXlqJ1xpu"
      },
      "source": [
        "Given that King was provided as an input to the equation, it is simple to filter the inputs from the outputs and Queen would be the top result.\n",
        "\n",
        "A pretrained model like the preceding can be used to vectorize a document. Using these embeddings, models can be trained for specific purposes."
      ]
    }
  ]
}