{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "tf23nlp",
      "language": "python",
      "name": "tf23nlp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "sentiment-analysis.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/advanced-natural-language-processing-with-tensorflow-2/blob/main/2-understanding-sentiment-in-natural-language/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Scx70575uMsZ"
      },
      "source": [
        "## Sentiment classification with LSTMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sai4exy_uOLe"
      },
      "source": [
        "Sentiment classification is an oft-cited use case of NLP. Models that predict the movement of stock prices by using sentiment analysis features from tweets have shown promising results. Tweet sentiment is also used to determine customers' perceptions of brands. Another use case is processing user reviews for movies, or products on e-commerce or other websites. To see LSTMs in action, let's use a dataset of movie reviews from IMDb."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbReVxkSuY6D"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bw9_EqwuZ5c"
      },
      "source": [
        "# !pip install tensorflow_datasets\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcGeFJj0tcAp"
      },
      "source": [
        "**Only if you have a GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:03:30.971832Z",
          "start_time": "2020-10-01T07:03:30.919332Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE9orZ-XtcAp",
        "outputId": "9513463b-c1ca-4272-c5ed-4563226e19c3"
      },
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:04:17.819473Z",
          "start_time": "2020-10-01T07:04:16.967326Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XOEbpCgtcAr",
        "outputId": "69b169cf-e7ef-4164-d4b5-10944db438dc"
      },
      "source": [
        "######## GPU CONFIGS FOR RTX 2070 ###############\n",
        "## Please ignore if not training on GPU       ##\n",
        "## this is important for running CuDNN on GPU ##\n",
        "\n",
        "# check if GPU can be seen by TF\n",
        "tf.config.list_physical_devices('GPU')\n",
        "# enable this line if you wish to see whether GPU/CPU executes\n",
        "# each command\n",
        "#tf.debugging.set_log_device_placement(True)\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Restrict TensorFlow to only use the first GPU\n",
        "  try:\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
        "  except RuntimeError as e:\n",
        "    # Visible devices must be set before GPUs have been initialized\n",
        "    print(e)\n",
        "###############################################"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 Physical GPUs, 1 Logical GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAOHJHkOtcAr"
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MSWwnKowbrp"
      },
      "source": [
        "When we load data using Pandas, it load the entire dataset into memory.However, sometimes data can be quite large, or spread into multiple files. In such cases, it may be too large for loading and need lots of pre-processing. Making text data ready to be used in a model requires normalization and vectorization at the very least. Often, this needs to be done outside of the TensorFlow graph using Python functions. Further, it creates issues for data pipelines in production where there is a higher chance of breakage as different dependent stages are being executed separately.\n",
        "\n",
        "TensorFlow provides a solution for the loading, transformation, and batching\n",
        "of data through the use of the tf.data package. In addition, a number of datasets are provided for download through the tensorflow_datasets package. We will use a combination of these to download the IMDb data, and perform the tokenization, encoding, and vectorization steps before training an LSTM model.\n",
        "\n",
        "The `tfds` package comes with a number of datasets in different domains such as images, audio, video, text, summarization, and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:04:34.432270Z",
          "start_time": "2020-10-01T07:04:34.425717Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "2jEYqjcNtcAr",
        "outputId": "2d31d0c6-cbe1-4e9b-c816-13c03962011b"
      },
      "source": [
        "# see available tfds data sets\n",
        "\", \".join(tfds.list_builders())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'abstract_reasoning, accentdb, aeslc, aflw2k3d, ag_news_subset, ai2_arc, ai2_arc_with_ir, amazon_us_reviews, anli, arc, bair_robot_pushing_small, bccd, beans, big_patent, bigearthnet, billsum, binarized_mnist, binary_alpha_digits, blimp, bool_q, c4, caltech101, caltech_birds2010, caltech_birds2011, cars196, cassava, cats_vs_dogs, celeb_a, celeb_a_hq, cfq, chexpert, cifar10, cifar100, cifar10_1, cifar10_corrupted, citrus_leaves, cityscapes, civil_comments, clevr, clic, clinc_oos, cmaterdb, cnn_dailymail, coco, coco_captions, coil100, colorectal_histology, colorectal_histology_large, common_voice, coqa, cos_e, cosmos_qa, covid19sum, crema_d, curated_breast_imaging_ddsm, cycle_gan, deep_weeds, definite_pronoun_resolution, dementiabank, diabetic_retinopathy_detection, div2k, dmlab, downsampled_imagenet, dsprites, dtd, duke_ultrasound, emnist, eraser_multi_rc, esnli, eurosat, fashion_mnist, flic, flores, food101, forest_fires, fuss, gap, geirhos_conflict_stimuli, genomics_ood, german_credit_numeric, gigaword, glue, goemotions, gpt3, groove, gtzan, gtzan_music_speech, hellaswag, higgs, horses_or_humans, i_naturalist2017, imagenet2012, imagenet2012_corrupted, imagenet2012_real, imagenet2012_subset, imagenet_a, imagenet_r, imagenet_resized, imagenet_v2, imagenette, imagewang, imdb_reviews, irc_disentanglement, iris, kitti, kmnist, lfw, librispeech, librispeech_lm, libritts, ljspeech, lm1b, lost_and_found, lsun, malaria, math_dataset, mctaco, mnist, mnist_corrupted, movie_lens, movie_rationales, movielens, moving_mnist, multi_news, multi_nli, multi_nli_mismatch, natural_questions, natural_questions_open, newsroom, nsynth, nyu_depth_v2, omniglot, open_images_challenge2019_detection, open_images_v4, openbookqa, opinion_abstracts, opinosis, opus, oxford_flowers102, oxford_iiit_pet, para_crawl, patch_camelyon, paws_wiki, paws_x_wiki, pet_finder, pg19, places365_small, plant_leaves, plant_village, plantae_k, qa4mre, qasc, quickdraw_bitmap, radon, reddit, reddit_disentanglement, reddit_tifu, resisc45, robonet, rock_paper_scissors, rock_you, salient_span_wikipedia, samsum, savee, scan, scene_parse150, scicite, scientific_papers, sentiment140, shapes3d, smallnorb, snli, so2sat, speech_commands, spoken_digit, squad, stanford_dogs, stanford_online_products, starcraft_video, stl10, sun397, super_glue, svhn_cropped, ted_hrlr_translate, ted_multi_translate, tedlium, tf_flowers, the300w_lp, tiny_shakespeare, titanic, trec, trivia_qa, tydi_qa, uc_merced, ucf101, vctk, vgg_face2, visual_domain_decathlon, voc, voxceleb, voxforge, waymo_open_dataset, web_questions, wider_face, wiki40b, wikihow, wikipedia, wikipedia_toxicity_subtypes, wine_quality, winogrande, wmt14_translate, wmt15_translate, wmt16_translate, wmt17_translate, wmt18_translate, wmt19_translate, wmt_t2t_translate, wmt_translate, wordnet, xnli, xquad, xsum, yelp_polarity_reviews, yes_no'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3886WT3ybxl"
      },
      "source": [
        "IMDb data is provided in three splits – training, test, and unsupervised. The training and testing splits have 25,000 rows each, with two columns. The first column is the text of the review, and the second is the label. \"0\" represents a review with negative sentiment while \"1\" represents a review with positive sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:04:37.653833Z",
          "start_time": "2020-10-01T07:04:37.554098Z"
        },
        "id": "ZbkDhDG6tcAs"
      },
      "source": [
        "# using TFDS dataset\n",
        "# note: as_supervised converts dicts to tuples\n",
        "imdb_train, ds_info = tfds.load(name=\"imdb_reviews\", split=\"train\", with_info=True, as_supervised=True)\n",
        "imdb_test = tfds.load(name=\"imdb_reviews\", split=\"test\", as_supervised=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:04:40.136066Z",
          "start_time": "2020-10-01T07:04:40.130384Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXhIBIN8tcAs",
        "outputId": "eaeefbcd-4312-4cce-fe64-c17e422f77fb"
      },
      "source": [
        "print(ds_info)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='imdb_reviews',\n",
            "    version=1.0.0,\n",
            "    description='Large Movie Review Dataset.\n",
            "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
            "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
            "    features=FeaturesDict({\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
            "        'text': Text(shape=(), dtype=tf.string),\n",
            "    }),\n",
            "    total_num_examples=100000,\n",
            "    splits={\n",
            "        'test': 25000,\n",
            "        'train': 25000,\n",
            "        'unsupervised': 50000,\n",
            "    },\n",
            "    supervised_keys=('text', 'label'),\n",
            "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
            "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
            "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
            "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
            "      month     = {June},\n",
            "      year      = {2011},\n",
            "      address   = {Portland, Oregon, USA},\n",
            "      publisher = {Association for Computational Linguistics},\n",
            "      pages     = {142--150},\n",
            "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
            "    }\"\"\",\n",
            "    redistribution_info=,\n",
            ")\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9WaY6hVzprr"
      },
      "source": [
        "We can see that two keys, text and label, are available in the supervised mode. Using the as_supervised parameter is key to loading the dataset as a tuple of values. If this parameter is not specified, data is loaded and made available as dictionary keys. In cases where the data has multiple inputs, that may be preferable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:04:43.390551Z",
          "start_time": "2020-10-01T07:04:43.326285Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_O53e5qtcAt",
        "outputId": "7de5153e-8432-4821-9c4a-df150cb964a5"
      },
      "source": [
        "for example, label in imdb_train.take(1):\n",
        "    print(example, '\\n', label)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string) \n",
            " tf.Tensor(0, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DunEceIcz2IN"
      },
      "source": [
        "## Normalization and vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdKLs4Hrz3BQ"
      },
      "source": [
        "Here, we are only going to tokenize the text into words and construct a\n",
        "vocabulary, and then encode the words using this vocabulary. This is a simplified approach. There can be a number of different approaches that can be used for building additional features.\n",
        "\n",
        "A vocabulary of the tokens occurring in the data needs to be constructed prior to vectorization. Tokenization breaks up the words in the text into individual tokens. The set of all the tokens forms the vocabulary.\n",
        "\n",
        "Normalization of the text, such as converting to lowercase, etc., is performed along with this tokenization step. `tfds` comes with a set of feature builders for text in the `tfds.features.text` package.\n",
        "\n",
        "First, a set of all the words in the training data needs to be created:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:04:54.364677Z",
          "start_time": "2020-10-01T07:04:47.490172Z"
        },
        "id": "tinBnMK3tcAt"
      },
      "source": [
        "# Use the default tokenizer settings\n",
        "tokenizer = tfds.deprecated.text.Tokenizer()\n",
        "\n",
        "vocabulary_set = set()\n",
        "MAX_TOKENS = 0\n",
        "\n",
        "for example, label in imdb_train:\n",
        "  some_tokens = tokenizer.tokenize(example.numpy())\n",
        "  if MAX_TOKENS < len(some_tokens):\n",
        "        MAX_TOKENS = len(some_tokens)\n",
        "  vocabulary_set.update(some_tokens)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS_dB8qi6Jeo"
      },
      "source": [
        "Using this vocabulary, an encoder can be created. TokenTextEncoder is one of three out-of-the-box encoders that are provided in tfds. Note how the list of tokens is converted into a set to ensure only unique tokens are retained in the vocabulary. The tokenizer used for generating the vocabulary is passed in, so that every successive call to encode a string can use the same tokenization scheme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:05:55.711915Z",
          "start_time": "2020-10-01T07:05:55.538523Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKvxTHKCtcAt",
        "outputId": "86eacf89-6394-4145-bef3-cbe79eafc734"
      },
      "source": [
        "imdb_encoder = tfds.deprecated.text.TokenTextEncoder(vocabulary_set, tokenizer=tokenizer)\n",
        "vocab_size = imdb_encoder.vocab_size\n",
        "\n",
        "print(vocab_size, MAX_TOKENS)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "93931 2525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9P2YAZ86yNA"
      },
      "source": [
        "The vocabulary has 93,931 tokens. The longest review has 2,525 tokens. That is one wordy review! Reviews are going to have different lengths.\n",
        "\n",
        "LSTMs expect sequences of equal length. Padding and truncating operations make reviews of equal length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:05:57.667656Z",
          "start_time": "2020-10-01T07:05:57.624948Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP5LBfQ8tcAu",
        "outputId": "47d92b86-01ec-4189-ecb0-016ed39b3738"
      },
      "source": [
        "# Lets verify tokenization and encoding works\n",
        "for example, label in imdb_train.take(1):\n",
        "    print(f\"Encoded: \\n\", example)\n",
        "    encoded = imdb_encoder.encode(example.numpy())\n",
        "    print(\"Decoded: \\n\", imdb_encoder.decode(encoded))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoded: \n",
            " tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n",
            "Decoded: \n",
            " This was an absolutely terrible movie Don t be lured in by Christopher Walken or Michael Ironside Both are great actors but this must simply be their worst role in history Even their great acting could not redeem this movie s ridiculous storyline This movie is an early nineties US propaganda piece The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions Maria Conchita Alonso appeared phony and her pseudo love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning I am disappointed that there are movies like this ruining actor s like Christopher Walken s good name I could barely sit through it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-eZ_53F85sl"
      },
      "source": [
        "Tokenization and encoding were done for a small set of rows at a time. TensorFlow provides mechanisms to perform these actions in bulk over large datasets, which can be shuffled and loaded in batches. This allows very large datasets to be loaded without running out of memory during training. To enable this, a function needs to be defined that performs a transformation on a row of data. Note that multiple transformations can be chained one after the other. It is also possible to use a Python function in defining these transformations. \n",
        "\n",
        "For processing the review above, the following steps need to be performed:\n",
        "- **Tokenization**: Reviews need to be tokenized into words.\n",
        "- **Encoding**: These words need to be mapped to integers using the vocabulary.\n",
        "- **Padding**: Reviews can have variable lengths, but LSTMs expect vectors of\n",
        "the same length. So, a constant length is chosen. Reviews shorter than this\n",
        "length are padded with a specific vocabulary index, usually 0 in TensorFlow.\n",
        "Reviews longer than this length are truncated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:05:59.963100Z",
          "start_time": "2020-10-01T07:05:59.954134Z"
        },
        "id": "BSITHfU8tcAu"
      },
      "source": [
        "# transformation fucntions to be used with the dataset\n",
        "def encode_pad_transform(sample):\n",
        "    # encoding, padding and truncating\n",
        "    encoded = imdb_encoder.encode(sample.numpy())\n",
        "    pad = sequence.pad_sequences([encoded], padding='post', maxlen=150)\n",
        "\n",
        "    return np.array(pad[0], dtype=np.int64)  \n",
        "\n",
        "\n",
        "def encode_tf_fn(sample, label):\n",
        "    encoded = tf.py_function(encode_pad_transform, inp=[sample], Tout=(tf.int64))\n",
        "    encoded.set_shape([None])\n",
        "    label.set_shape([])\n",
        "\n",
        "    return encoded, label"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smm75gUNBFHb"
      },
      "source": [
        "Transforming the data is quite trivial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:06:00.776090Z",
          "start_time": "2020-10-01T07:06:00.703019Z"
        },
        "id": "ItZP-gmVtcAu"
      },
      "source": [
        "# test the transformation on a small subset\n",
        "subset = imdb_train.take(10)\n",
        "tst = subset.map(encode_tf_fn)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:06:01.845914Z",
          "start_time": "2020-10-01T07:06:01.641109Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnGw5ELEtcAv",
        "outputId": "bf801e9f-6526-47cf-8901-c79ed55e9cf6"
      },
      "source": [
        "for review, label in tst.take(1):\n",
        "    print(review, label)\n",
        "    print(imdb_encoder.decode(review))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[10791 10817 40367 83856 44059 66762 60896 76281 86381 52497 75002 88045\n",
            " 60004 91183 28061 15113 23589 45274 29445 87375 16264 77090 81275 21491\n",
            " 91250 86381   596 48122 54858 75002 58898 65100   596 87375 21198 56449\n",
            "  6219 63042 81275 66762 27491  9280 22022 10791 66762 70078 40367 24997\n",
            "  9196 38223 84205  2447 22603  7336 29982 63572 68348 55554 35879 41569\n",
            " 72364 86059 68348 49412   596  4918  4740 59976 11831 18333 65828 60033\n",
            " 47683 83758 68578 11127 21426 87468 60847 91183 10817 80368 77090 28169\n",
            " 29982 50099 53376 75002 28169 66762 18138 10817 77349 23289 22940 72562\n",
            " 24166 42564 62669 87346 18138 82949 29445  4080 56663 81275 36517 47183\n",
            " 27491 56663 60004 91183 27491 75536  4318 42564 56449 58963 34040 29022\n",
            " 42565     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0], shape=(150,), dtype=int64) tf.Tensor(0, shape=(), dtype=int64)\n",
            "This was an absolutely terrible movie Don t be lured in by Christopher Walken or Michael Ironside Both are great actors but this must simply be their worst role in history Even their great acting could not redeem this movie s ridiculous storyline This movie is an early nineties US propaganda piece The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions Maria Conchita Alonso appeared phony and her pseudo love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning I am disappointed that there are movies like this ruining actor s like Christopher Walken s good name I could barely sit through it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xHaG_8NEAE7"
      },
      "source": [
        "Running this map over the entire dataset can be done like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:06:03.379137Z",
          "start_time": "2020-10-01T07:06:03.359510Z"
        },
        "id": "nANkfEvstcAv"
      },
      "source": [
        "#now tokenize/encode/pad all training and testing data\n",
        "encoded_train = imdb_train.map(encode_tf_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "encoded_test = imdb_test.map(encode_tf_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YszgY671EhUQ"
      },
      "source": [
        "While we have normalized and encoded the text of the reviews, we have not\n",
        "converted it into word vectors or embeddings. This step is performed along with the model training in the next step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxGj4ZvvtcAv"
      },
      "source": [
        "## Preparing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:06:19.472832Z",
          "start_time": "2020-10-01T07:06:19.467992Z"
        },
        "id": "xDVFLyf_tcAv"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = imdb_encoder.vocab_size # len(chars)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 64\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 64\n",
        "\n",
        "#batch size\n",
        "BATCH_SIZE=100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:06:21.557245Z",
          "start_time": "2020-10-01T07:06:21.550589Z"
        },
        "id": "kMKMONkrtcAw"
      },
      "source": [
        "def build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:06:26.612154Z",
          "start_time": "2020-10-01T07:06:25.825486Z"
        },
        "id": "lSiRLhzKtcAw",
        "outputId": "6ab2a674-3675-466f-a1f9-708ad3a1bad7"
      },
      "source": [
        "model = build_model_lstm(\n",
        "  vocab_size = vocab_size,\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (100, None, 64)           6011584   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (100, 64)                 33024     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (100, 1)                  65        \n",
            "=================================================================\n",
            "Total params: 6,044,673\n",
            "Trainable params: 6,044,673\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:06:31.817335Z",
          "start_time": "2020-10-01T07:06:31.799369Z"
        },
        "id": "fliMySU3tcAw"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "             optimizer='adam', \n",
        "             metrics=['accuracy', 'Precision', 'Recall'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e16J5NGYtcAw"
      },
      "source": [
        "# Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:06:37.541747Z",
          "start_time": "2020-10-01T07:06:37.536302Z"
        },
        "id": "bSpm2_ZgtcAx"
      },
      "source": [
        "# Prefetch for performance\n",
        "encoded_train_batched = encoded_train.batch(BATCH_SIZE).prefetch(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:10:27.683509Z",
          "start_time": "2020-10-01T07:06:40.292699Z"
        },
        "id": "AcQuzr2ftcAx",
        "outputId": "af7738d9-03b2-4307-de92-439b970d4ba8"
      },
      "source": [
        "model.fit(encoded_train_batched, epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "250/250 [==============================] - 21s 84ms/step - loss: 0.4106 - accuracy: 0.8049 - precision: 0.7849 - recall: 0.8400\n",
            "Epoch 2/10\n",
            "250/250 [==============================] - 22s 88ms/step - loss: 0.1692 - accuracy: 0.9380 - precision: 0.9403 - recall: 0.9353\n",
            "Epoch 3/10\n",
            "250/250 [==============================] - 22s 88ms/step - loss: 0.1109 - accuracy: 0.9622 - precision: 0.9640 - recall: 0.9603\n",
            "Epoch 4/10\n",
            "250/250 [==============================] - 22s 88ms/step - loss: 0.0674 - accuracy: 0.9777 - precision: 0.9781 - recall: 0.9773\n",
            "Epoch 5/10\n",
            "250/250 [==============================] - 22s 87ms/step - loss: 0.0552 - accuracy: 0.9823 - precision: 0.9819 - recall: 0.9828\n",
            "Epoch 6/10\n",
            "250/250 [==============================] - 23s 90ms/step - loss: 0.0654 - accuracy: 0.9774 - precision: 0.9770 - recall: 0.9779\n",
            "Epoch 7/10\n",
            "250/250 [==============================] - 22s 89ms/step - loss: 0.0257 - accuracy: 0.9924 - precision: 0.9921 - recall: 0.9928\n",
            "Epoch 8/10\n",
            "250/250 [==============================] - 22s 89ms/step - loss: 0.0117 - accuracy: 0.9973 - precision: 0.9975 - recall: 0.9970\n",
            "Epoch 9/10\n",
            "250/250 [==============================] - 23s 90ms/step - loss: 0.0115 - accuracy: 0.9969 - precision: 0.9976 - recall: 0.9962\n",
            "Epoch 10/10\n",
            "250/250 [==============================] - 22s 89ms/step - loss: 0.0041 - accuracy: 0.9993 - precision: 0.9994 - recall: 0.9992 0s - loss: 0.0039 - accuracy: 0.9993 - precision: 0.9993 - recall: 0.9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc89c58d410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:10:55.965511Z",
          "start_time": "2020-10-01T07:10:34.119008Z"
        },
        "id": "OGrKz5eWtcAx",
        "outputId": "20b116e5-b47c-423d-d38a-0c888b39876b"
      },
      "source": [
        "model.evaluate(encoded_test.batch(BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "250/250 [==============================] - 21s 82ms/step - loss: 0.9765 - accuracy: 0.8317 - precision: 0.8013 - recall: 0.8822\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9764885902404785,\n",
              " 0.8317199945449829,\n",
              " 0.8013225793838501,\n",
              " 0.8821600079536438]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bQSi6gstcAx"
      },
      "source": [
        "# BiLSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:11:23.892847Z",
          "start_time": "2020-10-01T07:11:23.887880Z"
        },
        "id": "hARXkpbDtcAx"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = imdb_encoder.vocab_size # len(chars)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 128\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 64\n",
        "\n",
        "#batch size\n",
        "BATCH_SIZE=50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:11:25.201887Z",
          "start_time": "2020-10-01T07:11:25.192055Z"
        },
        "id": "W-hhp5hEtcAy"
      },
      "source": [
        "dropout=0.2\n",
        "def build_model_bilstm(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.Dropout(dropout),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units, return_sequences=True, dropout=0.25)),\n",
        "    tf.keras.layers.Dropout(dropout),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units, dropout=0.25)),\n",
        "    tf.keras.layers.Dropout(dropout),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:11:29.441086Z",
          "start_time": "2020-10-01T07:11:26.487139Z"
        },
        "id": "KyLmAUANtcAy",
        "outputId": "2abd47ac-5a4a-4cfc-e5af-3d78c3aefa29"
      },
      "source": [
        "bilstm = build_model_bilstm(\n",
        "  vocab_size = vocab_size,\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)\n",
        "\n",
        "bilstm.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (50, None, 128)           12023168  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (50, None, 128)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (50, None, 128)           98816     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (50, None, 128)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (50, 128)                 98816     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (50, 128)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (50, 1)                   129       \n",
            "=================================================================\n",
            "Total params: 12,220,929\n",
            "Trainable params: 12,220,929\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:11:29.575709Z",
          "start_time": "2020-10-01T07:11:29.562501Z"
        },
        "id": "Zs5lNqTMtcAy"
      },
      "source": [
        "bilstm.compile(loss='binary_crossentropy', \n",
        "             optimizer='adam', \n",
        "             metrics=['accuracy', 'Precision', 'Recall'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:11:32.251708Z",
          "start_time": "2020-10-01T07:11:32.245767Z"
        },
        "id": "TwfkmL16tcAz"
      },
      "source": [
        "encoded_train_batched = encoded_train.batch(BATCH_SIZE).prefetch(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:17:29.418810Z",
          "start_time": "2020-10-01T07:11:34.295046Z"
        },
        "id": "WhGuKyKCtcAz",
        "outputId": "2ef1339a-8ff6-432b-dffc-7861b0164e21"
      },
      "source": [
        "bilstm.fit(encoded_train_batched, epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "500/500 [==============================] - 68s 136ms/step - loss: 0.3763 - accuracy: 0.8266 - precision: 0.8205 - recall: 0.8362\n",
            "Epoch 2/5\n",
            "500/500 [==============================] - 69s 138ms/step - loss: 0.1505 - accuracy: 0.9465 - precision: 0.9463 - recall: 0.9467\n",
            "Epoch 3/5\n",
            "500/500 [==============================] - 69s 137ms/step - loss: 0.0701 - accuracy: 0.9761 - precision: 0.9765 - recall: 0.9757\n",
            "Epoch 4/5\n",
            "500/500 [==============================] - 68s 137ms/step - loss: 0.0724 - accuracy: 0.9753 - precision: 0.9747 - recall: 0.9758\n",
            "Epoch 5/5\n",
            "500/500 [==============================] - 69s 137ms/step - loss: 0.0416 - accuracy: 0.9859 - precision: 0.9856 - recall: 0.9862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc85262dd10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-01T07:17:50.858013Z",
          "start_time": "2020-10-01T07:17:29.532181Z"
        },
        "id": "TBHVnvxJtcAz",
        "outputId": "25b01959-061d-40ed-f420-b46dff2c5828"
      },
      "source": [
        "bilstm.evaluate(encoded_test.batch(BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 17s 35ms/step - loss: 0.6753 - accuracy: 0.8384 - precision: 0.8459 - recall: 0.8274\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6753188967704773,\n",
              " 0.8383600115776062,\n",
              " 0.8459147810935974,\n",
              " 0.8274400234222412]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmb28yRqtcA0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}