{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-generating-text--one-character-at-a-time.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPPGb2BmYzVhjrQFsNe3Zoa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/advanced-natural-language-processing-with-tensorflow-2/blob/main/5-generating-text-with-rnn-and-gpt-2/01_generating_text_one_character_at_a_time.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY_xM_-pLhxR"
      },
      "source": [
        "##Generating text – Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL9OQjlNLilE"
      },
      "source": [
        "When your mobile phone completes a word as you type a message or when Gmail\n",
        "suggests a short reply or completes a sentence as you reply to an email, a text\n",
        "generation model is working in the background.\n",
        "\n",
        "The Transformer architecture forms the basis of state-of-the-art text generation models. **BERT uses only the encoder part of the Transformer architecture.**\n",
        "\n",
        "However, BERT, being bi-directional, is not suitable for the generation of text. A left-to-right (or right-to-left, depending on the language) language model built on the decoder part of the Transformer architecture is the foundation of text generation models today.\n",
        "\n",
        "Text can be generated a character at a time or with words and sentences together.\n",
        "\n",
        "Specifically, we will cover the following topics:\n",
        "\n",
        "- Generating text with:\n",
        "  - Character-based RNNs for generating news headlines and completing text messages\n",
        "  - GPT-2 to generate full sentences\n",
        "\n",
        "- Improving the quality of text generation using techniques such as:\n",
        "  - Greedy search\n",
        "  - Beam search\n",
        "  - Top-K sampling\n",
        "\n",
        "- Using advanced techniques such as learning rate annealing and\n",
        "checkpointing to enable long training times\n",
        "- Details of the Transformer decoder architecture\n",
        "- Details of the GPT and GPT-2 models\n",
        "\n",
        "A character-based approach for generating text is shown first. Such models can be quite useful for generating completions of a partially typed word in a sentence on a messaging platform, for example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n65YpytCOcM6"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkJSLHKzOd19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "78f9ac8c-1947-4635-c7b7-65fcdffe68ae"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "import datetime\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.5.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snZwbnGvOuIq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a0a0635-2b87-42e5-a4bd-9e7bc577de6b"
      },
      "source": [
        "######## GPU CONFIGS FOR RTX 2070 ###############\n",
        "## Please ignore if not training on GPU       ##\n",
        "## this is important for running CuDNN on GPU ##\n",
        "\n",
        "tf.keras.backend.clear_session() #- for easy reset of notebook state\n",
        "\n",
        "# chck if GPU can be seen by TF\n",
        "tf.config.list_physical_devices('GPU')\n",
        "#tf.debugging.set_log_device_placement(True)\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Restrict TensorFlow to only use the first GPU\n",
        "  try:\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
        "  except RuntimeError as e:\n",
        "    # Visible devices must be set before GPUs have been initialized\n",
        "    print(e)\n",
        "###############################################"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 Physical GPUs, 1 Logical GPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hf1YuOsOyPo"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!wget https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-TensorFlow-2/raw/master/chapter5-nlg-with-transformer-gpt/char-rnn/news-headlines.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R15577HNOeCE"
      },
      "source": [
        "##Generating text – one character at a time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbkTlxxzrQQI"
      },
      "source": [
        "Text generation yields a window into whether deep learning models are learning\n",
        "about the underlying structure of language. Text will be generated using two\n",
        "different approaches.The first approach is an RNN-based model that generates a character at a time.\n",
        "\n",
        "Text is tokenized into characters, which include capital and\n",
        "small letters, punctuation symbols, and digits. There are 96 tokens in total. This\n",
        "tokenization is an extreme example to test how much a model can learn about the\n",
        "language structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yy1EGcvsZvl"
      },
      "source": [
        "##Data loading and pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erXc8xRDsakd"
      },
      "source": [
        "For this particular example, we are going to use data from a constrained domain – a\n",
        "set of news headlines. The hypothesis is that news headlines are usually short and\n",
        "follow a particular structure. These headlines are usually a summary of an article and\n",
        "contain a large number of proper nouns like names of companies and celebrities.\n",
        "\n",
        "The first dataset is called the [News Aggregator dataset](https://archive.ics.uci.edu/ml/datasets/News+Aggregator).\n",
        "This dataset has\n",
        "over 420,000 news article titles, URLs, and other information.\n",
        "\n",
        "The second dataset is a\n",
        "set of over 200,000 news articles from The Huffington Post, called the [News Category dataset](https://www.kaggle.com/rmisra/news-category-dataset).\n",
        "\n",
        "News article headlines from both datasets are extracted and compiled into one file.\n",
        "\n",
        "Let's inspect the contents of the file to get a sense of the data:\n",
        "$ head\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM5uCAs6tYz3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bfda055-0c82-443f-d1ff-fef8b90ee84b"
      },
      "source": [
        "!head -5 news-headlines.tsv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV\tthere were 2 mass shootings in texas last week, but only 1 on tv\r\n",
            "Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song\twill smith joins diplo and nicky jam for the 2018 world cup's official song\r\n",
            "Hugh Grant Marries For The First Time At Age 57\thugh grant marries for the first time at age 57\r\n",
            "Jim Carrey Blasts 'Castrato' Adam Schiff And Democrats In New Artwork\tjim carrey blasts 'castrato' adam schiff and democrats in new artwork\r\n",
            "Julianna Margulies Uses Donald Trump Poop Bags To Pick Up After Her Dog\tjulianna margulies uses donald trump poop bags to pick up after her dog\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNoPFF4qtjj_"
      },
      "source": [
        "## Data normalization and tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9258yW99tkOG"
      },
      "source": [
        "This model uses a token per character. So, each letter, including\n",
        "punctuation, numbers, and space, becomes a token. Three additional tokens are\n",
        "added. These are:\n",
        "\n",
        "- `<EOS>`: Denotes end of sentences\n",
        "- `<UNK>`: it is common to replace out-of-vocabulary words with a special token\n",
        "- `<PAD>`: This is a unique padding token used to pad all headlines to the\n",
        "same length\n",
        "\n",
        "To start, the tokenization function needs to be set up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51koxP4utcPH"
      },
      "source": [
        "chars = sorted(set(\"abcdefghijklmnopqrstuvwxyz0123456789 -,;.!?:’’’/\\|_@#$%ˆ&*˜‘+-=()[]{}' ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\n",
        "chars = list(chars)\n",
        "\n",
        "EOS = \"<EOS>\"\n",
        "UNK = \"<UNK>\"\n",
        "PAD = \"<PAD>\"  # need to move mask to '0'index for Embedding layer\n",
        "\n",
        "chars.append(UNK)\n",
        "chars.append(EOS)  # end of sentence\n",
        "\n",
        "chars.insert(0, PAD)  # now padding should get index of 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l3YifsXwBCj"
      },
      "source": [
        "Once the token list is ready, methods need to be defined for converting characters to tokens and vice versa. \n",
        "\n",
        "Creating mapping is relatively straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J68YgjGYv_jP"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2index = {u: i for i, u in enumerate(chars)}\n",
        "index2char = np.array(chars)\n",
        "\n",
        "def char_index(ch):\n",
        "  # takes a character and returns an index\n",
        "  # if character is not in list, returns the unknown token\n",
        "  if ch in chars:\n",
        "    return char2index[ch]\n",
        "  return char2index[UNK]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dfOuhLsyghr"
      },
      "source": [
        "A maximum length of 75\n",
        "characters is used for the headlines. If the headlines are shorter than this length,\n",
        "they are padded. Any headlines longer than 75 characters are snipped. The <EOS>\n",
        "token is appended to the end of every headline.\n",
        "\n",
        "Let's set this up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B86SUqxw2zU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fb3b376-1dda-42e0-8b59-28e59614bd14"
      },
      "source": [
        "data = []     # load into this list of lists\n",
        "MAX_LEN = 75  # maximum length of a headline\n",
        "\n",
        "with open(\"news-headlines.tsv\", \"r\") as f:\n",
        "  lines = csv.reader(f, delimiter=\"\\t\")\n",
        "  for line in lines:\n",
        "    headline = line[0]\n",
        "    converted = [char_index(c) for c in headline[:-1]]  # convert to number\n",
        "\n",
        "    if len(converted) >= MAX_LEN:\n",
        "      converted = converted[0: MAX_LEN - 1]\n",
        "      converted.append(char2index[EOS])\n",
        "    else:\n",
        "      converted.append(char2index[EOS])\n",
        "      # add padding tokens\n",
        "      remaining = MAX_LEN - len(converted)\n",
        "      if remaining > 0:\n",
        "        for i in range(remaining):\n",
        "          converted.append(char2index[PAD])\n",
        "    data.append(converted)\n",
        "print(\"**** Data file loaded ****\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**** Data file loaded ****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYP_o1Mx1Udv"
      },
      "source": [
        "You may be wondering about\n",
        "the ground truth here for training as we only have a line of text. Since we want this\n",
        "model to generate text, the objective can be reduced to predicting the next character\n",
        "given a set of characters. \n",
        "\n",
        "Hence, a trick will be used to construct the ground truth\n",
        "– we will just shift the input sequence by one character and set it as the expected\n",
        "output.\n",
        "\n",
        "This transformation is quite easy do with numpy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nsh_oRX09A3"
      },
      "source": [
        "# now convert to numpy array\n",
        "np_data = np.array(data)\n",
        "\n",
        "# for training, we use one character shifted data\n",
        "np_data_in = np_data[:, :-1]\n",
        "np_data_out = np_data[:, 1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1guZRp421fl"
      },
      "source": [
        "With this nifty trick, we have both inputs and expected outputs ready for training.\n",
        "\n",
        "The final step is to convert it into tf.Data.DataSet for ease of batching and shuffling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taPJBrFC1tAE"
      },
      "source": [
        "# Create TF dataset\n",
        "X = tf.data.Dataset.from_tensor_slices((np_data_in, np_data_out))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZRSRWxR3C8S"
      },
      "source": [
        "Now everything is ready to start training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrnKjfLq3Dyv"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_CTtLLl6rxp"
      },
      "source": [
        "The model is quite simple. It has an embedding layer, followed by a GRU layer and a dense layer.\n",
        "\n",
        "The size of the vocabulary, the number of RNN units, and the size of the embeddings are set up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgX7S8WY3CaS"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(chars)\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "# batch size\n",
        "BATCH_SIZE = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrhzcQoMAkXi"
      },
      "source": [
        "With the batch size being defined, training data can be batched and ready for use by the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfaGxryuAc-r"
      },
      "source": [
        "# create tf.DataSet\n",
        "x_train = X.shuffle(100000, reshuffle_each_iteration=True).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQLfnsHEBNHx"
      },
      "source": [
        "A convenience method to build models is defined like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1U2rBI9BAQN"
      },
      "source": [
        "# define the model\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True, batch_input_shape=[batch_size, None]),\n",
        "      tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer=\"glorot_uniform\"),\n",
        "      tf.keras.layers.Dropout(0.1),\n",
        "      tf.keras.layers.Dense(vocab_size)                         \n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCs5V-NHCgYD"
      },
      "source": [
        "A model can be instantiated with this method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEq5qPGdCSw4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1bbcb96-ea06-4a05-a050-67b9378740c5"
      },
      "source": [
        "model = build_model(vocab_size=vocab_size, embedding_dim=embedding_dim, rnn_units=rnn_units, batch_size=BATCH_SIZE)\n",
        "\n",
        "print(\"**** Model Instantiated ****\")\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**** Model Instantiated ****\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (256, None, 256)          24576     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (256, None, 1024)         3938304   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (256, None, 1024)         0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (256, None, 96)           98400     \n",
            "=================================================================\n",
            "Total params: 4,061,280\n",
            "Trainable params: 4,061,280\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRJmHFx_EDwG"
      },
      "source": [
        "There are just over 4 million trainable parameters in this model. \n",
        "\n",
        "The Adam optimizer, with a sparse categorical loss function, is used for training this model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4fJ2t4UDlw5"
      },
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=\"adam\", loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYDZ7DiWFEFq"
      },
      "source": [
        "Since training is potentially going to take a long time, we need to set up checkpoints along with the training. If there is any problem in training and training stops, these checkpoints can be used to restart the training from the last saved checkpoint. \n",
        "\n",
        "A directory is created using the current timestamp for saving these checkpoints:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h9h7-YREbQQ"
      },
      "source": [
        "# Setup checkpoints\n",
        "\n",
        "# dynamically build folder names\n",
        "dt = datetime.datetime.today().strftime(\"%Y-%b-%d-%H-%M-%S\")\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = \"./training_checkpoints/\" + dt\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2evfZr9HHkS3"
      },
      "source": [
        "A custom callback that saves checkpoints during training is passed to the `model.fit()` function to be called at the end of every epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5k50SCQHWMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00113c81-e1bc-43b6-816a-99e93f64c095"
      },
      "source": [
        "print(\"**** Start Training ****\")\n",
        "EPOCHS = 25\n",
        "start = time.time()\n",
        "\n",
        "history = model.fit(x_train, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
        "\n",
        "print(\"**** End Training ****\")\n",
        "print(\"Training time: \", time.time()- start)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "**** Start Training ****\n",
            "Epoch 1/25\n",
            "2434/2434 [==============================] - 319s 128ms/step - loss: 1.2671\n",
            "Epoch 2/25\n",
            "2434/2434 [==============================] - 316s 130ms/step - loss: 0.9807\n",
            "Epoch 3/25\n",
            "2434/2434 [==============================] - 316s 130ms/step - loss: 0.9376\n",
            "Epoch 4/25\n",
            "2434/2434 [==============================] - 316s 130ms/step - loss: 0.9198\n",
            "Epoch 5/25\n",
            "2434/2434 [==============================] - 316s 130ms/step - loss: 0.9103\n",
            "Epoch 6/25\n",
            "2434/2434 [==============================] - 316s 130ms/step - loss: 0.9050\n",
            "Epoch 7/25\n",
            "2434/2434 [==============================] - 316s 130ms/step - loss: 0.9019\n",
            "Epoch 8/25\n",
            "2434/2434 [==============================] - 317s 130ms/step - loss: 0.8995\n",
            "Epoch 9/25\n",
            "2434/2434 [==============================] - 317s 130ms/step - loss: 0.8986\n",
            "Epoch 10/25\n",
            "2434/2434 [==============================] - 317s 130ms/step - loss: 0.8984\n",
            "Epoch 11/25\n",
            "2434/2434 [==============================] - 316s 130ms/step - loss: 0.8987\n",
            "Epoch 12/25\n",
            "2434/2434 [==============================] - 316s 130ms/step - loss: 0.9000\n",
            "Epoch 13/25\n",
            "2434/2434 [==============================] - 315s 129ms/step - loss: 0.9017\n",
            "Epoch 14/25\n",
            "2434/2434 [==============================] - 316s 130ms/step - loss: 0.9055\n",
            "Epoch 15/25\n",
            "2434/2434 [==============================] - 315s 130ms/step - loss: 0.9095\n",
            "Epoch 16/25\n",
            "2434/2434 [==============================] - 317s 130ms/step - loss: 0.9167\n",
            "Epoch 17/25\n",
            "2434/2434 [==============================] - 315s 130ms/step - loss: 0.9255\n",
            "Epoch 18/25\n",
            "2434/2434 [==============================] - 316s 130ms/step - loss: 0.9479\n",
            "Epoch 19/25\n",
            "2434/2434 [==============================] - 316s 130ms/step - loss: 1.1105\n",
            "Epoch 20/25\n",
            "2434/2434 [==============================] - 314s 129ms/step - loss: 1.7282\n",
            "Epoch 21/25\n",
            "2434/2434 [==============================] - 315s 129ms/step - loss: 1.7075\n",
            "Epoch 22/25\n",
            "2434/2434 [==============================] - 314s 129ms/step - loss: 1.7045\n",
            "Epoch 23/25\n",
            "2434/2434 [==============================] - 315s 129ms/step - loss: 1.6705\n",
            "Epoch 24/25\n",
            "2434/2434 [==============================] - 315s 129ms/step - loss: 1.5944\n",
            "Epoch 25/25\n",
            "2434/2434 [==============================] - 314s 129ms/step - loss: 1.4343\n",
            "**** End Training ****\n",
            "Training time:  7959.382402896881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bsV7lnQIXQ-"
      },
      "source": [
        "The final piece of code uses the history to plot the loss and\n",
        "save it as a PNG file in the same directory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRJCH2_PIJO3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "a2122a82-52d1-4261-8c0f-9e9af38e0f7b"
      },
      "source": [
        "# Plot accuracies\n",
        "lossplot = \"loss-\" + dt + \".png\"\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFe_Pop6I-Yd"
      },
      "source": [
        "As we can see, the loss decreases to a point and then shoots up. The standard\n",
        "expectation is that loss would monotonically decrease as the model was trained for more epochs.\n",
        "\n",
        "In the case shown above, the loss suddenly shoots up. In other cases,\n",
        "you may observe a NaN, or Not-A-Number, error. NaNs result from the exploding\n",
        "gradient problem during backpropagation through RNNs.\n",
        "\n",
        "The gradient direction causes weights to grow very large quickly and overflow, resulting in NaNs.\n",
        "\n",
        "The primary reason behind these occurrences is gradient descent overshooting the\n",
        "minima and starting to climb the slope before reducing again. This happens when\n",
        "the steps gradient descent is taking are too large. Another way to prevent the NaN issue is gradient clipping where gradients are clipped to an absolute maximum, preventing loss from exploding.\n",
        "\n",
        "**In the RNN model, a scheme needs to be used that reduces the learning rate over time. Reducing the learning rate over epochs\n",
        "reduces the chances for gradient descent to overshoot the minima. This technique\n",
        "of reducing the learning rate over time is called learning rate annealing or learning rate decay.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8ZfoLrjKPld"
      },
      "source": [
        "##Implementing learning rate decay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXfevTKzKgT-"
      },
      "source": [
        "There are two ways to implement learning rate decay in TensorFlow. The first way\n",
        "is to use one of the prebuilt schedulers that are part of the\n",
        "`tf.keras.optimizers.schedulers` package and use a configured instance with the optimizer.\n",
        "\n",
        "An example of a prebuilt scheduler is InverseTimeDecay."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx93DdWRVtEm"
      },
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    0.001,          # initial learning rate\n",
        "    decay_steps=STEPS_PER_EPOCH*(EPOCHS/10),\n",
        "    decay_rate=2,\n",
        "    staircase=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYPtEDo-V5-0"
      },
      "source": [
        "The\n",
        "number of steps per epoch can be calculated by dividing the number of training\n",
        "examples by batch size. The number of decay steps determines how the learning\n",
        "rate is reduced. The equation used to compute the learning rate is:\n",
        "\n",
        "$$\n",
        "new\\_rate = \\frac{initial\\_rate}{1 + decay\\_rate * (\\frac{step}{decay\\_step})} \n",
        "$$\n",
        "\n",
        "After being set up, all this function needs is the step number for computing the new learning rate. Once the schedule is set up, it can be passed to the optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmMKxnvfWwPB"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(lr_schedule)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ASi7CmoWyvC"
      },
      "source": [
        "That's it! The rest of the training loop code is unchanged. \n",
        "\n",
        "However, this learning rate scheduler starts reducing the learning rate from the first epoch itself. A lower learning rate increases the amount of training time. Ideally, we would keep the learning rate unchanged for the first few epochs and then reduce it.\n",
        "\n",
        "Looking at plot above, the learning rate is probably effective until about the\n",
        "tenth epoch.\n",
        "\n",
        "BERT also uses learning rate warmup before learning rate decay.\n",
        "Learning rate warmup generally refers to increasing the learning rate for a few\n",
        "epochs. BERT was trained for 1,000,000 steps, which roughly translates to 40 epochs.\n",
        "For the first 10,000 steps, the learning rate was increased, and then it was linearly\n",
        "decayed. Implementing such a learning rate schedule is better accomplished by a\n",
        "custom callback.\n",
        "\n",
        "Custom callbacks in TensorFlow enable the execution of custom logic at various\n",
        "points during training and inference. We saw an example of a prebuilt callback that saves checkpoints during training. \n",
        "\n",
        "A custom callback provides hooks that enable\n",
        "desired logic that can be executed at various points during training. This main\n",
        "step is to define a subclass of `tf.keras.callbacks.Callback`. Then, one or more of the following functions can be implemented to hook onto the events exposed by TensorFlow:\n",
        "\n",
        "- `on_[train,test,predict]_begin / on_[train,test,predict]_end`: This\n",
        "callback happens at the start of training or the end of the training.\n",
        "\n",
        "- `on_[train,test,predict]_batch_begin / on_[train,test,predict] _batch_\n",
        "end`: These callbacks happen when training for a specific batch starts or ends.\n",
        "\n",
        "- `on_epoch_begin / on_epoch_end`: This is a training-specific function called at the start or end of an epoch.\n",
        "\n",
        "We will implement a callback for the start of the epoch that adjusts that epoch's\n",
        "learning rate. Our implementation will keep the learning rate constant for a\n",
        "configurable number of initial epochs and then reduce the learning rate in a fashion\n",
        "similar to the inverse time decay function.\n",
        "\n",
        "<img src='https://github.com/rahiakela/advanced-natural-language-processing-with-tensorflow-2/blob/main/5-generating-text-with-rnn-and-gpt-2/images/1.png?raw=1' width='800'/>\n",
        "\n",
        "First, a subclass is created with the function defined in it. The best place to put this, is just around the checkpoint callback, before the start of training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enVs2I2avc3x"
      },
      "source": [
        "# Custom Callback for Learning Rate Decay\n",
        "class LearningRateScheduler(tf.keras.callbacks.Callback):\n",
        "  \"\"\"Learning rate scheduler which decays the learning rate\"\"\"\n",
        "\n",
        "  def __init__(self, init_lr, decay, steps, start_epoch):\n",
        "    super().__init__()\n",
        "    self.init_learning_rate = init_lr   #initial learning rate\n",
        "    self.decay = decay                  # how sharply to decay\n",
        "    self.steps = steps                  # total number of steps of decay\n",
        "    self.start_epoch = start_epoch      # which epoch to start decaying\n",
        "\n",
        "  def on_epoch_begin(self, epoch, logs=None):\n",
        "\n",
        "    if not hasattr(self.model.optimizer, \"lr\"):\n",
        "      raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
        "    # Get the current learning rate from model's optimizer.\n",
        "    lr = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
        "\n",
        "    if (epoch >= self.start_epoch):\n",
        "      # Call schedule function to get the scheduled learning rate.\n",
        "      scheduled_lr = self.init_learning_rate / (1 + self.decay * (epoch / self.steps))\n",
        "      # Set the value back to the optimizer before this epoch starts\n",
        "      tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
        "      print(\"\\nEpoch %05d: Learning rate is %6.8f.\" % (epoch, scheduled_lr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVRPp7CPDFMK"
      },
      "source": [
        "Using this callback in the training loop requires the instantiation of the callback. The following parameters are set while instantiating the callback:\n",
        "\n",
        "- The initial learning rate is set to 0.001.\n",
        "- The decay rate is set to 4. Please feel free to play around with different\n",
        "settings.\n",
        "- The number of steps is set to the number of epochs. The model is trained for\n",
        "150 epochs.\n",
        "- Learning rate decay should start after epoch 10, so the start epoch is set to 10.\n",
        "\n",
        "The training loop is updated to include the callback like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAnkZS4_DVdX"
      },
      "source": [
        "print(\"**** Start Training ****\")\n",
        "\n",
        "EPOCHS = 150\n",
        "lr_decay = LearningRateScheduler(0.001, 4., EPOCHS, 10)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "history = model.fit(x_train, epochs=EPOCHS, callbacks=[checkpoint_callback, lr_decay])\n",
        "\n",
        "print(\"**** End Training ****\")\n",
        "print(\"Training time: \", time.time()- start)\n",
        "\n",
        "print(\"Checkpoint directory: \", checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RXWWOA6EB4e"
      },
      "source": [
        "Now, the model is ready to be trained. Training 150 epochs took over 10 hours on the GPU-capable machine.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDDNxuDWEvJs"
      },
      "source": [
        "# Plot accuracies\n",
        "lossplot = \"loss-\" + dt + \".png\"\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxcdGUycEvw5"
      },
      "source": [
        "The loss drops very fast for the first few epochs before plateauing\n",
        "near epoch 10. Learning rate decay kicks in at that point, and the loss starts to fall again.\n",
        "\n",
        "Training this model took much time and advanced tricks like learning rate decay to train."
      ]
    }
  ]
}