{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02-generating-text-with-greedy-search.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMlgWgVmpXYTAt0dKAvJ88C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/advanced-natural-language-processing-with-tensorflow-2/blob/main/5-generating-text-with-rnn-and-gpt-2/02_generating_text_with_greedy_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY_xM_-pLhxR"
      },
      "source": [
        "##Generating text – Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL9OQjlNLilE"
      },
      "source": [
        "When your mobile phone completes a word as you type a message or when Gmail\n",
        "suggests a short reply or completes a sentence as you reply to an email, a text\n",
        "generation model is working in the background.\n",
        "\n",
        "The Transformer architecture forms the basis of state-of-the-art text generation models. **BERT uses only the encoder part of the Transformer architecture.**\n",
        "\n",
        "However, BERT, being bi-directional, is not suitable for the generation of text. A left-to-right (or right-to-left, depending on the language) language model built on the decoder part of the Transformer architecture is the foundation of text generation models today.\n",
        "\n",
        "Text can be generated a character at a time or with words and sentences together.\n",
        "\n",
        "Specifically, we will cover the following topics:\n",
        "\n",
        "- Generating text with:\n",
        "  - Character-based RNNs for generating news headlines and completing text messages\n",
        "  - GPT-2 to generate full sentences\n",
        "\n",
        "- Improving the quality of text generation using techniques such as:\n",
        "  - Greedy search\n",
        "  - Beam search\n",
        "  - Top-K sampling\n",
        "\n",
        "- Using advanced techniques such as learning rate annealing and\n",
        "checkpointing to enable long training times\n",
        "- Details of the Transformer decoder architecture\n",
        "- Details of the GPT and GPT-2 models\n",
        "\n",
        "A character-based approach for generating text is shown first. Such models can be quite useful for generating completions of a partially typed word in a sentence on a messaging platform, for example.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n65YpytCOcM6"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkJSLHKzOd19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "142e87a7-e58c-4920-d0ab-de0a99badc13"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "import datetime\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snZwbnGvOuIq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2b01475-b020-433f-d606-9882325eae98"
      },
      "source": [
        "######## GPU CONFIGS FOR RTX 2070 ###############\n",
        "## Please ignore if not training on GPU       ##\n",
        "## this is important for running CuDNN on GPU ##\n",
        "\n",
        "tf.keras.backend.clear_session() #- for easy reset of notebook state\n",
        "\n",
        "# chck if GPU can be seen by TF\n",
        "tf.config.list_physical_devices('GPU')\n",
        "#tf.debugging.set_log_device_placement(True)\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  # Restrict TensorFlow to only use the first GPU\n",
        "  try:\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
        "  except RuntimeError as e:\n",
        "    # Visible devices must be set before GPUs have been initialized\n",
        "    print(e)\n",
        "###############################################"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Physical GPUs, 1 Logical GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hf1YuOsOyPo"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!wget https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-TensorFlow-2/raw/master/chapter5-nlg-with-transformer-gpt/char-rnn/news-headlines.tsv"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R15577HNOeCE"
      },
      "source": [
        "##Generating text with greedy search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbkTlxxzrQQI"
      },
      "source": [
        "There are two main steps in generating text. The first step is restoring a trained model from the checkpoint. The second step is generating a character at a time from a trained model until a specific end condition is met."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yy1EGcvsZvl"
      },
      "source": [
        "##Data loading and pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erXc8xRDsakd"
      },
      "source": [
        "For this particular example, we are going to use data from a constrained domain – a\n",
        "set of news headlines. The hypothesis is that news headlines are usually short and\n",
        "follow a particular structure. These headlines are usually a summary of an article and\n",
        "contain a large number of proper nouns like names of companies and celebrities.\n",
        "\n",
        "The first dataset is called the [News Aggregator dataset](https://archive.ics.uci.edu/ml/datasets/News+Aggregator).\n",
        "This dataset has\n",
        "over 420,000 news article titles, URLs, and other information.\n",
        "\n",
        "The second dataset is a\n",
        "set of over 200,000 news articles from The Huffington Post, called the [News Category dataset](https://www.kaggle.com/rmisra/news-category-dataset).\n",
        "\n",
        "News article headlines from both datasets are extracted and compiled into one file.\n",
        "\n",
        "Let's inspect the contents of the file to get a sense of the data:\n",
        "$ head\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM5uCAs6tYz3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0564f1d9-5720-44a3-c7d9-431e03ea62ed"
      },
      "source": [
        "!head -5 news-headlines.tsv"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV\tthere were 2 mass shootings in texas last week, but only 1 on tv\r\n",
            "Will Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song\twill smith joins diplo and nicky jam for the 2018 world cup's official song\r\n",
            "Hugh Grant Marries For The First Time At Age 57\thugh grant marries for the first time at age 57\r\n",
            "Jim Carrey Blasts 'Castrato' Adam Schiff And Democrats In New Artwork\tjim carrey blasts 'castrato' adam schiff and democrats in new artwork\r\n",
            "Julianna Margulies Uses Donald Trump Poop Bags To Pick Up After Her Dog\tjulianna margulies uses donald trump poop bags to pick up after her dog\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNoPFF4qtjj_"
      },
      "source": [
        "## Data normalization and tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9258yW99tkOG"
      },
      "source": [
        "This model uses a token per character. So, each letter, including\n",
        "punctuation, numbers, and space, becomes a token. Three additional tokens are\n",
        "added. These are:\n",
        "\n",
        "- `<EOS>`: Denotes end of sentences\n",
        "- `<UNK>`: it is common to replace out-of-vocabulary words with a special token\n",
        "- `<PAD>`: This is a unique padding token used to pad all headlines to the\n",
        "same length\n",
        "\n",
        "To start, the tokenization function needs to be set up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51koxP4utcPH"
      },
      "source": [
        "chars = sorted(set(\"abcdefghijklmnopqrstuvwxyz0123456789 -,;.!?:’’’/\\|_@#$%ˆ&*˜‘+-=()[]{}' ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\n",
        "chars = list(chars)\n",
        "\n",
        "EOS = \"<EOS>\"\n",
        "UNK = \"<UNK>\"\n",
        "PAD = \"<PAD>\"  # need to move mask to '0'index for Embedding layer\n",
        "\n",
        "chars.append(UNK)\n",
        "chars.append(EOS)  # end of sentence\n",
        "\n",
        "chars.insert(0, PAD)  # now padding should get index of 0"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l3YifsXwBCj"
      },
      "source": [
        "Once the token list is ready, methods need to be defined for converting characters to tokens and vice versa. \n",
        "\n",
        "Creating mapping is relatively straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J68YgjGYv_jP"
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2index = {u: i for i, u in enumerate(chars)}\n",
        "index2char = np.array(chars)\n",
        "\n",
        "def char_index(ch):\n",
        "  # takes a character and returns an index\n",
        "  # if character is not in list, returns the unknown token\n",
        "  if ch in chars:\n",
        "    return char2index[ch]\n",
        "  return char2index[UNK]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dfOuhLsyghr"
      },
      "source": [
        "A maximum length of 75\n",
        "characters is used for the headlines. If the headlines are shorter than this length,\n",
        "they are padded. Any headlines longer than 75 characters are snipped. The <EOS>\n",
        "token is appended to the end of every headline.\n",
        "\n",
        "Let's set this up:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B86SUqxw2zU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef00f320-9043-4723-de85-078e88744abb"
      },
      "source": [
        "data = []     # load into this list of lists\n",
        "MAX_LEN = 75  # maximum length of a headline\n",
        "\n",
        "with open(\"news-headlines.tsv\", \"r\") as f:\n",
        "  lines = csv.reader(f, delimiter=\"\\t\")\n",
        "  for line in lines:\n",
        "    headline = line[0]\n",
        "    converted = [char_index(c) for c in headline[:-1]]  # convert to number\n",
        "\n",
        "    if len(converted) >= MAX_LEN:\n",
        "      converted = converted[0: MAX_LEN - 1]\n",
        "      converted.append(char2index[EOS])\n",
        "    else:\n",
        "      converted.append(char2index[EOS])\n",
        "      # add padding tokens\n",
        "      remaining = MAX_LEN - len(converted)\n",
        "      if remaining > 0:\n",
        "        for i in range(remaining):\n",
        "          converted.append(char2index[PAD])\n",
        "    data.append(converted)\n",
        "print(\"**** Data file loaded ****\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**** Data file loaded ****\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYP_o1Mx1Udv"
      },
      "source": [
        "You may be wondering about\n",
        "the ground truth here for training as we only have a line of text. Since we want this\n",
        "model to generate text, the objective can be reduced to predicting the next character\n",
        "given a set of characters. \n",
        "\n",
        "Hence, a trick will be used to construct the ground truth\n",
        "– we will just shift the input sequence by one character and set it as the expected\n",
        "output.\n",
        "\n",
        "This transformation is quite easy do with numpy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nsh_oRX09A3"
      },
      "source": [
        "# now convert to numpy array\n",
        "np_data = np.array(data)\n",
        "\n",
        "# for training, we use one character shifted data\n",
        "np_data_in = np_data[:, :-1]\n",
        "np_data_out = np_data[:, 1:]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1guZRp421fl"
      },
      "source": [
        "With this nifty trick, we have both inputs and expected outputs ready for training.\n",
        "\n",
        "The final step is to convert it into tf.Data.DataSet for ease of batching and shuffling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taPJBrFC1tAE"
      },
      "source": [
        "# Create TF dataset\n",
        "X = tf.data.Dataset.from_tensor_slices((np_data_in, np_data_out))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZRSRWxR3C8S"
      },
      "source": [
        "Now everything is ready to start training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrnKjfLq3Dyv"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_CTtLLl6rxp"
      },
      "source": [
        "Since the checkpoints only stored the weights for the layers, defining the model structure is important. The main difference from the training network is the batch size. We want to generate a sentence at a time, so we set the batch size as 1:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgX7S8WY3CaS"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(chars)\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "# batch size\n",
        "BATCH_SIZE = 256"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQLfnsHEBNHx"
      },
      "source": [
        "A convenience function for setting up the model structure is defined like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1U2rBI9BAQN"
      },
      "source": [
        "#this one is without padding masking or dropout layer\n",
        "def build_gen_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "      tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer=\"glorot_uniform\"),\n",
        "      tf.keras.layers.Dense(vocab_size)                         \n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_380qm2PH-J"
      },
      "source": [
        "A model can be instantiated with this method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMEMVfFoPIZX"
      },
      "source": [
        "gen_model = build_gen_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCs5V-NHCgYD"
      },
      "source": [
        "Note that the embedding layer does not use masking because, in text generation,\n",
        "we are not passing an entire sequence but only part of a sequence that needs to be completed. \n",
        "\n",
        "Now that the model is defined, the weights for the layers can be loaded\n",
        "in from the checkpoint. Please remember to replace the checkpoint directory with\n",
        "your local directory containing the checkpoints from training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEq5qPGdCSw4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "20eb5eb4-0f0a-467d-d7ef-440b76639846"
      },
      "source": [
        "# Now setup the location of the checkpoint and load the latest checkpoint\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = \"./training_checkpoints/2020-Oct-01-14-29-55\"\n",
        "\n",
        "gen_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "gen_model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "print(\"**** Model Instantiated ****\")\n",
        "print(gen_model.summary())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-cca8cc78cd19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./training_checkpoints/2020-Oct-01-14-29-55\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgen_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mgen_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2327\u001b[0m           'True when by_name is True.')\n\u001b[1;32m   2328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2329\u001b[0;31m     \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_detect_save_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2331\u001b[0m       \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_detect_save_format\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m   3006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3007\u001b[0m   \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3008\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0msaving_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3009\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/saving_utils.py\u001b[0m in \u001b[0;36mis_hdf5_filepath\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m   return (filepath.endswith('.h5') or filepath.endswith('.keras') or\n\u001b[0m\u001b[1;32m    321\u001b[0m           filepath.endswith('.hdf5'))\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'endswith'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRJmHFx_EDwG"
      },
      "source": [
        "The second main step is to generate text a character at a time. Generating text needs a seed or a starting few letters, which are completed by the model into a sentence.\n",
        "\n",
        "The process of generation is encapsulated in the function below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4fJ2t4UDlw5"
      },
      "source": [
        "# Low temperatures results in more predictable text.\n",
        "# Higher temperatures results in more surprising text.\n",
        "# Experiment to find the best setting.\n",
        "def generate_text(model, start_string, temperature=0.7, num_generate=75):\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx(s) for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Here batch size == 1\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a categorical distribution to predict the word returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "    # We pass the predicted word as the next input to the model along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    # lets break is <EOS> token is generated\n",
        "    if idx2char[predicted_id] == EOS:\n",
        "      break #end of a sentence reached, lets stop\n",
        "  \n",
        "  return (start_string + \"\".join(text_generated))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFe_Pop6I-Yd"
      },
      "source": [
        "At every point, the character with the highest likelihood is chosen. Choosing the\n",
        "next letter with the highest probability is called greedy search. However, there\n",
        "is a configuration parameter called temperature, which can be used to adjust the\n",
        "predictability of the generated text.\n",
        "\n",
        "Once probabilities for all characters are predicted, dividing the probabilities by the temperature changes the distribution of the generated characters. Smaller values of the temperature generate text that is closer to the original text. Larger values of the temperature generate more creative text. Here, a value of 0.7 is chosen to bias more on the surprising side.\n",
        "\n",
        "To generate the text, all that is needed is one line of code:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4nbEP-sZQMm"
      },
      "source": [
        "print(generate_text(gen_model, start_string=u\"Google\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-whAIrYdJfz"
      },
      "source": [
        "print(generate_text(gen_model, start_string=u\"Apple\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf5Bd3GbdKL_"
      },
      "source": [
        "Each execution of the command may generate slightly different results. The line\n",
        "generated above, while obviously nonsensical, is pretty well structured. The model has learned capitalization rules and headline structure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc3Wafg6dNw6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8ZfoLrjKPld"
      },
      "source": [
        "##Implementing learning rate decay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXfevTKzKgT-"
      },
      "source": [
        "There are two ways to implement learning rate decay in TensorFlow. The first way\n",
        "is to use one of the prebuilt schedulers that are part of the\n",
        "`tf.keras.optimizers.schedulers` package and use a configured instance with the optimizer.\n",
        "\n",
        "An example of a prebuilt scheduler is InverseTimeDecay."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx93DdWRVtEm"
      },
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    0.001,          # initial learning rate\n",
        "    decay_steps=STEPS_PER_EPOCH*(EPOCHS/10),\n",
        "    decay_rate=2,\n",
        "    staircase=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYPtEDo-V5-0"
      },
      "source": [
        "The\n",
        "number of steps per epoch can be calculated by dividing the number of training\n",
        "examples by batch size. The number of decay steps determines how the learning\n",
        "rate is reduced. The equation used to compute the learning rate is:\n",
        "\n",
        "$$\n",
        "new\\_rate = \\frac{initial\\_rate}{1 + decay\\_rate * (\\frac{step}{decay\\_step})} \n",
        "$$\n",
        "\n",
        "After being set up, all this function needs is the step number for computing the new learning rate. Once the schedule is set up, it can be passed to the optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmMKxnvfWwPB"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(lr_schedule)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ASi7CmoWyvC"
      },
      "source": [
        "That's it! The rest of the training loop code is unchanged. \n",
        "\n",
        "However, this learning rate scheduler starts reducing the learning rate from the first epoch itself. A lower learning rate increases the amount of training time. Ideally, we would keep the learning rate unchanged for the first few epochs and then reduce it.\n",
        "\n",
        "Looking at plot above, the learning rate is probably effective until about the\n",
        "tenth epoch.\n",
        "\n",
        "BERT also uses learning rate warmup before learning rate decay.\n",
        "Learning rate warmup generally refers to increasing the learning rate for a few\n",
        "epochs. BERT was trained for 1,000,000 steps, which roughly translates to 40 epochs.\n",
        "For the first 10,000 steps, the learning rate was increased, and then it was linearly\n",
        "decayed. Implementing such a learning rate schedule is better accomplished by a\n",
        "custom callback.\n",
        "\n",
        "Custom callbacks in TensorFlow enable the execution of custom logic at various\n",
        "points during training and inference. We saw an example of a prebuilt callback that saves checkpoints during training. \n",
        "\n",
        "A custom callback provides hooks that enable\n",
        "desired logic that can be executed at various points during training. This main\n",
        "step is to define a subclass of `tf.keras.callbacks.Callback`. Then, one or more of the following functions can be implemented to hook onto the events exposed by TensorFlow:\n",
        "\n",
        "- `on_[train,test,predict]_begin / on_[train,test,predict]_end`: This\n",
        "callback happens at the start of training or the end of the training.\n",
        "\n",
        "- `on_[train,test,predict]_batch_begin / on_[train,test,predict] _batch_\n",
        "end`: These callbacks happen when training for a specific batch starts or ends.\n",
        "\n",
        "- `on_epoch_begin / on_epoch_end`: This is a training-specific function called at the start or end of an epoch.\n",
        "\n",
        "We will implement a callback for the start of the epoch that adjusts that epoch's\n",
        "learning rate. Our implementation will keep the learning rate constant for a\n",
        "configurable number of initial epochs and then reduce the learning rate in a fashion\n",
        "similar to the inverse time decay function.\n",
        "\n",
        "<img src='https://github.com/rahiakela/advanced-natural-language-processing-with-tensorflow-2/blob/main/5-generating-text-with-rnn-and-gpt-2/images/1.png?raw=1' width='800'/>\n",
        "\n",
        "First, a subclass is created with the function defined in it. The best place to put this, is just around the checkpoint callback, before the start of training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enVs2I2avc3x"
      },
      "source": [
        "# Custom Callback for Learning Rate Decay\n",
        "class LearningRateScheduler(tf.keras.callbacks.Callback):\n",
        "  \"\"\"Learning rate scheduler which decays the learning rate\"\"\"\n",
        "\n",
        "  def __init__(self, init_lr, decay, steps, start_epoch):\n",
        "    super().__init__()\n",
        "    self.init_learning_rate = init_lr   #initial learning rate\n",
        "    self.decay = decay                  # how sharply to decay\n",
        "    self.steps = steps                  # total number of steps of decay\n",
        "    self.start_epoch = start_epoch      # which epoch to start decaying\n",
        "\n",
        "  def on_epoch_begin(self, epoch, logs=None):\n",
        "\n",
        "    if not hasattr(self.model.optimizer, \"lr\"):\n",
        "      raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
        "    # Get the current learning rate from model's optimizer.\n",
        "    lr = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
        "\n",
        "    if (epoch >= self.start_epoch):\n",
        "      # Call schedule function to get the scheduled learning rate.\n",
        "      scheduled_lr = self.init_learning_rate / (1 + self.decay * (epoch / self.steps))\n",
        "      # Set the value back to the optimizer before this epoch starts\n",
        "      tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
        "      print(\"\\nEpoch %05d: Learning rate is %6.8f.\" % (epoch, scheduled_lr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVRPp7CPDFMK"
      },
      "source": [
        "Using this callback in the training loop requires the instantiation of the callback. The following parameters are set while instantiating the callback:\n",
        "\n",
        "- The initial learning rate is set to 0.001.\n",
        "- The decay rate is set to 4. Please feel free to play around with different\n",
        "settings.\n",
        "- The number of steps is set to the number of epochs. The model is trained for\n",
        "150 epochs.\n",
        "- Learning rate decay should start after epoch 10, so the start epoch is set to 10.\n",
        "\n",
        "The training loop is updated to include the callback like so:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAnkZS4_DVdX"
      },
      "source": [
        "print(\"**** Start Training ****\")\n",
        "\n",
        "EPOCHS = 150\n",
        "lr_decay = LearningRateScheduler(0.001, 4., EPOCHS, 10)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "history = model.fit(x_train, epochs=EPOCHS, callbacks=[checkpoint_callback, lr_decay])\n",
        "\n",
        "print(\"**** End Training ****\")\n",
        "print(\"Training time: \", time.time()- start)\n",
        "\n",
        "print(\"Checkpoint directory: \", checkpoint_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RXWWOA6EB4e"
      },
      "source": [
        "Now, the model is ready to be trained. Training 150 epochs took over 10 hours on the GPU-capable machine.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDDNxuDWEvJs"
      },
      "source": [
        "# Plot accuracies\n",
        "lossplot = \"loss-\" + dt + \".png\"\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxcdGUycEvw5"
      },
      "source": [
        "The loss drops very fast for the first few epochs before plateauing\n",
        "near epoch 10. Learning rate decay kicks in at that point, and the loss starts to fall again.\n",
        "\n",
        "Training this model took much time and advanced tricks like learning rate decay to train."
      ]
    }
  ]
}