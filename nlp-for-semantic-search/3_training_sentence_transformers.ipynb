{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-training-sentence-transformers.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN0M2LO+zpU/JGIh5Yz6liJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d5501b64d92349279280388f464c8b36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f90935ba2cc64f76823af2bb2cbce8c8",
              "IPY_MODEL_6e0454de4fd04a23afe79bb4582b68f2",
              "IPY_MODEL_b43a59843edd4bebadd9ed754af98037"
            ],
            "layout": "IPY_MODEL_345aaa093c854cad9976b6d12a897f69"
          }
        },
        "f90935ba2cc64f76823af2bb2cbce8c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51c1cc264a46409d976a2f73c64e66db",
            "placeholder": "​",
            "style": "IPY_MODEL_bfa244d044274e0bbc44ff477bb132dd",
            "value": "100%"
          }
        },
        "6e0454de4fd04a23afe79bb4582b68f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93b09913a39b4cf5b5bc56c3724cedd6",
            "max": 943,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1686e4497fd44e3cae8e6740a0fd8c33",
            "value": 943
          }
        },
        "b43a59843edd4bebadd9ed754af98037": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79723d3afd85431cab8dd31746ceddc7",
            "placeholder": "​",
            "style": "IPY_MODEL_e58da1fb4e8d4b0ba7b5fca04d22aa15",
            "value": " 943/943 [00:08&lt;00:00, 120.62ba/s]"
          }
        },
        "345aaa093c854cad9976b6d12a897f69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51c1cc264a46409d976a2f73c64e66db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfa244d044274e0bbc44ff477bb132dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93b09913a39b4cf5b5bc56c3724cedd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1686e4497fd44e3cae8e6740a0fd8c33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79723d3afd85431cab8dd31746ceddc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e58da1fb4e8d4b0ba7b5fca04d22aa15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23ea3cffc1484dd6a3b63c37e847faa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0dbb6ebd71334ab391d6cb7ceab90f3f",
              "IPY_MODEL_fc1bfa59a0f34a258d29e4bc0b5ae4b4",
              "IPY_MODEL_955ebcf0e7f446f0b7af29dfd7d58600"
            ],
            "layout": "IPY_MODEL_7dbc659e789b40ddb4c2e868d49810eb"
          }
        },
        "0dbb6ebd71334ab391d6cb7ceab90f3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d971a063e57487e98984350a9df0b9b",
            "placeholder": "​",
            "style": "IPY_MODEL_f620cfcd0cd64c8487c3f9c60e46c7a0",
            "value": "  0%"
          }
        },
        "fc1bfa59a0f34a258d29e4bc0b5ae4b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_504d7a64451e47d3a2df9b76be718ce2",
            "max": 58880,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_541c175963874170a5c4830e17b0f183",
            "value": 0
          }
        },
        "955ebcf0e7f446f0b7af29dfd7d58600": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31e6ac5fffa04db7b5969d0eea25de42",
            "placeholder": "​",
            "style": "IPY_MODEL_b60257d1790244378fa8c28699006618",
            "value": " 0/58880 [00:12&lt;?, ?it/s]"
          }
        },
        "7dbc659e789b40ddb4c2e868d49810eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d971a063e57487e98984350a9df0b9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f620cfcd0cd64c8487c3f9c60e46c7a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "504d7a64451e47d3a2df9b76be718ce2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "541c175963874170a5c4830e17b0f183": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31e6ac5fffa04db7b5969d0eea25de42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b60257d1790244378fa8c28699006618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/natural-language-processing-research-and-practice/blob/main/nlp-for-semantic-search/3_training_sentence_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training Sentence Transformers the OG Way (with Softmax Loss)"
      ],
      "metadata": {
        "id": "EqrgFWqGQzLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several ways of training sentence transformers. One of the most popular (and the approach we will cover) is using Natural Language Inference (NLI) datasets.\n",
        "\n",
        "NLI focus on identifying sentence pairs that infer or do not infer one another. We will use two of these datasets; the Stanford Natural Language Inference (SNLI) and Multi-Genre NLI (MNLI) corpora.\n",
        "\n",
        "Merging these two corpora gives us 943K sentence pairs (550K from SNLI, 393K from MNLI). All pairs include a `premise` and a `hypothesis`, and each pair is assigned a `label`:\n",
        "\n",
        "- 0 — entailment, e.g. the premise suggests the hypothesis.\n",
        "- 1 — neutral, the premise and hypothesis could both be true, but they are not necessarily related.\n",
        "- 2 — contradiction, the premise and hypothesis contradict each other.\n",
        "\n",
        "**Reference**:\n",
        "\n",
        "https://www.pinecone.io/learn/train-sentence-transformers-softmax/"
      ],
      "metadata": {
        "id": "ELjuLzNkRBQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup"
      ],
      "metadata": {
        "id": "L-Nwxah9RaPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install datasets\n",
        "!pip -q install transformers\n",
        "!pip -q install sentence_transformers"
      ],
      "metadata": {
        "id": "ed7werBARbiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel\n",
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "\n",
        "from sentence_transformers import InputExample\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import os\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "FGv2kJ2MRkSd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "-ZQTUvsHIu-R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NLI Training"
      ],
      "metadata": {
        "id": "NfBPmR3NULOP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When training the model, we will be feeding sentence A (the premise) into BERT, followed by sentence B (the hypothesis) on the next step.\n",
        "\n",
        "From there, the models are optimized using softmax loss using the label field. We will explain this in more depth soon."
      ],
      "metadata": {
        "id": "Dv74x-SLUNSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snli = datasets.load_dataset(\"snli\", split=\"train\")"
      ],
      "metadata": {
        "id": "kjRK2ZcYUQDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snli"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2W_JUH8UZf0",
        "outputId": "8a77e7c2-a338-4ab0-968e-4a663bf03ca1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['premise', 'hypothesis', 'label'],\n",
              "    num_rows: 550152\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(snli[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K22ZdWALUefI",
        "outputId": "84ca9cd0-5d3a-4e8d-ad0d-8f1548f8422b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'premise': 'A person on a horse jumps over a broken down airplane.', 'hypothesis': 'A person is training his horse for a competition.', 'label': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m_nli = datasets.load_dataset(\"glue\", \"mnli\", split=\"train\")"
      ],
      "metadata": {
        "id": "d624GlocUqZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_nli"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgceGeCuU5Dn",
        "outputId": "a11cafdc-86b6-476f-9b5f-05481a74ad26"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['premise', 'hypothesis', 'label', 'idx'],\n",
              "    num_rows: 392702\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m_nli = m_nli.remove_columns([\"idx\"])\n",
        "snli =  snli.cast(m_nli.features)\n",
        "dataset = datasets.concatenate_datasets([snli, m_nli])"
      ],
      "metadata": {
        "id": "tpuj_n1_U_Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqdbLDjsVl-L",
        "outputId": "250a6031-bdb6-4491-9c0f-c180ed100144"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['premise', 'hypothesis', 'label'],\n",
              "    num_rows: 942854\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both datasets contain `-1` values in the label feature where no confident class could be assigned. We remove them using the `filter` method."
      ],
      "metadata": {
        "id": "-CT7JaIRVv74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset))\n",
        "\n",
        "# there are -1 values in the label feature, these are where no class could be decided so we remove\n",
        "dataset = dataset.filter(lambda x: 0 if x[\"label\"] == -1 else 1)\n",
        "print(len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "d5501b64d92349279280388f464c8b36",
            "f90935ba2cc64f76823af2bb2cbce8c8",
            "6e0454de4fd04a23afe79bb4582b68f2",
            "b43a59843edd4bebadd9ed754af98037",
            "345aaa093c854cad9976b6d12a897f69",
            "51c1cc264a46409d976a2f73c64e66db",
            "bfa244d044274e0bbc44ff477bb132dd",
            "93b09913a39b4cf5b5bc56c3724cedd6",
            "1686e4497fd44e3cae8e6740a0fd8c33",
            "79723d3afd85431cab8dd31746ceddc7",
            "e58da1fb4e8d4b0ba7b5fca04d22aa15"
          ]
        },
        "id": "LmJUoILxVy18",
        "outputId": "e73a5f29-16a0-4292-ad3e-4d58562ef635"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "942854\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/943 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5501b64d92349279280388f464c8b36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "942069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We must convert our human-readable sentences into transformer-readable tokens, so we go ahead and tokenize our sentences. Both `premise` and `hypothesis` features must be split into their own `input_ids` and `attention_mask` tensors."
      ],
      "metadata": {
        "id": "ARnhS62VWSKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "KJLTPxRXWXc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_cols = [\"label\"]\n",
        "\n",
        "for part in [\"premise\", \"hypothesis\"]:\n",
        "  dataset = dataset.map(\n",
        "      lambda x: tokenizer(x[part], max_length=128, padding=\"max_length\", truncation=True),\n",
        "      batched=True\n",
        "  )\n",
        "  for col in [\"input_ids\", \"attention_mask\"]:\n",
        "    dataset = dataset.rename_column(col, part + \"_\" + col)\n",
        "    all_cols.append(part + \"_\" + col)"
      ],
      "metadata": {
        "id": "6lRWlIkaWo65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_cols)"
      ],
      "metadata": {
        "id": "vaQ5aIJI4yu4",
        "outputId": "0ddabc1d-49b8-4300-d1ef-baaba56a5d9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['label', 'premise_input_ids', 'premise_attention_mask', 'hypothesis_input_ids', 'hypothesis_attention_mask']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, all we need to do is prepare the data to be read into the model. \n",
        "\n",
        "To do this, we first convert the `dataset` features into PyTorch tensors and then initialize a data loader which will feed data into our model during training."
      ],
      "metadata": {
        "id": "lt3a8Z4SZJJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# covert dataset features to PyTorch tensors\n",
        "dataset.set_format(type=\"torch\", columns=all_cols)\n",
        "\n",
        "# initialize the dataloader\n",
        "batch_size = 16\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "Jf98V0xTZL_V"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we’re done with data preparation. Let’s move on to the training approach."
      ],
      "metadata": {
        "id": "2jMpmKUlZps6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Softmax Loss"
      ],
      "metadata": {
        "id": "lk9X2-7gZqNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizing with softmax loss was the primary method used by Reimers and Gurevych in the original SBERT paper.\n",
        "\n",
        "Although this was used to train the first sentence transformer model, it is no longer the go-to training approach. Instead, [the MNR loss approach](https://www.pinecone.io/learn/fine-tune-sentence-transformers-mnr/) is most common today.\n",
        "\n",
        "However, we hope that explaining softmax loss will help demystify the different approaches applied to training sentence transformers."
      ],
      "metadata": {
        "id": "EhQRAKjeZsLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model Preparation"
      ],
      "metadata": {
        "id": "Q2Vh5nPk1z5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we train an SBERT model, we don’t need to start from scratch. We begin with an already pretrained BERT model (and tokenizer)."
      ],
      "metadata": {
        "id": "NIahSy9z108E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start from a pretrained bert-base-uncased model\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "QoBcCfP_16EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using what is called a `siamese`-BERT architecture during training. \n",
        "\n",
        "All this means is that given a sentence pair, we feed sentence A into BERT first, then feed sentence B once BERT has finished processing the first.\n",
        "\n",
        "This has the effect of creating a siamese-like network where we can imagine two identical BERTs are being trained in parallel on sentence pairs. \n",
        "\n",
        "In reality, there is just a single model processing two sentences one after the other.\n",
        "\n",
        "<img src=\"https://d33wubrfki0l68.cloudfront.net/b340f39a6c100e322d5354315e678e4caeea39a0/c5297/images/train-sentence-transformer-2.jpg\" width=600 alt=\"Start SBERT\">\n",
        "\n",
        "BERT will output `512x768`-dimensional embeddings. We will convert these into an average embedding using mean-pooling. This pooled output is our sentence embedding. We will have two per step — one for sentence A that we call $u$, and one for sentence B, called $v$.\n",
        "\n"
      ],
      "metadata": {
        "id": "rJ00jcON2NWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define mean pooling function\n",
        "def mean_pool(token_embeds, attention_mask):\n",
        "  # reshape attention_mask to cover 768-dimension embeddings\n",
        "  mask = attention_mask.unsqueeze(-1).expand(token_embeds.size()).float()\n",
        "  # perform mean-pooling but exclude padding tokens (specified by in_mask)\n",
        "  pooling = torch.sum(token_embeds * mask, 1) / torch.clamp(mask.sum(1), min=1e-9)\n",
        "\n",
        "  return pooling"
      ],
      "metadata": {
        "id": "jVAD4Y9i2elV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we take BERT’s token embeddings output and the sentence’s `attention_mask` tensor. We then resize the `attention_mask` to align to the higher 768-dimensionality of the token embeddings.\n",
        "\n",
        "We apply this resized mask to those token embeddings to exclude padding tokens from the mean pooling operation. Our mean pooling takes the average activation of values across each dimension to produce a single value. This brings our tensor sizes from `(512*768) to (1*768)`.\n",
        "\n",
        "The next step is to concatenate these embeddings. Several different approaches to this were presented in the paper:\n",
        "\n",
        "| | |\n",
        "|--|--|\n",
        "|Concatenation\t| NLI Performance|\n",
        "| (u, v)\t|66.04|\n",
        "| (\\|u-v\\|)\t|69.78|\n",
        "| (u*v)\t|70.54|\n",
        "| (\\|u-v\\|, u*v)\t|78.37|\n",
        "| (u, v, u*v)\t|77.44|\n",
        "| (u, v, \\|u-v\\|)\t|80.78|\n",
        "| (u, v, \\|u-v\\|, u*v)\t|80.44|\n",
        "\n",
        "Of these, the best performing is built by concatenating vectors $u, v$, and $|u-v|$. \n",
        "\n",
        "Concatenation of them all produces a vector three times the length of each original vector. We label this concatenated vector $(u, v, |u-v|)$. \n",
        "\n",
        "Where $|u-v|$ is the element-wise difference between vectors $u$ and $v$.\n",
        "\n",
        "<img src=\"https://d33wubrfki0l68.cloudfront.net/d1da925240ca6265229f0e0e5cc896931f41f9bf/5a050/images/train-sentence-transformer-3.jpg\" width=400 alt=\"UV Vectors\">\n",
        "\n",
        "We will perform this concatenation operation using PyTorch. \n",
        "\n",
        "Once we have our mean-pooled sentence vectors $u$ and $v$ we concatenate with:"
      ],
      "metadata": {
        "id": "rdFR0RpV5xrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "u = torch.tensor([3, 3])\n",
        "v = torch.tensor([2, 2])\n",
        "\n",
        "# produces |u-v| tensor\n",
        "uv_abs = torch.abs(torch.sub(u, v))\n",
        "# then we concatenate\n",
        "x = torch.cat([u, v, uv_abs], dim=-1)\n",
        "x"
      ],
      "metadata": {
        "id": "ZwvLdrHf7of7",
        "outputId": "0af872ae-904c-458c-b070-815366c65a87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3, 3, 2, 2, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vector $(u, v, |u-v|)$ is fed into a feed-forward neural network (FFNN). \n",
        "\n",
        "The FFNN processes the vector and outputs three activation values. One for each of our label classes; `entailment`, `neutral`, and `contradiction`.\n",
        "\n",
        "```python\n",
        "# we would initialize the feed-forward NN first\n",
        "ffnn = torch.nn.Linear(768 * 3, 3)\n",
        "...\n",
        "# then later in the code process our concatenated vector with it\n",
        "x = ffnn(x)\n",
        "```\n",
        "\n",
        "As these activations and label classes are aligned, we now calculate the softmax loss between them.\n",
        "\n",
        "<img src=\"https://d33wubrfki0l68.cloudfront.net/48ecded904827a6e7c02dd72b9fe4a1f6227052b/13070/images/train-sentence-transformer-4.jpg\" width=600 alt=\"SBERT Training\">\n",
        "\n",
        "Softmax loss is calculated by applying a softmax function across the three activation values (or nodes), producing a predicted label. \n",
        "\n",
        "We then use cross-entropy loss to calculate the difference between our predicted label and true label.\n",
        "\n",
        "```python\n",
        "# as before, we would initialize the loss function first\n",
        "loss_func = torch.nn.CrossEntropyLoss()\n",
        "...\n",
        "# then later in the code add them to the process\n",
        "x = loss_func(x, label)  # label is our *true* 0, 1, 2 class\n",
        "```\n",
        "\n",
        "The model is then optimized using this loss. \n",
        "\n",
        "We use an Adam optimizer with a learning rate of `2e-5` and a linear warmup period of 10% of the total training data for the optimization function. \n",
        "\n",
        "To set that up, we use the standard PyTorch Adam optimizer alongside a learning rate scheduler provided by HF transformers:\n",
        "\n",
        "```python\n",
        "# we would initialize everything first\n",
        "optim = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "# and setup a warmup for the first ~10% steps\n",
        "total_steps = int(dataset / batch_size)\n",
        "warmup_steps = int(0.1 * total_steps)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optim, \n",
        "                                            num_warmup_steps=warmup_steps, \n",
        "                                            num_training_steps=total_steps - warmup_steps)\n",
        "...\n",
        "# then during the training loop we update the scheduler per step\n",
        "scheduler.step()\n",
        "```\n",
        "\n",
        "Now let’s put all of that together in a PyTorch training loop.\n",
        "\n"
      ],
      "metadata": {
        "id": "xlyOj3N7_2fT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we would initialize everything first\n",
        "optim = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "# and setup a warmup for the first ~10% steps\n",
        "total_steps = int(len(dataset) / batch_size)\n",
        "warmup_steps = int(0.1 * total_steps)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optim, \n",
        "                                            num_warmup_steps=warmup_steps, \n",
        "                                            num_training_steps=total_steps - warmup_steps)\n",
        "ffnn = torch.nn.Linear(768*3, 3)\n",
        "loss_func = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "2CqoyFbyHXUq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 epoch should be enough, increase if wanted\n",
        "for epoch in range(1):\n",
        "  # make sure model is in training mode\n",
        "  model.train()\n",
        "\n",
        "  # initialize the dataloader loop with tqdm (tqdm == progress bar)\n",
        "  loop = tqdm(dataloader, leave=True)\n",
        "  for batch in loop:\n",
        "    # zero all gradients on each new step\n",
        "    optim.zero_grad()\n",
        "\n",
        "    # prepare batches and more all to the active device\n",
        "    inputs_ids_a = batch[\"premise_input_ids\"].to(device)\n",
        "    inputs_ids_b = batch[\"hypothesis_input_ids\"].to(device)\n",
        "    attention_a = batch[\"premise_attention_mask\"].to(device)\n",
        "    attention_b = batch[\"hypothesis_attention_mask\"].to(device)\n",
        "    label = batch[\"label\"].to(device)\n",
        "\n",
        "    # extract token embeddings from BERT\n",
        "    u = model(inputs_ids_a, attention_mask=attention_a)[0]  # all token embeddings A\n",
        "    v = model(inputs_ids_b, attention_mask=attention_b)[0]  # all token embeddings B\n",
        "\n",
        "    # get the mean pooled vectors\n",
        "    u = mean_pool(u, attention_a)\n",
        "    v = mean_pool(v, attention_b)\n",
        "\n",
        "    # build the |u-v| tensor\n",
        "    uv = torch.sub(u, v)\n",
        "    uv_abs = torch.abs(uv)\n",
        "\n",
        "    # concatenate u, v, |u-v|\n",
        "    x = torch.cat([u, v, uv_abs], dim=-1)\n",
        "\n",
        "    # process concatenated tensor through FFNN\n",
        "    x = ffnn(x)\n",
        "\n",
        "    # calculate the 'softmax-loss' between predicted and true label\n",
        "    loss = loss_func(x, label)\n",
        "    \n",
        "    # using loss, calculate gradients and then optimize\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "\n",
        "    # update learning rate scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # update the TDQM progress bar\n",
        "    loop.set_description(f\"Epoch {epoch}\")\n",
        "    loop.set_postfix(loss=loss.item())"
      ],
      "metadata": {
        "id": "_H3CAtFGCSLS",
        "outputId": "8bada17d-bc67-4b65-a6f3-ce6736902a7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447,
          "referenced_widgets": [
            "23ea3cffc1484dd6a3b63c37e847faa1",
            "0dbb6ebd71334ab391d6cb7ceab90f3f",
            "fc1bfa59a0f34a258d29e4bc0b5ae4b4",
            "955ebcf0e7f446f0b7af29dfd7d58600",
            "7dbc659e789b40ddb4c2e868d49810eb",
            "1d971a063e57487e98984350a9df0b9b",
            "f620cfcd0cd64c8487c3f9c60e46c7a0",
            "504d7a64451e47d3a2df9b76be718ce2",
            "541c175963874170a5c4830e17b0f183",
            "31e6ac5fffa04db7b5969d0eea25de42",
            "b60257d1790244378fa8c28699006618"
          ]
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/58880 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23ea3cffc1484dd6a3b63c37e847faa1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-cf47e6aba5ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# extract token embeddings from BERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_ids_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# all token embeddings A\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_ids_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# all token embeddings B\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    992\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m         )\n\u001b[1;32m    996\u001b[0m         encoder_outputs = self.encoder(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper__index_select)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We only train for a single epoch here. Realistically this should be enough.\n",
        "\n",
        "\n",
        "The last thing we need to do is save the model."
      ],
      "metadata": {
        "id": "C31zkJJyJFnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"./sbert_test_a\"\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "  os.mkdir(model_path)\n",
        "\n",
        "model.save_pretrained(model_path)"
      ],
      "metadata": {
        "id": "oefPTAI3JE4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let’s compare everything we’ve done so far with `sentence-transformers` training utilities."
      ],
      "metadata": {
        "id": "qDWXhZZYKeaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fine-Tuning With Sentence Transformers"
      ],
      "metadata": {
        "id": "XkVw31OuKgZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eKtDAIACKjYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7erwIjdk_5pT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}